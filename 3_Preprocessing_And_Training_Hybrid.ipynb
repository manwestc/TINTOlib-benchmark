{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import os\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "# Local application/library imports\n",
    "from utils import load_search_space\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    RocCurveDisplay, PrecisionRecallDisplay,\n",
    "    ConfusionMatrixDisplay, roc_auc_score, average_precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'boston'        \n",
    "dataset_subpath = 'Regression/boston'       \n",
    "task_type = 'Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'cmc'        \n",
    "dataset_subpath = 'Multiclass/cmc'       \n",
    "task_type = 'Multiclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'preprocessed_heloc'        \n",
    "dataset_subpath = 'Binary/heloc'       \n",
    "task_type = 'Binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"./data/{dataset_subpath}/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1232, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>League</th>\n",
       "      <th>Year</th>\n",
       "      <th>RA</th>\n",
       "      <th>W</th>\n",
       "      <th>OBP</th>\n",
       "      <th>SLG</th>\n",
       "      <th>BA</th>\n",
       "      <th>Playoffs</th>\n",
       "      <th>RankSeason</th>\n",
       "      <th>RankPlayoffs</th>\n",
       "      <th>G</th>\n",
       "      <th>OOBP</th>\n",
       "      <th>OSLG</th>\n",
       "      <th>RS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARI</td>\n",
       "      <td>NL</td>\n",
       "      <td>2012</td>\n",
       "      <td>688</td>\n",
       "      <td>81</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0</td>\n",
       "      <td>___null___</td>\n",
       "      <td>___null___</td>\n",
       "      <td>162</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.415</td>\n",
       "      <td>734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL</td>\n",
       "      <td>NL</td>\n",
       "      <td>2012</td>\n",
       "      <td>600</td>\n",
       "      <td>94</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.247</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>162</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.378</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAL</td>\n",
       "      <td>AL</td>\n",
       "      <td>2012</td>\n",
       "      <td>705</td>\n",
       "      <td>93</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.247</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>162</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.403</td>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BOS</td>\n",
       "      <td>AL</td>\n",
       "      <td>2012</td>\n",
       "      <td>806</td>\n",
       "      <td>69</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0</td>\n",
       "      <td>___null___</td>\n",
       "      <td>___null___</td>\n",
       "      <td>162</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.428</td>\n",
       "      <td>734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHC</td>\n",
       "      <td>NL</td>\n",
       "      <td>2012</td>\n",
       "      <td>759</td>\n",
       "      <td>61</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0</td>\n",
       "      <td>___null___</td>\n",
       "      <td>___null___</td>\n",
       "      <td>162</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.424</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Team League  Year   RA   W    OBP    SLG     BA  Playoffs  RankSeason  \\\n",
       "0  ARI     NL  2012  688  81  0.328  0.418  0.259         0  ___null___   \n",
       "1  ATL     NL  2012  600  94  0.320  0.389  0.247         1         4.0   \n",
       "2  BAL     AL  2012  705  93  0.311  0.417  0.247         1         5.0   \n",
       "3  BOS     AL  2012  806  69  0.315  0.415  0.260         0  ___null___   \n",
       "4  CHC     NL  2012  759  61  0.302  0.378  0.240         0  ___null___   \n",
       "\n",
       "  RankPlayoffs    G   OOBP   OSLG   RS  \n",
       "0   ___null___  162  0.317  0.415  734  \n",
       "1          5.0  162  0.306  0.378  700  \n",
       "2          4.0  162  0.315  0.403  712  \n",
       "3   ___null___  162  0.331  0.428  734  \n",
       "4   ___null___  162  0.335  0.424  613  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce = True if len(df) > 20000 else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target_tensor(y, task):\n",
    "    task = task.lower()\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "    elif isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "        \n",
    "    if task == \"regression\" or task == \"binary\":\n",
    "        return torch.as_tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    elif task == \"multiclass\":\n",
    "        return torch.as_tensor(y, dtype=torch.long)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "from typing import Optional, Tuple, Union\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def _read_split_rgb(images_folder: str, split: str, problem_type: str) -> np.ndarray:\n",
    "    \"\"\"Read RGB uint8 images for a split based on <split>/<problem_type>.csv (column 'images').\"\"\"\n",
    "    csv_path = os.path.join(images_folder, split, f\"{problem_type}.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    img_paths = [os.path.join(images_folder, split, p) for p in df[\"images\"].tolist()]\n",
    "    imgs = []\n",
    "    for p in img_paths:\n",
    "        im = cv2.imread(p, cv2.IMREAD_COLOR)\n",
    "        if im is None:\n",
    "            raise FileNotFoundError(f\"Could not read image: {p}\")\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # HxWx3 uint8\n",
    "        imgs.append(im)\n",
    "    return np.stack(imgs, axis=0)  # [N,H,W,3] uint8 (assumes same size as you stated)\n",
    "\n",
    "def _pad_constant_right_bottom_batch(\n",
    "    imgs_uint8: np.ndarray,\n",
    "    target_size: Union[int, Tuple[int, int]],\n",
    "    fill_rgb01: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Constant pad (right/bottom) to target size with fill = TRAIN mean RGB in [0,1].\n",
    "    Input:  imgs_uint8 [N,H,W,3] uint8\n",
    "    Output: float32 [N,3,Ht,Wt] in [0,1]\n",
    "    \"\"\"\n",
    "    if isinstance(target_size, int):\n",
    "        tw, th = target_size, target_size\n",
    "    else:\n",
    "        tw, th = int(target_size[0]), int(target_size[1])\n",
    "\n",
    "    N = imgs_uint8.shape[0]\n",
    "    out = np.empty((N, 3, th, tw), dtype=np.float32)\n",
    "    fill = fill_rgb01.reshape(1, 1, 3)  # (1,1,3) in [0,1]\n",
    "\n",
    "    for i in range(N):\n",
    "        im01 = imgs_uint8[i].astype(np.float32) / 255.0  # [H,W,3] in [0,1]\n",
    "        h, w, _ = im01.shape\n",
    "        if w > tw or h > th:\n",
    "            raise ValueError(f\"Image {w}x{h} larger than target {tw}x{th}. Increase target_size or resize upstream.\")\n",
    "        canvas = np.empty((th, tw, 3), dtype=np.float32)\n",
    "        canvas[:] = fill\n",
    "        canvas[:h, :w, :] = im01\n",
    "        out[i] = np.transpose(canvas, (2, 0, 1))\n",
    "    return out\n",
    "\n",
    "def load_and_preprocess_data(\n",
    "    df, dataset_name, images_folder,\n",
    "    problem_type, task_type,\n",
    "    seed: int = 42, batch_size: int = 32, device: str = 'cpu',\n",
    "    pad_images: bool = False, target_size: Optional[Union[int, Tuple[int, int]]] = None,\n",
    "):\n",
    "    task_type = task_type.lower()\n",
    "\n",
    "    # ----- Config -----\n",
    "    with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "        config = json.load(f)\n",
    "    categorical_cols = config[\"categorical_cols\"]\n",
    "    numerical_cols = config[\"numerical_cols\"]\n",
    "    encoding = config[\"encoding\"]\n",
    "\n",
    "    # ----- Features / target -----\n",
    "    X = df[numerical_cols + categorical_cols].copy()\n",
    "    y = df.iloc[:, -1].copy()\n",
    "\n",
    "    le = None\n",
    "    if encoding.get(\"target\") == \"label\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        label_mapping = None\n",
    "\n",
    "    # ----- Splits (70/15/15) -----\n",
    "    if task_type == \"regression\":\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "        X_val_raw,   X_test_raw, y_val,  y_test  = train_test_split(X_temp_raw, y_temp, test_size=0.5, random_state=seed)\n",
    "    else:\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "        X_val_raw,   X_test_raw, y_val,  y_test  = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed, stratify=y_temp\n",
    "        )\n",
    "\n",
    "    # ----- Class weights (optional) -----\n",
    "    class_weight = None\n",
    "    if task_type in [\"binary\", \"multiclass\"]:\n",
    "        cw_vals = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "        classes_sorted = np.sort(np.unique(y_train))\n",
    "        if task_type == \"binary\":\n",
    "            wd = dict(zip(classes_sorted, cw_vals))\n",
    "            pos_weight = wd[1] / wd[0]\n",
    "            class_weight = torch.tensor(pos_weight, dtype=torch.float32)\n",
    "            print(f\"Binary pos_weight (for BCEWithLogitsLoss): {class_weight.item():.6f}\")\n",
    "        else:\n",
    "            class_weight = torch.tensor(cw_vals, dtype=torch.float32)\n",
    "            print(f\"Multiclass class weights (for CrossEntropyLoss): {class_weight.tolist()}\")\n",
    "\n",
    "    # ----- ColumnTransformer (fit on TRAIN only) -----\n",
    "    transformers = []\n",
    "    if encoding.get(\"numerical_features\") == \"minmax\":\n",
    "        transformers.append((\"num\", MinMaxScaler(), numerical_cols))\n",
    "    elif encoding.get(\"numerical_features\") == \"standard\":\n",
    "        transformers.append((\"num\", StandardScaler(), numerical_cols))\n",
    "    if categorical_cols and encoding.get(\"categorical_features\") == \"onehot\":\n",
    "        transformers.append((\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), categorical_cols))\n",
    "\n",
    "    if transformers:\n",
    "        preprocessor = ColumnTransformer(transformers=transformers)\n",
    "        X_train = preprocessor.fit_transform(X_train_raw)\n",
    "        X_val   = preprocessor.transform(X_val_raw)\n",
    "        X_test  = preprocessor.transform(X_test_raw)\n",
    "\n",
    "        if \"cat\" in preprocessor.named_transformers_:\n",
    "            cat_feature_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    "            all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "        else:\n",
    "            all_feature_names = numerical_cols + categorical_cols\n",
    "\n",
    "        X_train_num = pd.DataFrame(X_train, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val_num   = pd.DataFrame(X_val,   columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test_num  = pd.DataFrame(X_test,  columns=all_feature_names, index=X_test_raw.index)\n",
    "    else:\n",
    "        all_feature_names = numerical_cols + categorical_cols\n",
    "        X_train_num = pd.DataFrame(X_train_raw, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val_num   = pd.DataFrame(X_val_raw,   columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test_num  = pd.DataFrame(X_test_raw,  columns=all_feature_names, index=X_test_raw.index)\n",
    "\n",
    "    print(f\"Shapes — Train: {X_train_num.shape}, Val: {X_val_num.shape}, Test: {X_test_num.shape}\")\n",
    "    print(f\"Numerical features: {len(numerical_cols)} — {numerical_cols}\")\n",
    "    print(f\"Categorical features: {len(categorical_cols)} — {categorical_cols}\")\n",
    "    print(f\"Total features: {X_train_num.shape[1]}\")\n",
    "    if label_mapping:\n",
    "        print(f\"Target label mapping: {label_mapping}\")\n",
    "\n",
    "    # ----- Images (uint8 RGB) -----\n",
    "    X_train_img_u8 = _read_split_rgb(images_folder, \"train\", problem_type)\n",
    "    X_val_img_u8   = _read_split_rgb(images_folder, \"val\",   problem_type)\n",
    "    X_test_img_u8  = _read_split_rgb(images_folder, \"test\",  problem_type)\n",
    "\n",
    "    # ----- Optional padding with TRAIN mean (no normalization) -----\n",
    "    if pad_images:\n",
    "        if target_size is None:\n",
    "            raise ValueError(\"pad_images=True requires target_size (int or (W,H)).\")\n",
    "        train_mean_rgb01 = (X_train_img_u8.astype(np.float32) / 255.0).reshape(-1, 3).mean(axis=0).astype(np.float32)\n",
    "        X_train_arr = _pad_constant_right_bottom_batch(X_train_img_u8, target_size, train_mean_rgb01)\n",
    "        X_val_arr   = _pad_constant_right_bottom_batch(X_val_img_u8,   target_size, train_mean_rgb01)\n",
    "        X_test_arr  = _pad_constant_right_bottom_batch(X_test_img_u8,  target_size, train_mean_rgb01)\n",
    "\n",
    "        if isinstance(target_size, int):\n",
    "            tw = th = int(target_size)\n",
    "        else:\n",
    "            tw, th = int(target_size[0]), int(target_size[1])\n",
    "        imgs_shape = (3, th, tw)\n",
    "    else:\n",
    "        # Scale to [0,1] and convert to NCHW.\n",
    "        X_train_arr = (X_train_img_u8.astype(np.float32) / 255.0).transpose(0, 3, 1, 2)\n",
    "        X_val_arr   = (X_val_img_u8.astype(np.float32)   / 255.0).transpose(0, 3, 1, 2)\n",
    "        X_test_arr  = (X_test_img_u8.astype(np.float32)  / 255.0).transpose(0, 3, 1, 2)\n",
    "        _, C, H, W = X_train_arr.shape\n",
    "        imgs_shape = (C, H, W)\n",
    "\n",
    "    # ----- Tensors & DataLoaders -----\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor   = torch.as_tensor(X_val_num.values,   dtype=torch.float32)\n",
    "    X_test_num_tensor  = torch.as_tensor(X_test_num.values,  dtype=torch.float32)\n",
    "\n",
    "    X_train_img_tensor = torch.from_numpy(X_train_arr)  # float32 [0,1], NCHW\n",
    "    X_val_img_tensor   = torch.from_numpy(X_val_arr)\n",
    "    X_test_img_tensor  = torch.from_numpy(X_test_arr)\n",
    "\n",
    "    y_train_tensor = prepare_target_tensor(y_train, task_type)\n",
    "    y_val_tensor   = prepare_target_tensor(y_val,   task_type)\n",
    "    y_test_tensor  = prepare_target_tensor(y_test,  task_type)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_num_tensor, X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset   = TensorDataset(X_val_num_tensor,   X_val_img_tensor,   y_val_tensor)\n",
    "    test_dataset  = TensorDataset(X_test_num_tensor,  X_test_img_tensor,  y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    attributes = X_train_num.shape[1]\n",
    "    print(\"Images shape (C,H,W):\", imgs_shape)\n",
    "    print(\"Attributes:\", attributes)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape, le, class_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Check to include both divisors if they are not the same\n",
    "                divisors.append(n // i)\n",
    "    divisors.sort()\n",
    "    return divisors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit_pytorch.simple_vit_with_register_tokens import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# --- helpers ---\n",
    "def get_act(name: str):\n",
    "    return nn.ReLU if str(name).lower() == \"relu\" else nn.GELU\n",
    "\n",
    "class ViTMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid ViT + Tabular MLP (simple concat fusion, no projections, no LayerNorm).\n",
    "    Required params:\n",
    "      ViT: patch_size, dim, depth, heads, mlp_dim, vit_dropout, vit_emb_dropout\n",
    "      Tab: activation, mlp_hidden_dims, tab_dropout\n",
    "      Fusion: fusion_hidden_dims, fusion_dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, imgs_shape, num_input_dim, params, task, num_classes=None):\n",
    "        super().__init__()\n",
    "        act = get_act(params[\"activation\"])\n",
    "\n",
    "        # ---- ViT branch (pooled vector of size dim) ----\n",
    "        self.vit = ViT(\n",
    "            image_size=imgs_shape,\n",
    "            patch_size=params[\"patch_size\"],\n",
    "            dim=params[\"dim\"],\n",
    "            depth=params[\"depth\"],\n",
    "            heads=params[\"heads\"],\n",
    "            mlp_dim=params[\"mlp_dim\"] * params[\"dim\"],  # expansion ratio → hidden dim\n",
    "            dropout=params[\"vit_dropout\"],\n",
    "            emb_dropout=params[\"vit_emb_dropout\"]\n",
    "        )\n",
    "        vit_out_dim = int(params[\"dim\"])\n",
    "\n",
    "        # ---- Tabular MLP branch (no LayerNorm) ----\n",
    "        tab_layers, in_dim = [], int(num_input_dim)\n",
    "        tab_dropout = float(params.get(\"tab_dropout\", 0.0))\n",
    "        for h in params[\"mlp_hidden_dims\"]:\n",
    "            h = int(h)\n",
    "            tab_layers += [\n",
    "                nn.Linear(in_dim, h),\n",
    "                act(),\n",
    "                nn.Dropout(tab_dropout) if tab_dropout > 0 else nn.Identity()\n",
    "            ]\n",
    "            in_dim = h\n",
    "        self.tabular_mlp = nn.Sequential(*tab_layers)\n",
    "        tab_out_dim = in_dim if tab_layers else int(num_input_dim)\n",
    "\n",
    "        # ---- Simple concat fusion head (no LayerNorm) ----\n",
    "        fused_in_dim = vit_out_dim + tab_out_dim\n",
    "        fusion_layers = []\n",
    "        fusion_dropout = float(params.get(\"fusion_dropout\", 0.0))\n",
    "        for h in params[\"fusion_hidden_dims\"]:\n",
    "            h = int(h)\n",
    "            fusion_layers += [\n",
    "                nn.Linear(fused_in_dim, h),\n",
    "                act(),\n",
    "                nn.Dropout(fusion_dropout) if fusion_dropout > 0 else nn.Identity()\n",
    "            ]\n",
    "            fused_in_dim = h\n",
    "\n",
    "        out_dim = 1 if task in (\"regression\", \"binary\") else int(num_classes)\n",
    "        fusion_layers.append(nn.Linear(fused_in_dim, out_dim))\n",
    "        self.fusion_mlp = nn.Sequential(*fusion_layers)\n",
    "\n",
    "    def forward(self, num_input, vit_input):\n",
    "        vit_feat = self.vit(vit_input)         # (B, dim)\n",
    "        tab_feat = self.tabular_mlp(num_input) # (B, tab_out)\n",
    "        fused = torch.cat([vit_feat, tab_feat], dim=1)\n",
    "        return self.fusion_mlp(fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Reuse your get_act helper\n",
    "def get_act(name: str):\n",
    "    return nn.ReLU if str(name).lower() == \"relu\" else nn.GELU\n",
    "\n",
    "# ---------------- Stem ----------------\n",
    "class UnifiedStem(nn.Module):\n",
    "    \"\"\"\n",
    "    - '3x3' stem: safe for tiny images (3x3, 5x5, 32x32).\n",
    "    - '7x7' stem (+ optional maxpool): classic ImageNet style for large images.\n",
    "    Only apply 7x7+stride2 when max(H,W) >= 64; otherwise fallback to 3x3.\n",
    "    \"\"\"\n",
    "    def __init__(self, C, stem_width, stem_type=\"3x3\", use_maxpool=True, H=None, W=None):\n",
    "        super().__init__()\n",
    "        large = (max(H or 0, W or 0) >= 64)\n",
    "        if stem_type == \"7x7\" and large:\n",
    "            layers = [\n",
    "                nn.Conv2d(C, stem_width, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "                nn.BatchNorm2d(stem_width),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1) if use_maxpool else nn.Identity(),\n",
    "            ]\n",
    "        else:\n",
    "            layers = [\n",
    "                nn.Conv2d(C, stem_width, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(stem_width),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --------------- Basic Block ---------------\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.down is not None:\n",
    "            identity = self.down(identity)\n",
    "        out = self.relu(out + identity)\n",
    "        return out\n",
    "\n",
    "# --------------- ResNet Backbone (features only) ---------------\n",
    "class ResNetBackboneAnySize(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic ResNet (BasicBlocks), size-agnostic via AdaptiveAvgPool2d(1).\n",
    "    Outputs a feature vector (no classifier).\n",
    "    \"\"\"\n",
    "    def __init__(self, params, imgs_shape):\n",
    "        super().__init__()\n",
    "        C, H, W = imgs_shape\n",
    "        assert params[\"in_channels\"] == C, \"in_channels must match imgs_shape[0]\"\n",
    "\n",
    "        # knobs\n",
    "        stem_type     = params[\"stem_type\"]\n",
    "        use_maxpool   = params[\"use_maxpool\"]\n",
    "        stem_width    = params[\"stem_width\"]\n",
    "        blocks_ps     = params[\"blocks_per_stage\"]  # e.g., \"[2,2,2,2]\"\n",
    "        n_stages      = len(blocks_ps)\n",
    "        base_width    = params[\"base_width\"]\n",
    "        width_mul     = params[\"width_mul\"]\n",
    "\n",
    "        # stem\n",
    "        self.stem = UnifiedStem(C, stem_width, stem_type=stem_type, use_maxpool=use_maxpool, H=H, W=W)\n",
    "\n",
    "        # stage widths\n",
    "        B = int(base_width * width_mul)\n",
    "        all_out = [B, B*2, B*4, B*8][:n_stages]\n",
    "        blocks_ps = [max(1, int(x)) for x in list(blocks_ps)[:n_stages]]\n",
    "\n",
    "        # approximate current spatial size after stem\n",
    "        curH, curW = H, W\n",
    "        if stem_type == \"7x7\" and max(H, W) >= 64:\n",
    "            curH = max(1, curH // 2)\n",
    "            curW = max(1, curW // 2)\n",
    "            if use_maxpool:\n",
    "                curH = max(1, curH // 2)\n",
    "                curW = max(1, curW // 2)\n",
    "\n",
    "        in_planes = stem_width\n",
    "        layers = []\n",
    "\n",
    "        def can_downsample(h, w):\n",
    "            return (h >= 4 and w >= 4)\n",
    "\n",
    "        for si in range(n_stages):\n",
    "            out_planes = all_out[si]\n",
    "            n_blocks   = blocks_ps[si]\n",
    "            stride = 2 if (si > 0 and can_downsample(curH, curW)) else 1\n",
    "            layers.append(BasicBlock(in_planes, out_planes, stride=stride))\n",
    "            in_planes = out_planes\n",
    "            if stride == 2:\n",
    "                curH = max(1, curH // 2)\n",
    "                curW = max(1, curW // 2)\n",
    "            for _ in range(n_blocks - 1):\n",
    "                layers.append(BasicBlock(in_planes, out_planes, stride=1))\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        # infer feature dim\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, C, max(1, H), max(1, W))\n",
    "            x = self.stem(dummy)\n",
    "            x = self.features(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.flat(x)\n",
    "            self.feat_dim = x.shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat(x)\n",
    "        return x  # (B, feat_dim)\n",
    "\n",
    "# CNN\n",
    "\n",
    "def get_act(name: str):\n",
    "    return nn.ReLU if str(name).lower() == \"relu\" else nn.GELU\n",
    "\n",
    "class CNNMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN backbone -> MLP head (mirrors ViTMLP: encoder + MLP head).\n",
    "    Uses your ResNetBackboneAnySize.\n",
    "    \"\"\"\n",
    "    def __init__(self, imgs_shape, num_input_dim, params, task, num_classes=None, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.task = task.lower()\n",
    "        act = get_act(params.get(\"activation\", \"relu\"))\n",
    "\n",
    "        # Backbone (reuse your class; params must include cnn knobs)\n",
    "        self.backbone = ResNetBackboneAnySize(params, imgs_shape)\n",
    "        feat_dim = self.backbone.feat_dim\n",
    "        \n",
    "        # ---- Tabular MLP branch ----\n",
    "        tab_layers, in_dim = [], num_input_dim\n",
    "        tab_dropout = float(params[\"tab_dropout\"])\n",
    "        for h in params[\"mlp_hidden_dims\"]:\n",
    "            tab_layers += [\n",
    "                nn.Linear(in_dim, h),\n",
    "                act(),\n",
    "                nn.Dropout(tab_dropout) if tab_dropout > 0 else nn.Identity()\n",
    "            ]\n",
    "            in_dim = h\n",
    "        self.tab_mlp = nn.Sequential(*tab_layers)\n",
    "        tab_out = in_dim if tab_layers else num_input_dim\n",
    "\n",
    "        # ---- Fusion (concat) ----\n",
    "        fused_dim = feat_dim + tab_out\n",
    "        fusion_layers = []\n",
    "        fusion_dropout = float(params[\"fusion_dropout\"])\n",
    "        for h in params[\"fusion_hidden_dims\"]:\n",
    "            fusion_layers += [\n",
    "                nn.Linear(fused_dim, h),\n",
    "                act(),\n",
    "                nn.Dropout(fusion_dropout) if fusion_dropout > 0 else nn.Identity()\n",
    "            ]\n",
    "            fused_dim = h\n",
    "\n",
    "        out_dim = 1 if task in (\"regression\", \"binary\") else num_classes\n",
    "        fusion_layers.append(nn.Linear(fused_dim, out_dim))\n",
    "        self.fusion = nn.Sequential(*fusion_layers)\n",
    "\n",
    "    def forward(self, num_input, img_input):\n",
    "        cnn_feat = self.backbone(img_input)\n",
    "        tab_feat = self.tab_mlp(num_input) if len(self.tab_mlp) else num_input\n",
    "        fused = torch.cat([cnn_feat, tab_feat], dim=1)\n",
    "        out = self.fusion(fused)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetMLP(nn.Module):\n",
    "    def __init__(self, imgs_shape, num_input_dim, params, task_type, num_classes=None):\n",
    "        super(ResNetMLP, self).__init__()\n",
    "\n",
    "        # Load a ResNet50 with or without pretrained weights\n",
    "        base_resnet = models.resnet50(weights=None)\n",
    "\n",
    "        self.resnet_backbone = nn.Sequential(*list(base_resnet.children())[:-1])  # (B, 2048, 1, 1)\n",
    "        self.flatten = nn.Flatten()  # Converts (B, 2048, 1, 1) → (B, 2048)\n",
    "\n",
    "        # Tabular MLP branch\n",
    "        tabular_layers = []\n",
    "        input_dim = num_input_dim\n",
    "        for hidden_dim in params[\"mlp_hidden_dims\"]:\n",
    "            tabular_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            tabular_layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "        self.tabular_mlp = nn.Sequential(*tabular_layers)\n",
    "\n",
    "        # Create a dummy image based on the input image shape to calculate the output size\n",
    "        dummy_img = torch.randn(4, *imgs_shape)  # (B, 3, H, W)\n",
    "        with torch.no_grad():\n",
    "            # Pass the dummy image through ResNet to get feature map\n",
    "            img_feat = self.resnet_backbone(dummy_img)  \n",
    "            resnet_output_dim = self.flatten(img_feat)\n",
    "\n",
    "        # Fusion MLP head (ResNet features + Tabular MLP)\n",
    "        fusion_input_dim = resnet_output_dim.shape[1] + input_dim\n",
    "        fusion_layers = []\n",
    "        for hidden_dim in params.get(\"fusion_hidden_dims\", [128]):\n",
    "            fusion_layers.append(nn.Linear(fusion_input_dim, hidden_dim))\n",
    "            fusion_layers.append(nn.ReLU())\n",
    "            fusion_input_dim = hidden_dim\n",
    "\n",
    "        output_dim = 1 if task_type in [\"regression\", \"binary\"] else num_classes\n",
    "        fusion_layers.append(nn.Linear(fusion_input_dim, output_dim))\n",
    "        self.fusion_mlp = nn.Sequential(*fusion_layers)\n",
    "\n",
    "        # Output activation\n",
    "        self.activation = nn.Identity()\n",
    "\n",
    "    def forward(self, num_input, img_input):\n",
    "        # ResNet feature extraction\n",
    "        img_feat = self.resnet_backbone(img_input)  # (B, 2048, 1, 1)\n",
    "        img_feat = self.flatten(img_feat)           # (B, 2048)\n",
    "\n",
    "        # Tabular feature extraction\n",
    "        tab_feat = self.tabular_mlp(num_input)      # (B, D_tabular)\n",
    "\n",
    "        # Fusion and classification\n",
    "        fusion = torch.cat([img_feat, tab_feat], dim=1)\n",
    "\n",
    "        output = self.fusion_mlp(fusion)\n",
    "\n",
    "        return self.activation(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from models.utils import get_loss_fn, calculate_metrics, calculate_metrics_hybrid, calculate_metrics_hybrid_manuel, calculate_metrics_from_numpy, get_class_weighted_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "def compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, \n",
    "                    model_name, image_name, trial_name=None, task='regression', epochs=200, max_lr=1, \n",
    "                    div_factor=10, final_div_factor=1, device='cuda', weight_decay=1e-2, pct_start=0.3, save_model=False, class_weights=None, save_dir=None, study=None, patch=None, verbose=False):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if class_weights != None:\n",
    "        loss_fn = get_class_weighted_loss_fn(task, class_weights)\n",
    "    else:\n",
    "        loss_fn = get_loss_fn(task)\n",
    "\n",
    "    # Compute min_lr from max_lr and div_factor\n",
    "    min_lr = max_lr / div_factor\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=div_factor, final_div_factor=final_div_factor, total_steps=total_steps, pct_start=pct_start, anneal_strategy=\"cos\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "    #early_stopping_counter = 0\n",
    "    #patience = 10  # Early stopping patience\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    if task == 'regression':\n",
    "        history.update({'train_mse': [],  'val_mse': [], 'train_mae': [],  'val_mae': [], 'train_rmse': [], 'val_rmse': [], 'train_r2': [], 'val_r2': []})\n",
    "    elif task in ['binary', 'multiclass']:\n",
    "        history.update({'train_accuracy': [], 'val_accuracy': [], 'train_precision': [], 'val_precision': [], 'train_recall': [], 'val_recall': [], 'train_f1': [], 'val_f1': []})\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "\n",
    "        for num_data, img_data, targets in train_loader:\n",
    "            num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        if task == 'multiclass':\n",
    "            y_train_pred = np.vstack(train_preds)\n",
    "            y_train_true = train_targets\n",
    "        else:\n",
    "            y_train_pred = np.concatenate(train_preds)\n",
    "            y_train_true = np.concatenate(train_targets)\n",
    "        train_metrics = calculate_metrics_from_numpy(y_train_true, y_train_pred, task)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for num_data, img_data, targets in val_loader:\n",
    "                num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                outputs = model(num_data, img_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        if task == 'multiclass':\n",
    "            y_val_pred = np.vstack(val_preds)\n",
    "            y_val_true = val_targets\n",
    "        else:\n",
    "            y_val_pred = np.concatenate(val_preds)\n",
    "            y_val_true = np.concatenate(val_targets)\n",
    "        val_metrics = calculate_metrics_from_numpy(y_val_true, y_val_pred, task)\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "        for k, v in train_metrics.items():\n",
    "            history[f'train_{k}'].append(v)\n",
    "        for k, v in val_metrics.items():\n",
    "            history[f'val_{k}'].append(v)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "            #early_stopping_counter = 0\n",
    "        #else:\n",
    "        #    early_stopping_counter += 1\n",
    "        #    if early_stopping_counter >= patience:\n",
    "        #        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        #        break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Recompute metrics using the best model\n",
    "    train_metrics, y_true_train, y_pred_train, y_prob_train = calculate_metrics_hybrid(model, train_loader, device, class_weights, task)\n",
    "    val_metrics, y_true_val, y_pred_val, y_prob_val  = calculate_metrics_hybrid(model, val_loader, device, class_weights, task)\n",
    "    test_metrics, y_true_test, y_pred_test, y_prob_test = calculate_metrics_hybrid(model, test_loader, device, class_weights, task)\n",
    "\n",
    "    # Store recomputed metrics\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "\n",
    "    # Add task-specific metrics\n",
    "    for k in train_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'train_{k}'] = train_metrics[k]\n",
    "    for k in val_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'val_{k}'] = val_metrics[k]\n",
    "    for k in test_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'test_{k}'] = test_metrics[k]\n",
    "            \n",
    "    if verbose:     \n",
    "        print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "        print(f\"Best model found at epoch {best_epoch}/{epochs}\")\n",
    "        print(f\"Best Train Loss: {metrics['train_loss']:.4f}, Best Val Loss: {metrics['val_loss']:.4f}\")\n",
    "        print(metrics)\n",
    "    \n",
    "    if save_model:\n",
    "        if model_name == \"CNN_hybrid\":\n",
    "            save_path = os.path.join(save_dir, f\"{model_name}/{image_name}/best_model/{trial_name}\")\n",
    "        else:\n",
    "            save_path = os.path.join(save_dir, f\"{model_name}/{image_name}/best_model/{trial_name}\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        plot_metric(history['train_loss'], history['val_loss'], 'Loss', save_path)\n",
    "        if task == 'regression':\n",
    "            plot_metric(history['train_mse'], history['val_mse'], 'MSE', save_path)\n",
    "            plot_metric(history['train_rmse'], history['val_rmse'], 'RMSE', save_path)\n",
    "        else:\n",
    "            plot_metric(history['train_accuracy'], history['val_accuracy'], 'Accuracy', save_path)\n",
    "            plot_metric(history['train_f1'], history['val_f1'], 'F1', save_path)\n",
    "\n",
    "        plot_learning_rate(history['learning_rate'], save_path)\n",
    "\n",
    "        # Save metrics\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        with open(f'{save_path}/best_model_metrics.txt', 'w') as f:\n",
    "            for key, value in metrics.items():\n",
    "                f.write(f'{key}: {value}\\n')\n",
    "\n",
    "        # Save model\n",
    "        torch.save(best_model, f\"{save_path}/best_model.pth\")\n",
    "        print(f\"Best model saved to {save_path}/best_model.pth\")\n",
    "\n",
    "        # Additional plots for classification\n",
    "        if task in [\"binary\"]:\n",
    "            plot_extra(\"Train\", y_true_train, y_pred_train, y_prob_train, save_path)\n",
    "            plot_extra(\"Validation\", y_true_val, y_pred_val, y_prob_val, save_path)\n",
    "            plot_extra(\"Test\", y_true_test, y_pred_test, y_prob_test, save_path)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_extra(split_name, y_true, y_pred, y_prob, save_path):\n",
    "    y_true = y_true.ravel()\n",
    "    y_pred = y_pred.ravel()\n",
    "\n",
    "    # ROC Curve\n",
    "    RocCurveDisplay.from_predictions(y_true, y_prob)\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
    "    plt.title(f\"{split_name} ROC Curve (AUC = {auc_score:.2f})\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_roc_curve.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    PrecisionRecallDisplay.from_predictions(y_true, y_prob)\n",
    "    avg_prec = average_precision_score(y_true, y_prob)\n",
    "    plt.title(f\"{split_name} PR Curve (AP = {avg_prec:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_pr_curve.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Normalized confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true').plot(cmap='Blues')\n",
    "    plt.title(f\"{split_name} Confusion Matrix (Normalized)\")\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_confusion_matrix_normalized.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Raw confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize=None).plot(cmap='Blues')\n",
    "    plt.title(f\"{split_name} Confusion Matrix (Counts)\")\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_confusion_matrix_counts.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "\n",
    "def plot_metric(train_metric, val_metric, metric_name, save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(train_metric, label=f'Train {metric_name}')\n",
    "    plt.plot(val_metric, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric_name} vs. Epoch')\n",
    "    save_path = f\"{save_path}/{metric_name.lower()}_plot.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(\"all\")\n",
    "\n",
    "def plot_learning_rate(learning_rates, save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate vs. Epoch')\n",
    "    save_path = f\"{save_path}/learning_rate_plot.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HyViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir =  os.path.join(\"logs\", task_type, dataset_name)\n",
    "model_name = \"ViT_hybrid\"\n",
    "\n",
    "# Load config\n",
    "with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "    config = json.load(f)\n",
    "batch_size = config[\"batch_size\"]\n",
    "\n",
    "epochs = 50\n",
    "n_trials = 50\n",
    "\n",
    "if task_type.lower() == 'multiclass':\n",
    "    num_classes = df.iloc[:,-1].nunique()\n",
    "else:\n",
    "    num_classes = 1\n",
    "\n",
    "device='cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir =  os.path.join(\"logs\", task_type, dataset_name)\n",
    "model_name = \"CNN_hybrid\"\n",
    "\n",
    "# Load config\n",
    "with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "    config = json.load(f)\n",
    "batch_size = config[\"batch_size\"]\n",
    "\n",
    "epochs = 50\n",
    "n_trials = 50\n",
    "\n",
    "if task_type.lower() == 'multiclass':\n",
    "    num_classes = df.iloc[:,-1].nunique()\n",
    "else:\n",
    "    num_classes = 1\n",
    "\n",
    "device='cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model_name, image_name, task_type, \n",
    "              train_loader, val_loader, test_loader,\n",
    "              divisors, attributes, imgs_shape, num_classes=None,\n",
    "              device='cuda', save_dir=None, class_weight=None, epochs=100, path_vision=None, path_mlp=None):\n",
    "    \n",
    "    if model_name == \"ViT_hybrid\":\n",
    "        task = task_type.lower()\n",
    "        \n",
    "        params = load_search_space(model_name, trial)\n",
    "\n",
    "        with open(f\"{path_vision}/best_params.json\", \"r\") as f:\n",
    "            params_best_vit = json.load(f)\n",
    "            \n",
    "        params_best_vit = {\n",
    "            k: v for k, v in params_best_vit.items()\n",
    "            if k in [\"patch_size\", \"dim\", \"depth\", \"heads\", \"mlp_dim\", \"dropout\", \"emb_dropout\"]\n",
    "        }\n",
    "        \n",
    "        with open(f\"{path_mlp}/best_params.json\", \"r\") as f:\n",
    "            params_best_mlp = json.load(f)\n",
    "            \n",
    "        params_best_mlp = {\n",
    "            k: v for k, v in params_best_mlp.items()\n",
    "            if k in [\"mlp_hidden_dims\", \"dropout\"]\n",
    "        }\n",
    "        \n",
    "        params_best_vit[\"vit_dropout\"] = params_best_vit.pop(\"dropout\")\n",
    "        params_best_vit[\"vit_emb_dropout\"] = params_best_vit.pop(\"emb_dropout\")\n",
    "        params_best_mlp[\"tab_dropout\"] = params_best_mlp.pop(\"dropout\")\n",
    "            \n",
    "        params = {**params, **params_best_vit, **params_best_mlp}\n",
    "\n",
    "        params[\"mlp_hidden_dims\"] = json.loads(params[\"mlp_hidden_dims\"])\n",
    "\n",
    "        params[\"fusion_hidden_dims\"] = json.loads(params[\"fusion_hidden_dims\"])\n",
    "\n",
    "        with open(f\"configs/optuna_search/{model_name}.json\", \"r\") as f:\n",
    "            full_config = json.load(f)\n",
    "\n",
    "        config = full_config[model_name][\"fit\"]  # Access the model key\n",
    "\n",
    "        # Initialize model\n",
    "        model = ViTMLP(imgs_shape[1], attributes, params, task, num_classes)\n",
    "    else:\n",
    "        task = task_type.lower()\n",
    "\n",
    "        params = load_search_space(model_name, trial)\n",
    "\n",
    "        with open(f\"{path_vision}/best_params.json\", \"r\") as f:\n",
    "            params_best_cnn = json.load(f)\n",
    "            \n",
    "        params_best_cnn = {\n",
    "            k: v for k, v in params_best_cnn.items()\n",
    "            if k in [\"in_channels\", \"activation\", \"stem_type\", \"use_maxpool\", \"stem_width\",\n",
    "                     \"blocks_per_stage\", \"base_width\", \"width_mul\"]\n",
    "        }\n",
    "        \n",
    "        with open(f\"{path_mlp}/best_params.json\", \"r\") as f:\n",
    "            params_best_mlp = json.load(f)\n",
    "        \n",
    "        params_best_mlp = {\n",
    "            k: v for k, v in params_best_mlp.items()\n",
    "            if k in [\"mlp_hidden_dims\", \"dropout\"]\n",
    "        }\n",
    "        \n",
    "        params_best_mlp[\"tab_dropout\"] = params_best_mlp.pop(\"dropout\")\n",
    "        \n",
    "        with open(f\"configs/optuna_search/{model_name}.json\", \"r\") as f:\n",
    "            full_config = json.load(f)\n",
    "            \n",
    "        params = {**params, **params_best_cnn, **params_best_mlp}\n",
    "            \n",
    "        # parse head dims safely (keeps your JSON format)\n",
    "        params[\"mlp_hidden_dims\"] = json.loads(params[\"mlp_hidden_dims\"])\n",
    "        \n",
    "        params[\"fusion_hidden_dims\"] = json.loads(params[\"fusion_hidden_dims\"])\n",
    "                    \n",
    "        params[\"blocks_per_stage\"] = json.loads(params[\"blocks_per_stage\"])\n",
    "            \n",
    "        config = full_config[model_name][\"fit\"]  # Access the model key\n",
    "        \n",
    "        # Build and train model\n",
    "        model = CNNMLP(imgs_shape, attributes, params, task, num_classes)\n",
    "        \n",
    "    metrics = compile_and_fit(\n",
    "        model,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=f\"trial_{trial.number}\",\n",
    "        image_name=image_name,\n",
    "        task=task,  # assumed to be defined externally\n",
    "        max_lr=trial.suggest_float(\"max_lr\", config[\"max_lr\"][1], config[\"max_lr\"][2], log=True),\n",
    "        div_factor=trial.suggest_int(\"div_factor\", config[\"div_factor\"][1], config[\"div_factor\"][2]),\n",
    "        final_div_factor=trial.suggest_int(\"final_div_factor\", config[\"final_div_factor\"][1], config[\"final_div_factor\"][2]),\n",
    "        weight_decay=trial.suggest_float(\"weight_decay\", config[\"weight_decay\"][1], config[\"weight_decay\"][2], log=True),\n",
    "        pct_start=trial.suggest_float(\"pct_start\", config[\"pct_start\"][1], config[\"pct_start\"][2]),\n",
    "        epochs=epochs,\n",
    "        save_model=False,\n",
    "        class_weights=class_weight\n",
    "    )\n",
    "\n",
    "    save_dir = os.path.join(save_dir, model_name, image_name, \"optuna\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if task == 'regression':\n",
    "        score = metrics[\"val_rmse\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-RMSE: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    elif task == 'binary':\n",
    "        score = metrics[\"val_roc_auc\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-AUC: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    elif task == 'multiclass':\n",
    "        score = metrics[\"val_accuracy\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-Accuracy: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "    \n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_model_seed(seed: int):\n",
    "    # Python built-in RNG\n",
    "    random.seed(seed)\n",
    "    # NumPy RNG\n",
    "    np.random.seed(seed)\n",
    "    # Torch RNG\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you use multi-GPU\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === benchmark_eval.py ========================================================\n",
    "from numbers import Number\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# NEW: optional calflops (robust fallback if not installed)\n",
    "# -----------------------------------------------------------------------------\n",
    "from calflops import calculate_flops as _calflops_calc\n",
    "_HAVE_CALFLOPS = True\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Config (adjust as needed)\n",
    "# -----------------------------------------------------------------------------\n",
    "TOP_K = 5\n",
    "SINGLE_PASS_SEED = 0              # seed for one-time eval of top-K\n",
    "FINAL_SEEDS = [0, 1, 2, 3, 4]     # seeds for the final winner\n",
    "FULL_EPOCHS = 100\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def _count_params(model: nn.Module, trainable_only: bool = False) -> int:\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def _ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def _is_minimize_study(study):\n",
    "    try:\n",
    "        return study.direction == optuna.study.StudyDirection.MINIMIZE\n",
    "    except Exception:\n",
    "        try:\n",
    "            return study.directions[0] == optuna.study.StudyDirection.MINIMIZE\n",
    "        except Exception:\n",
    "            return True  # fallback\n",
    "\n",
    "def primary_val_key_for_task(task_type: str):\n",
    "    t = task_type.lower()\n",
    "    if t == \"regression\":   # lower is better\n",
    "        return \"val_rmse\", True\n",
    "    if t == \"binary\":       # higher is better\n",
    "        return \"val_roc_auc\", False\n",
    "    if t == \"multiclass\":   # higher is better\n",
    "        return \"val_accuracy\", False\n",
    "    return \"val_loss\", True\n",
    "\n",
    "def _sort_trials(trials, minimize: bool):\n",
    "    return sorted(trials, key=lambda t: t.value, reverse=not minimize)\n",
    "\n",
    "def _metric_or_default(m: dict, key: str, minimize: bool):\n",
    "    if key in m and isinstance(m[key], (Number, np.floating, np.integer)):\n",
    "        return float(m[key])\n",
    "    return (np.inf if minimize else -np.inf)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# NEW: FLOPs/MACs helpers (hybrid-aware)\n",
    "# -----------------------------------------------------------------------------\n",
    "def _humanize(n: float, unit: str = \"\") -> str:\n",
    "    try:\n",
    "        n = float(n)\n",
    "        for u in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "            if abs(n) < 1000.0:\n",
    "                return f\"{n:.3f}{u}{unit}\"\n",
    "            n /= 1000.0\n",
    "        return f\"{n:.3f}E{unit}\"\n",
    "    except Exception:\n",
    "        return str(n)\n",
    "\n",
    "def _try_flops_single_input(model: nn.Module, input_shape):\n",
    "    \"\"\"\n",
    "    calflops on a single-input submodule (e.g., vit, tabular_mlp, fusion_mlp).\n",
    "    Returns dict or None.\n",
    "    \"\"\"\n",
    "    if not _HAVE_CALFLOPS:\n",
    "        return None\n",
    "    try:\n",
    "        flops, macs, params_cf = _calflops_calc(\n",
    "            model=model,\n",
    "            input_shape=tuple(int(x) for x in input_shape),\n",
    "            output_as_string=False\n",
    "        )\n",
    "        return {\n",
    "            \"flops\": float(flops),\n",
    "            \"macs\": float(macs),\n",
    "            \"params_from_calflops\": float(params_cf),\n",
    "            \"flops_str\": _humanize(flops),\n",
    "            \"macs_str\": _humanize(macs),\n",
    "            \"input_shape\": tuple(int(x) for x in input_shape),\n",
    "            \"tool\": \"calflops\"\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _probe_branch_dims(model, imgs_shape, attributes):\n",
    "    \"\"\"\n",
    "    Infer (visual_out_dim, tabular_out_dim) by a tiny no-grad forward.\n",
    "    Handles:\n",
    "      - ViTMLP: model.vit + model.tabular_mlp (may be empty)\n",
    "      - CNNMLP: model.backbone (with .feat_dim or forward) + model.tab_mlp (may be empty)\n",
    "      - Fallback: model.cnn if present\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        dev = next(model.parameters()).device\n",
    "\n",
    "        # Resolve image shape (C,H,W)\n",
    "        if isinstance(imgs_shape, (list, tuple)) and len(imgs_shape) >= 3:\n",
    "            C, H, W = int(imgs_shape[-3]), int(imgs_shape[-2]), int(imgs_shape[-1])\n",
    "        else:\n",
    "            C, H, W = 3, 224, 224\n",
    "\n",
    "        dummy_img = torch.zeros(1, C, H, W, device=dev)\n",
    "        dummy_tab = torch.zeros(1, int(attributes), device=dev)\n",
    "\n",
    "        # ---- Visual branch out dim ----\n",
    "        if hasattr(model, \"vit\"):\n",
    "            vis_feat = model.vit(dummy_img)\n",
    "            vis_out_dim = int(vis_feat.shape[1])\n",
    "        elif hasattr(model, \"backbone\"):\n",
    "            # Prefer cached feat_dim if provided by the backbone\n",
    "            if hasattr(model.backbone, \"feat_dim\"):\n",
    "                vis_out_dim = int(model.backbone.feat_dim)\n",
    "            else:\n",
    "                vis_feat = model.backbone(dummy_img)\n",
    "                vis_out_dim = int(vis_feat.shape[1])\n",
    "        elif hasattr(model, \"cnn\"):\n",
    "            vis_feat = model.cnn(dummy_img)\n",
    "            vis_out_dim = int(vis_feat.shape[1])\n",
    "        else:\n",
    "            raise RuntimeError(\"Visual branch not found (expected .vit, .backbone, or .cnn).\")\n",
    "\n",
    "        # ---- Tabular branch out dim ----\n",
    "        if hasattr(model, \"tab_mlp\"):\n",
    "            tab_mlp = model.tab_mlp\n",
    "            if isinstance(tab_mlp, nn.Sequential) and len(tab_mlp) == 0:\n",
    "                tab_out_dim = int(attributes)\n",
    "            else:\n",
    "                tab_feat = tab_mlp(dummy_tab)\n",
    "                tab_out_dim = int(tab_feat.shape[1])\n",
    "        else:\n",
    "            # No explicit tab MLP—assume raw tab features are used\n",
    "            tab_out_dim = int(attributes)\n",
    "\n",
    "    return vis_out_dim, tab_out_dim\n",
    "\n",
    "        \n",
    "\n",
    "def _try_compute_flops_hybrid(model, imgs_shape, attributes, vit_or_cnn_out_dim, tab_out_dim, batch_size: int = 1):\n",
    "    \"\"\"\n",
    "    Compute FLOPs/MACs for a hybrid model by summing three parts:\n",
    "      - Visual branch with image input (B,C,H,W)\n",
    "      - Tabular MLP with input (B, attributes)\n",
    "      - Fusion MLP with input (B, vit_or_cnn_out_dim + tab_out_dim)\n",
    "    Works for:\n",
    "      - ViTMLP:    .vit + .tabular_mlp + .fusion_mlp\n",
    "      - CNNMLP:    .backbone + .tab_mlp + .fusion\n",
    "    Returns a dict with totals and a per-part breakdown, or None if unavailable.\n",
    "    \"\"\"\n",
    "    if not _HAVE_CALFLOPS:\n",
    "        return None\n",
    "\n",
    "    # Resolve image shape (C,H,W)\n",
    "    if isinstance(imgs_shape, (list, tuple)) and len(imgs_shape) >= 3:\n",
    "        C, H, W = imgs_shape[-3], imgs_shape[-2], imgs_shape[-1]\n",
    "    else:\n",
    "        C, H, W = 3, 224, 224\n",
    "\n",
    "    # --- Visual module (ViT or CNN backbone)\n",
    "    vis_mod = None\n",
    "    if hasattr(model, \"vit\"):\n",
    "        vis_mod = model.vit\n",
    "    elif hasattr(model, \"backbone\"):\n",
    "        vis_mod = model.backbone\n",
    "    elif hasattr(model, \"cnn\"):\n",
    "        vis_mod = model.cnn  # if you ever expose a raw cnn module\n",
    "\n",
    "    vis_info = _try_flops_single_input(vis_mod, (batch_size, int(C), int(H), int(W))) if vis_mod is not None else None\n",
    "\n",
    "    # --- Tabular module\n",
    "    tab_mod = None\n",
    "    if hasattr(model, \"tabular_mlp\"):\n",
    "        tab_mod = model.tabular_mlp\n",
    "    elif hasattr(model, \"tab_mlp\"):\n",
    "        tab_mod = model.tab_mlp\n",
    "\n",
    "    tab_info = _try_flops_single_input(tab_mod, (batch_size, int(attributes))) if tab_mod is not None else None\n",
    "\n",
    "    # --- Fusion module\n",
    "    fusion_mod = None\n",
    "    if hasattr(model, \"fusion_mlp\"):\n",
    "        fusion_mod = model.fusion_mlp\n",
    "    elif hasattr(model, \"fusion\"):\n",
    "        fusion_mod = model.fusion\n",
    "\n",
    "    fused_in = int(vit_or_cnn_out_dim) + int(tab_out_dim)\n",
    "    fusion_info = _try_flops_single_input(fusion_mod, (batch_size, fused_in)) if fusion_mod is not None else None\n",
    "\n",
    "    parts = {\"vision\": vis_info, \"tabular\": tab_info, \"fusion\": fusion_info}\n",
    "    if not any(p is not None for p in parts.values()):\n",
    "        return None\n",
    "\n",
    "    total_flops = sum(p[\"flops\"] for p in parts.values() if p is not None)\n",
    "    total_macs  = sum(p[\"macs\"]  for p in parts.values() if p is not None)\n",
    "\n",
    "    return {\n",
    "        \"flops\": float(total_flops),\n",
    "        \"macs\": float(total_macs),\n",
    "        \"flops_str\": _humanize(total_flops),\n",
    "        \"macs_str\": _humanize(total_macs),\n",
    "        \"parts\": parts,\n",
    "        \"tool\": \"calflops(hybrid-sum)\",\n",
    "        \"inputs\": {\n",
    "            \"image_input_shape\": (batch_size, int(C), int(H), int(W)),\n",
    "            \"tabular_input_shape\": (batch_size, int(attributes)),\n",
    "            \"fusion_input_shape\":  (batch_size, fused_in),\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build + train + return metrics (with param counts + FLOPs) for a trial\n",
    "# -----------------------------------------------------------------------------\n",
    "def evaluate_best_model(\n",
    "    best_trial,\n",
    "    train_loader, val_loader, test_loader,\n",
    "    dataset_name, image_name, task_type,\n",
    "    save_dir, imgs_shape, attributes, trial_name,\n",
    "    class_weight=None, num_classes=None, epochs=10,\n",
    "    path_vision=None, path_mlp=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds model from the trial, counts params, trains, saves, and returns metrics.\n",
    "    - For ViT_hybrid: loads frozen best vision+MLP branches from path_vision/path_mlp; ONLY fusion comes from best_trial.\n",
    "    - Else (CNN+MLP hybrid): loads frozen best CNN from path_vision and MLP from path_mlp; ONLY fusion comes from best_trial.\n",
    "    \"\"\"\n",
    "    task = task_type.lower()\n",
    "    best_params = best_trial.params\n",
    "\n",
    "    print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "    print(f\"  Best Score: {best_trial.value:.4f}\")\n",
    "    print(\"  Best Hyperparameters:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"    {k}: {v}\")\n",
    "\n",
    "    # ---------------- Build model params ----------------\n",
    "    if model_name == \"ViT_hybrid\":\n",
    "        # fusion hyperparams that come from the trial\n",
    "        architecture_params = {\n",
    "            \"activation\": best_params[\"activation\"],\n",
    "            \"fusion_hidden_dims\": best_params[\"fusion_hidden_dims\"],\n",
    "            \"fusion_dropout\": best_params[\"fusion_dropout\"],\n",
    "        }\n",
    "\n",
    "        if (path_vision is None) or (path_mlp is None):\n",
    "            raise RuntimeError(\"ViT_hybrid requires path_vision and path_mlp.\")\n",
    "\n",
    "        # 1) Load frozen best ViT branch\n",
    "        with open(os.path.join(path_vision, \"best_params.json\"), \"r\") as f:\n",
    "            params_best_v = json.load(f)\n",
    "        params_best_v = {\n",
    "            k: v for k, v in params_best_v.items()\n",
    "            if k in [\"patch_size\", \"dim\", \"depth\", \"heads\", \"mlp_dim\", \"dropout\", \"emb_dropout\"]\n",
    "        }\n",
    "        params_best_v[\"vit_dropout\"]     = params_best_v.pop(\"dropout\")\n",
    "        params_best_v[\"vit_emb_dropout\"] = params_best_v.pop(\"emb_dropout\")\n",
    "\n",
    "        # 2) Load frozen best tabular MLP\n",
    "        with open(os.path.join(path_mlp, \"best_params.json\"), \"r\") as f:\n",
    "            params_best_t = json.load(f)\n",
    "        params_best_t = {\n",
    "            k: v for k, v in params_best_t.items()\n",
    "            if k in [\"mlp_hidden_dims\", \"dropout\", \"activation\"]\n",
    "        }\n",
    "        params_best_t[\"tab_dropout\"] = params_best_t.pop(\"dropout\")\n",
    "        if isinstance(params_best_t.get(\"mlp_hidden_dims\"), str):\n",
    "            params_best_t[\"mlp_hidden_dims\"] = json.loads(params_best_t[\"mlp_hidden_dims\"])\n",
    "\n",
    "        # merge\n",
    "        architecture_params = {**architecture_params, **params_best_v, **params_best_t}\n",
    "\n",
    "        # ensure lists are lists\n",
    "        architecture_params[\"fusion_hidden_dims\"] = json.loads(architecture_params[\"fusion_hidden_dims\"])\n",
    "\n",
    "        patch = architecture_params[\"patch_size\"]\n",
    "        model = ViTMLP(imgs_shape[1], attributes, architecture_params, task, num_classes)\n",
    "\n",
    "        # Fit params (tuned for fusion in hybrid HPO)\n",
    "        fit_params = {\n",
    "            \"max_lr\": best_params[\"max_lr\"],\n",
    "            \"div_factor\": best_params[\"div_factor\"],\n",
    "            \"final_div_factor\": best_params[\"final_div_factor\"],\n",
    "            \"weight_decay\": best_params[\"weight_decay\"],\n",
    "            \"pct_start\": best_params[\"pct_start\"],\n",
    "        }\n",
    "\n",
    "        # FLOPs dims: probe actual branch outputs\n",
    "        vit_or_cnn_out_dim, tab_out_dim = _probe_branch_dims(model, imgs_shape, attributes)\n",
    "\n",
    "        # ---------------- Count & save param stats ----------------\n",
    "        total_params = _count_params(model, trainable_only=False)\n",
    "        trainable_params = _count_params(model, trainable_only=True)\n",
    "        print(f\"  Params: total={total_params:,}  trainable={trainable_params:,}\")\n",
    "\n",
    "        base_dir = _ensure_dir(os.path.join(save_dir, f\"{model_name}/{image_name}/best_model/{trial_name}\"))\n",
    "\n",
    "        # ---------------- NEW: FLOPs/MACs (hybrid breakdown, saved; no prints) ----------------\n",
    "        flops_info = _try_compute_flops_hybrid(\n",
    "            model, imgs_shape=imgs_shape, attributes=attributes,\n",
    "            vit_or_cnn_out_dim=vit_or_cnn_out_dim, tab_out_dim=tab_out_dim, batch_size=1\n",
    "        )\n",
    "\n",
    "        # Save compact stats + compute summary\n",
    "        model_stats = {\n",
    "            \"total_params\": int(total_params),\n",
    "            \"trainable_params\": int(trainable_params),\n",
    "            \"architecture_params\": architecture_params,\n",
    "        }\n",
    "        if flops_info is not None:\n",
    "            model_stats.update({\n",
    "                \"flops\": flops_info[\"flops\"],\n",
    "                \"macs\": flops_info[\"macs\"],\n",
    "                \"flops_str\": flops_info[\"flops_str\"],\n",
    "                \"macs_str\": flops_info[\"macs_str\"],\n",
    "            })\n",
    "        with open(os.path.join(base_dir, \"model_stats.json\"), \"w\") as f:\n",
    "            json.dump(model_stats, f, indent=4)\n",
    "\n",
    "        # Save detailed FLOPs breakdown\n",
    "        if flops_info is not None:\n",
    "            with open(os.path.join(base_dir, \"flops_details.json\"), \"w\") as f:\n",
    "                json.dump(flops_info, f, indent=4)\n",
    "\n",
    "    else:\n",
    "        # CNN+MLP hybrid (vision = CNN; tabular = MLP). Fusion hyperparams from trial.\n",
    "        architecture_params = {\n",
    "            \"activation\": best_params[\"activation\"],\n",
    "            \"fusion_hidden_dims\": best_params[\"fusion_hidden_dims\"],\n",
    "            \"fusion_dropout\": best_params[\"fusion_dropout\"],\n",
    "        }\n",
    "        \n",
    "        if (path_vision is None) or (path_mlp is None):\n",
    "            raise RuntimeError(\"CNN hybrid requires path_vision and path_mlp.\")\n",
    "\n",
    "        # 1) Frozen best CNN\n",
    "        with open(os.path.join(path_vision, \"best_params.json\"), \"r\") as f:\n",
    "            params_best_cnn = json.load(f)\n",
    "        params_best_cnn = {\n",
    "            k: v for k, v in params_best_cnn.items()\n",
    "            if k in [\"in_channels\", \"activation\", \"stem_type\", \"use_maxpool\", \"stem_width\",\n",
    "                     \"n_stages\", \"blocks_per_stage\", \"base_width\", \"width_mul\"]\n",
    "        }\n",
    "\n",
    "        # 2) Frozen best MLP (tabular)\n",
    "        with open(os.path.join(path_mlp, \"best_params.json\"), \"r\") as f:\n",
    "            params_best_t = json.load(f)\n",
    "        params_best_t = {\n",
    "            k: v for k, v in params_best_t.items()\n",
    "            if k in [\"mlp_hidden_dims\", \"dropout\", \"activation\"]\n",
    "        }\n",
    "        params_best_t[\"tab_dropout\"] = params_best_t.pop(\"dropout\")\n",
    "        if isinstance(params_best_t.get(\"mlp_hidden_dims\"), str):\n",
    "            params_best_t[\"mlp_hidden_dims\"] = json.loads(params_best_t[\"mlp_hidden_dims\"])\n",
    "\n",
    "        # merge\n",
    "        architecture_params = {**architecture_params, **params_best_cnn, **params_best_t}\n",
    "\n",
    "        # ensure lists are lists\n",
    "        architecture_params[\"fusion_hidden_dims\"] = json.loads(architecture_params[\"fusion_hidden_dims\"])\n",
    "        architecture_params[\"blocks_per_stage\"] = json.loads(architecture_params[\"blocks_per_stage\"])\n",
    "\n",
    "        patch = \"\"\n",
    "        model = CNNMLP(imgs_shape, attributes, architecture_params, task, num_classes)\n",
    "\n",
    "        fit_params = {\n",
    "            \"max_lr\": best_params[\"max_lr\"],\n",
    "            \"div_factor\": best_params[\"div_factor\"],\n",
    "            \"final_div_factor\": best_params[\"final_div_factor\"],\n",
    "            \"weight_decay\": best_params[\"weight_decay\"],\n",
    "            \"pct_start\": best_params[\"pct_start\"],\n",
    "        }\n",
    "\n",
    "        # FLOPs dims: probe actual branch outputs\n",
    "        vit_or_cnn_out_dim, tab_out_dim = _probe_branch_dims(model, imgs_shape, attributes)\n",
    "\n",
    "        # ---------------- Count & save param stats ----------------\n",
    "        total_params = _count_params(model, trainable_only=False)\n",
    "        trainable_params = _count_params(model, trainable_only=True)\n",
    "        print(f\"  Params: total={total_params:,}  trainable={trainable_params:,}\")\n",
    "\n",
    "        base_dir = _ensure_dir(os.path.join(save_dir, f\"{model_name}/{image_name}/best_model/{trial_name}\"))\n",
    "\n",
    "        # ---------------- NEW: FLOPs/MACs (hybrid breakdown, saved; no prints) ----------------\n",
    "        flops_info = _try_compute_flops_hybrid(\n",
    "            model, imgs_shape=imgs_shape, attributes=attributes,\n",
    "            vit_or_cnn_out_dim=vit_or_cnn_out_dim, tab_out_dim=tab_out_dim, batch_size=1\n",
    "        )\n",
    "\n",
    "        # Save compact stats + compute summary\n",
    "        model_stats = {\n",
    "            \"total_params\": int(total_params),\n",
    "            \"trainable_params\": int(trainable_params),\n",
    "            \"architecture_params\": architecture_params,\n",
    "        }\n",
    "        if flops_info is not None:\n",
    "            model_stats.update({\n",
    "                \"flops\": flops_info[\"flops\"],\n",
    "                \"macs\": flops_info[\"macs\"],\n",
    "                \"flops_str\": flops_info[\"flops_str\"],\n",
    "                \"macs_str\": flops_info[\"macs_str\"],\n",
    "            })\n",
    "        with open(os.path.join(base_dir, \"model_stats.json\"), \"w\") as f:\n",
    "            json.dump(model_stats, f, indent=4)\n",
    "\n",
    "        # Save detailed FLOPs breakdown\n",
    "        if flops_info is not None:\n",
    "            with open(os.path.join(base_dir, \"flops_details.json\"), \"w\") as f:\n",
    "                json.dump(flops_info, f, indent=4)\n",
    "\n",
    "    # ---------------- Train & evaluate ----------------\n",
    "    metrics = compile_and_fit(\n",
    "        model,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        image_name=image_name,\n",
    "        model_name=model_name,\n",
    "        trial_name=trial_name,\n",
    "        task=task,\n",
    "        max_lr=fit_params[\"max_lr\"],\n",
    "        div_factor=fit_params[\"div_factor\"],\n",
    "        final_div_factor=fit_params[\"final_div_factor\"],\n",
    "        weight_decay=fit_params[\"weight_decay\"],\n",
    "        pct_start=fit_params[\"pct_start\"],\n",
    "        epochs=epochs,\n",
    "        save_model=True,\n",
    "        class_weights=class_weight,\n",
    "        save_dir=save_dir,\n",
    "        patch=(architecture_params[\"patch_size\"] if model_name == \"ViT_hybrid\" else \"\")\n",
    "    )\n",
    "\n",
    "    # Augment metrics with parameter counts + (if available) FLOPs\n",
    "    metrics[\"total_params\"] = int(total_params)\n",
    "    metrics[\"trainable_params\"] = int(trainable_params)\n",
    "    if flops_info is not None:\n",
    "        metrics[\"flops\"] = flops_info[\"flops\"]\n",
    "        metrics[\"macs\"] = flops_info[\"macs\"]\n",
    "        metrics[\"flops_str\"] = flops_info[\"flops_str\"]\n",
    "        metrics[\"macs_str\"] = flops_info[\"macs_str\"]\n",
    "    return metrics\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Top-K → single-pass → winner → multi-seed driver\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_topk_and_multiseed(\n",
    "    study, model_name, dataset_name, name, task_type, save_dir,\n",
    "    imgs_shape, attributes, num_classes, class_weight,\n",
    "    train_loader, val_loader, test_loader,\n",
    "    path_vision=None, path_mlp=None,\n",
    "):\n",
    "    # Load patch_size from the frozen best ViT (used only for printing headers in hybrid)\n",
    "    if model_name == \"ViT_hybrid\":\n",
    "        with open(os.path.join(path_vision, \"best_params.json\"), \"r\") as f:\n",
    "            params_best_vit = json.load(f)\n",
    "        patch_size = params_best_vit[\"patch_size\"]\n",
    "    else:\n",
    "        patch_size = None\n",
    "\n",
    "    minimize = _is_minimize_study(study)\n",
    "    completed = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    if not completed:\n",
    "        raise RuntimeError(\"No completed trials in the study.\")\n",
    "\n",
    "    top_trials = _sort_trials(completed, minimize)[:TOP_K]\n",
    "    primary_key, primary_minimize = primary_val_key_for_task(task_type)\n",
    "    maximize_primary = not primary_minimize\n",
    "\n",
    "    print(f\"\\nEvaluating top-{len(top_trials)} trials once at {FULL_EPOCHS} epochs (seed={SINGLE_PASS_SEED})...\\n\")\n",
    "\n",
    "    # Single-pass over top-K\n",
    "    single_pass_results = []  # list of (trial, trial_name, metrics_dict)\n",
    "    for trial in top_trials:\n",
    "        if model_name == \"ViT_hybrid\":\n",
    "            trial_name = f\"trial_{trial.number}_patch{patch_size}\"\n",
    "            header = f\"(Trial {trial.number}, ValObjective: {trial.value:.4f}, patch_size={patch_size})\"\n",
    "        else:\n",
    "            trial_name = f\"trial_{trial.number}\"\n",
    "            header = f\"(Trial {trial.number}, ValObjective: {trial.value:.4f})\"\n",
    "\n",
    "        print(f\"→ Single-pass full run {header}\")\n",
    "        set_model_seed(SINGLE_PASS_SEED)\n",
    "\n",
    "        metrics = evaluate_best_model(\n",
    "            best_trial=trial,\n",
    "            train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "            dataset_name=dataset_name,\n",
    "            image_name=name,\n",
    "            task_type=task_type,\n",
    "            save_dir=save_dir,\n",
    "            imgs_shape=imgs_shape,\n",
    "            attributes=attributes,\n",
    "            class_weight=class_weight,\n",
    "            num_classes=num_classes,\n",
    "            epochs=FULL_EPOCHS,\n",
    "            trial_name=trial_name,\n",
    "            path_vision=path_vision, path_mlp=path_mlp\n",
    "        )\n",
    "        if not isinstance(metrics, dict):\n",
    "            raise TypeError(f\"evaluate_best_model must return dict, got: {type(metrics)}\")\n",
    "\n",
    "        # brief printout (metric + params)\n",
    "        if primary_key in metrics:\n",
    "            print(f\"   {primary_key}={float(metrics[primary_key]):.6f}\")\n",
    "        tp = metrics.get(\"total_params\"); trp = metrics.get(\"trainable_params\")\n",
    "        if tp is not None:\n",
    "            print(f\"   params: total={tp:,}, trainable={trp:,}\")\n",
    "        single_pass_results.append((trial, trial_name, metrics))\n",
    "\n",
    "    # Winner by primary metric\n",
    "    if maximize_primary:\n",
    "        winner_tuple = max(single_pass_results, key=lambda x: _metric_or_default(x[2], primary_key, primary_minimize))\n",
    "    else:\n",
    "        winner_tuple = min(single_pass_results, key=lambda x: _metric_or_default(x[2], primary_key, primary_minimize))\n",
    "\n",
    "    best_trial, best_trial_name, best_single_metrics = winner_tuple\n",
    "    best_primary_val = _metric_or_default(best_single_metrics, primary_key, primary_minimize)\n",
    "    print(f\"\\nWinner after single-pass: Trial {best_trial.number} ({best_trial_name}) \"\n",
    "          f\"by {primary_key}={best_primary_val:.6f}\")\n",
    "\n",
    "    # Multi-seed evaluation of winner\n",
    "    print(f\"\\nRe-running winner with seeds {FINAL_SEEDS} at {FULL_EPOCHS} epochs...\\n\")\n",
    "    winner_save_path = _ensure_dir(os.path.join(save_dir, f\"{model_name}/{name}/best_model/{best_trial_name}\"))\n",
    "\n",
    "    per_seed_metrics = []\n",
    "    numeric_keys = None\n",
    "\n",
    "    for s in FINAL_SEEDS:\n",
    "        set_model_seed(s)\n",
    "        m = evaluate_best_model(\n",
    "            best_trial=best_trial,\n",
    "            train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "            dataset_name=dataset_name,\n",
    "            image_name=name,\n",
    "            task_type=task_type,\n",
    "            save_dir=save_dir,\n",
    "            imgs_shape=imgs_shape,\n",
    "            attributes=attributes,\n",
    "            class_weight=class_weight,\n",
    "            num_classes=num_classes,\n",
    "            epochs=FULL_EPOCHS,\n",
    "            trial_name=f\"{best_trial_name}_seed{s}\",\n",
    "            path_vision=path_vision, path_mlp=path_mlp\n",
    "        )\n",
    "        if not isinstance(m, dict):\n",
    "            raise TypeError(f\"evaluate_best_model must return dict, got: {type(m)}\")\n",
    "\n",
    "        if numeric_keys is None:\n",
    "            numeric_keys = [k for k, v in m.items() if isinstance(v, (Number, np.floating, np.integer))]\n",
    "        per_seed_metrics.append(m)\n",
    "\n",
    "        # quick line\n",
    "        pk_val = _metric_or_default(m, primary_key, primary_minimize)\n",
    "        extras = []\n",
    "        for k in [\"test_loss\", \"test_accuracy\", \"test_roc_auc\", \"test_rmse\", \"val_loss\"]:\n",
    "            if k in m and isinstance(m[k], (Number, np.floating, np.integer)):\n",
    "                extras.append(f\"{k}={float(m[k]):.6f}\")\n",
    "        print(f\"   Seed {s}: {primary_key}={pk_val:.6f}\" + (\", \" + \", \".join(extras) if extras else \"\"))\n",
    "\n",
    "    # Aggregate across seeds\n",
    "    aggregates = {}\n",
    "    for k in (numeric_keys or []):\n",
    "        vals = [float(m[k]) for m in per_seed_metrics if k in m]\n",
    "        if not vals:\n",
    "            continue\n",
    "        mean_k = float(np.mean(vals))\n",
    "        std_k = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "        aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "    # Param counts (same across seeds) — take from single-pass winner metrics\n",
    "    winner_total_params = best_single_metrics.get(\"total_params\")\n",
    "    winner_train_params = best_single_metrics.get(\"trainable_params\")\n",
    "    # Optional: FLOPs (if evaluate saved them in metrics)\n",
    "    winner_flops_str = best_single_metrics.get(\"flops_str\")\n",
    "    winner_macs_str  = best_single_metrics.get(\"macs_str\")\n",
    "    winner_flops_num = best_single_metrics.get(\"flops\")\n",
    "    winner_macs_num  = best_single_metrics.get(\"macs\")\n",
    "\n",
    "    # Save summary (includes compute if available)\n",
    "    out_file = os.path.join(winner_save_path, \"winner_multi_seed_summary.txt\")\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# Final winner multi-seed evaluation\\n\")\n",
    "        if model_name == \"ViT_hybrid\":\n",
    "            f.write(f\"patch_size: {patch_size}\\n\")\n",
    "        f.write(f\"trial_number: {best_trial.number}\\n\")\n",
    "        f.write(f\"primary_metric: {primary_key}\\n\")\n",
    "        f.write(f\"seeds: {FINAL_SEEDS}\\n\")\n",
    "        if (winner_total_params is not None) or (winner_train_params is not None):\n",
    "            f.write(\"model_size:\\n\")\n",
    "            if winner_total_params is not None:\n",
    "                f.write(f\"  total_params: {winner_total_params}\\n\")\n",
    "            if winner_train_params is not None:\n",
    "                f.write(f\"  trainable_params: {winner_train_params}\\n\")\n",
    "        if winner_flops_str is not None and winner_macs_str is not None:\n",
    "            f.write(\"compute:\\n\")\n",
    "            f.write(f\"  flops: {winner_flops_str}\\n\")\n",
    "            f.write(f\"  macs: {winner_macs_str}\\n\")\n",
    "            if winner_flops_num is not None and winner_macs_num is not None:\n",
    "                f.write(f\"  flops_num: {winner_flops_num}\\n\")\n",
    "                f.write(f\"  macs_num: {winner_macs_num}\\n\")\n",
    "        f.write(\"per_seed_metrics:\\n\")\n",
    "        for s, m in zip(FINAL_SEEDS, per_seed_metrics):\n",
    "            f.write(f\"  - seed: {s}\\n\")\n",
    "            for k in (numeric_keys or []):\n",
    "                if k in m:\n",
    "                    f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "        f.write(\"aggregates:\\n\")\n",
    "        for k, mm in aggregates.items():\n",
    "            f.write(f\"  {k}:\\n\")\n",
    "            f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "            f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "    # Console summary\n",
    "    if primary_key in aggregates:\n",
    "        print(f\"\\nWinner aggregated {primary_key}: {aggregates[primary_key]['mean']:.6f} \"\n",
    "              f\"± {aggregates[primary_key]['std']:.6f}\")\n",
    "    elif \"val_loss\" in aggregates:\n",
    "        print(f\"\\nWinner aggregated val_loss: {aggregates['val_loss']['mean']:.6f} \"\n",
    "              f\"± {aggregates['val_loss']['std']:.6f}\")\n",
    "    if winner_total_params is not None:\n",
    "        print(f\"Model params: total={winner_total_params:,}, trainable={winner_train_params:,}\")\n",
    "    print(f\"Saved multi-seed summary to: {out_file}\")\n",
    "\n",
    "    # Return winner identifiers & aggregates for downstream use\n",
    "    return {\n",
    "        \"winner_trial_number\": best_trial.number,\n",
    "        \"winner_trial_name\": best_trial_name,\n",
    "        \"primary_metric\": primary_key,\n",
    "        \"aggregates\": aggregates,\n",
    "        \"total_params\": winner_total_params,\n",
    "        \"trainable_params\": winner_train_params,\n",
    "        \"flops\": winner_flops_num,\n",
    "        \"macs\": winner_macs_num,\n",
    "        \"summary_path\": out_file,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def reduce_dataloader(train_loader, fraction=0.25, stratify=True, seed=42):\n",
    "    \"\"\"\n",
    "    Return a new DataLoader that draws from ~fraction of the original train dataset.\n",
    "    For classification (TensorDataset(..., y)), uses a stratified subsample.\n",
    "    \"\"\"\n",
    "    assert 0 < fraction <= 1.0\n",
    "    ds = train_loader.dataset\n",
    "    n = len(ds)\n",
    "    num_keep = max(1, int(round(n * fraction)))\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    # Try stratified pick if labels are available (TensorDataset last tensor is y)\n",
    "    subset_idx = None\n",
    "    if stratify and hasattr(ds, \"tensors\") and len(ds.tensors) >= 2:\n",
    "        y = ds.tensors[-1].cpu().numpy().ravel()\n",
    "        try:\n",
    "            from sklearn.model_selection import StratifiedShuffleSplit\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, train_size=fraction, random_state=seed)\n",
    "            chosen, _ = next(sss.split(idx, y))\n",
    "            subset_idx = idx[chosen]\n",
    "        except Exception:\n",
    "            subset_idx = None  # fallback to random below\n",
    "\n",
    "    # Fallback: random subset with a fixed seed\n",
    "    if subset_idx is None:\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        subset_idx = torch.randperm(n, generator=g)[:num_keep].tolist()\n",
    "\n",
    "    # Build subset dataset and a new DataLoader (reuse original loader settings)\n",
    "    subset = Subset(ds, subset_idx)  # official Subset utility\n",
    "    new_loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=train_loader.batch_size,\n",
    "        shuffle=True,                               # shuffle within the subset\n",
    "        num_workers=getattr(train_loader, \"num_workers\", 0),\n",
    "        pin_memory=getattr(train_loader, \"pin_memory\", False),\n",
    "        drop_last=getattr(train_loader, \"drop_last\", False),\n",
    "        persistent_workers=getattr(train_loader, \"persistent_workers\", False),\n",
    "    )\n",
    "    return new_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"ViT_hybrid\":\n",
    "    vision_name = \"vit\"\n",
    "else:\n",
    "    vision_name = \"CNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT: TINTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "name = f\"TINTO_blur\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes — Train: (862, 72), Val: (185, 72), Test: (185, 72)\n",
      "Numerical features: 8 — ['Year', 'RA', 'W', 'OBP', 'SLG', 'BA', 'OOBP', 'OSLG']\n",
      "Categorical features: 6 — ['Team', 'League', 'Playoffs', 'RankSeason', 'RankPlayoffs', 'G']\n",
      "Total features: 72\n",
      "Images shape (C,H,W): (3, 20, 20)\n",
      "Attributes: 72\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 5, 10, 20]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = [2, 4, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vision=f\"./logs/{task_type}/{dataset_name}/{vision_name}/{name}/best_model/trial_11\"\n",
    "path_mlp=f\"./logs/{task_type}/{dataset_name}/mlp/best_model/trial_38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-16 01:39:38,819] A new study created in memory with name: no-name-66e44275-5d5b-46c1-b6e2-2f687d8290db\n",
      "[I 2025-12-16 01:39:57,044] Trial 0 finished with value: 24.34951592498812 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.23590550280075764, 'max_lr': 0.001673273132425617, 'div_factor': 475, 'final_div_factor': 497, 'weight_decay': 0.0022338035414797485, 'pct_start': 0.2204038817190335}. Best is trial 0 with value: 24.34951592498812.\n",
      "[I 2025-12-16 01:40:13,855] Trial 1 finished with value: 46.55179721792436 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.04348186011664775, 'max_lr': 5.758367890492339e-05, 'div_factor': 871, 'final_div_factor': 744, 'weight_decay': 0.00045543363743479363, 'pct_start': 0.356977655318339}. Best is trial 0 with value: 24.34951592498812.\n",
      "[I 2025-12-16 01:40:31,605] Trial 2 finished with value: 21.951059872840208 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.18283192435965834, 'max_lr': 0.008496367913192639, 'div_factor': 563, 'final_div_factor': 204, 'weight_decay': 0.000983112528256884, 'pct_start': 0.22272509456073408}. Best is trial 2 with value: 21.951059872840208.\n",
      "[I 2025-12-16 01:40:48,608] Trial 3 finished with value: 30.437241320127676 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.29470763816472345, 'max_lr': 0.0010324619087294096, 'div_factor': 73, 'final_div_factor': 212, 'weight_decay': 4.875933664441849e-05, 'pct_start': 0.3642913785192471}. Best is trial 2 with value: 21.951059872840208.\n",
      "[I 2025-12-16 01:41:05,158] Trial 4 finished with value: 35.17710666055144 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.1311411275722334, 'max_lr': 5.262743436454288e-05, 'div_factor': 907, 'final_div_factor': 817, 'weight_decay': 0.000436575720850675, 'pct_start': 0.2958041259942378}. Best is trial 2 with value: 21.951059872840208.\n",
      "[I 2025-12-16 01:41:22,103] Trial 5 finished with value: 703.9036244401644 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.06913689750164619, 'max_lr': 1.5267763912118693e-05, 'div_factor': 732, 'final_div_factor': 454, 'weight_decay': 0.003915081310410044, 'pct_start': 0.20354310549757654}. Best is trial 2 with value: 21.951059872840208.\n",
      "[I 2025-12-16 01:41:38,536] Trial 6 finished with value: 474.2686969166318 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.05038953170538296, 'max_lr': 1.9131883079360545e-05, 'div_factor': 396, 'final_div_factor': 137, 'weight_decay': 0.0033837890339537997, 'pct_start': 0.10750332776649238}. Best is trial 2 with value: 21.951059872840208.\n",
      "[I 2025-12-16 01:41:54,320] Trial 7 finished with value: 21.85410134518161 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.010551167716428821, 'max_lr': 0.004344932638001145, 'div_factor': 917, 'final_div_factor': 761, 'weight_decay': 0.007546545752378669, 'pct_start': 0.24508675691489537}. Best is trial 7 with value: 21.85410134518161.\n",
      "[I 2025-12-16 01:42:10,705] Trial 8 finished with value: 24.489667932604124 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.17375386654305913, 'max_lr': 0.0039023448258017, 'div_factor': 30, 'final_div_factor': 390, 'weight_decay': 0.001179860366857723, 'pct_start': 0.19978179698852433}. Best is trial 7 with value: 21.85410134518161.\n",
      "[I 2025-12-16 01:42:27,579] Trial 9 finished with value: 25.49130878810401 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.16260457119479416, 'max_lr': 0.0003951891389358282, 'div_factor': 534, 'final_div_factor': 284, 'weight_decay': 0.00011190534463165601, 'pct_start': 0.10975246594061347}. Best is trial 7 with value: 21.85410134518161.\n",
      "[I 2025-12-16 01:42:43,530] Trial 10 finished with value: 37.14517187933463 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.00032606275929819746, 'max_lr': 0.0002787716563630703, 'div_factor': 278, 'final_div_factor': 669, 'weight_decay': 2.008036309134626e-06, 'pct_start': 0.29334232238381097}. Best is trial 7 with value: 21.85410134518161.\n",
      "[I 2025-12-16 01:43:00,260] Trial 11 finished with value: 21.97124960775267 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.21460145472286407, 'max_lr': 0.009557423531249639, 'div_factor': 682, 'final_div_factor': 893, 'weight_decay': 0.007808634560265438, 'pct_start': 0.267059341597368}. Best is trial 7 with value: 21.85410134518161.\n",
      "[I 2025-12-16 01:43:17,148] Trial 12 finished with value: 22.196921140227282 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.10351793134851116, 'max_lr': 0.008195940614540655, 'div_factor': 699, 'final_div_factor': 979, 'weight_decay': 7.227712015359058e-05, 'pct_start': 0.16224474227397437}. Best is trial 7 with value: 21.85410134518161.\n",
      "[I 2025-12-16 01:43:34,325] Trial 13 finished with value: 21.587652336426892 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.2143070446424099, 'max_lr': 0.0024655550095200205, 'div_factor': 987, 'final_div_factor': 671, 'weight_decay': 0.0006948046340169658, 'pct_start': 0.2547225422286914}. Best is trial 13 with value: 21.587652336426892.\n",
      "[I 2025-12-16 01:43:50,780] Trial 14 finished with value: 21.865948880828025 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.2631446097600346, 'max_lr': 0.0019338980147834625, 'div_factor': 958, 'final_div_factor': 643, 'weight_decay': 1.092228928182316e-05, 'pct_start': 0.3052683748704123}. Best is trial 13 with value: 21.587652336426892.\n",
      "[I 2025-12-16 01:44:08,184] Trial 15 finished with value: 23.708108356751467 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.005427386406548191, 'max_lr': 0.0005366222902169373, 'div_factor': 843, 'final_div_factor': 613, 'weight_decay': 0.007547750936772207, 'pct_start': 0.25063378493774446}. Best is trial 13 with value: 21.587652336426892.\n",
      "[I 2025-12-16 01:44:25,755] Trial 16 finished with value: 22.66440720609975 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.11433023241271495, 'max_lr': 0.0027647735804086507, 'div_factor': 995, 'final_div_factor': 768, 'weight_decay': 0.00023725648036013866, 'pct_start': 0.16191715822013225}. Best is trial 13 with value: 21.587652336426892.\n",
      "[I 2025-12-16 01:44:44,202] Trial 17 finished with value: 27.969110071006238 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.21172278460122176, 'max_lr': 0.0001581531654409783, 'div_factor': 780, 'final_div_factor': 575, 'weight_decay': 0.0013344045050314984, 'pct_start': 0.3343909874775969}. Best is trial 13 with value: 21.587652336426892.\n",
      "[I 2025-12-16 01:45:01,961] Trial 18 finished with value: 22.75470928035763 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.25768141222073704, 'max_lr': 0.0009912839063982214, 'div_factor': 1000, 'final_div_factor': 879, 'weight_decay': 1.5437053914496024e-05, 'pct_start': 0.39251453982862694}. Best is trial 13 with value: 21.587652336426892.\n",
      "[I 2025-12-16 01:45:19,215] Trial 19 finished with value: 21.33809219369897 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.09589750173651276, 'max_lr': 0.00545129493081012, 'div_factor': 819, 'final_div_factor': 728, 'weight_decay': 0.0005278046995253924, 'pct_start': 0.26424996403715356}. Best is trial 19 with value: 21.33809219369897.\n",
      "[I 2025-12-16 01:45:36,893] Trial 20 finished with value: 24.48313852972337 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.08993514406048161, 'max_lr': 0.0009727929307746657, 'div_factor': 621, 'final_div_factor': 991, 'weight_decay': 0.0002550618651125338, 'pct_start': 0.272756640867554}. Best is trial 19 with value: 21.33809219369897.\n",
      "[I 2025-12-16 01:45:54,287] Trial 21 finished with value: 21.618239917235183 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.026232805737830142, 'max_lr': 0.004288364161495009, 'div_factor': 806, 'final_div_factor': 711, 'weight_decay': 0.0008242759919734417, 'pct_start': 0.23911178718844353}. Best is trial 19 with value: 21.33809219369897.\n",
      "[I 2025-12-16 01:46:12,779] Trial 22 finished with value: 21.299057286851777 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.13869830193599034, 'max_lr': 0.0047229973025830025, 'div_factor': 807, 'final_div_factor': 705, 'weight_decay': 0.0005705036928566149, 'pct_start': 0.16768226950744392}. Best is trial 22 with value: 21.299057286851777.\n",
      "[I 2025-12-16 01:46:31,458] Trial 23 finished with value: 22.11857278865504 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.13642988048543325, 'max_lr': 0.005184911060922269, 'div_factor': 808, 'final_div_factor': 503, 'weight_decay': 0.00015981552163782113, 'pct_start': 0.18212769127548256}. Best is trial 22 with value: 21.299057286851777.\n",
      "[I 2025-12-16 01:46:54,918] Trial 24 finished with value: 22.400668461226008 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.19628244364429476, 'max_lr': 0.0022136405739081985, 'div_factor': 641, 'final_div_factor': 836, 'weight_decay': 3.577415694014191e-05, 'pct_start': 0.1279450657415854}. Best is trial 22 with value: 21.299057286851777.\n",
      "[I 2025-12-16 01:47:17,198] Trial 25 finished with value: 22.782237257843143 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.149123655852925, 'max_lr': 0.0013726694765298629, 'div_factor': 749, 'final_div_factor': 688, 'weight_decay': 0.0005482470433876158, 'pct_start': 0.27358297831612444}. Best is trial 22 with value: 21.299057286851777.\n",
      "[I 2025-12-16 01:47:38,193] Trial 26 finished with value: 21.569037731708733 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.07055983744018862, 'max_lr': 0.0028839084962480957, 'div_factor': 878, 'final_div_factor': 556, 'weight_decay': 0.00025008814055081554, 'pct_start': 0.3235443382475256}. Best is trial 22 with value: 21.299057286851777.\n",
      "[I 2025-12-16 01:48:00,966] Trial 27 finished with value: 20.696256968114653 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.07751887561997378, 'max_lr': 0.0056748821295034306, 'div_factor': 407, 'final_div_factor': 382, 'weight_decay': 0.0003117192337143475, 'pct_start': 0.3351180030040058}. Best is trial 27 with value: 20.696256968114653.\n",
      "[I 2025-12-16 01:48:21,286] Trial 28 finished with value: 21.664177306538566 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.09029383163359314, 'max_lr': 0.006654940405691587, 'div_factor': 317, 'final_div_factor': 352, 'weight_decay': 0.001805566005486021, 'pct_start': 0.3989024243101644}. Best is trial 27 with value: 20.696256968114653.\n",
      "[I 2025-12-16 01:48:40,655] Trial 29 finished with value: 23.977267625669292 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.12020652252442117, 'max_lr': 0.0006228447431675522, 'div_factor': 401, 'final_div_factor': 458, 'weight_decay': 2.421357560708904e-05, 'pct_start': 0.14456910109359186}. Best is trial 27 with value: 20.696256968114653.\n",
      "[I 2025-12-16 01:48:59,965] Trial 30 finished with value: 29.606091094280117 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.07176294779045832, 'max_lr': 0.000175686795299613, 'div_factor': 183, 'final_div_factor': 341, 'weight_decay': 0.003063654091003197, 'pct_start': 0.3458982941751181}. Best is trial 27 with value: 20.696256968114653.\n",
      "[I 2025-12-16 01:49:19,753] Trial 31 finished with value: 20.76421926087451 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.0671715477984629, 'max_lr': 0.003283469175981608, 'div_factor': 454, 'final_div_factor': 570, 'weight_decay': 0.0002568685276166657, 'pct_start': 0.30709784035026777}. Best is trial 27 with value: 20.696256968114653.\n",
      "[I 2025-12-16 01:49:39,880] Trial 32 finished with value: 20.97509087598572 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.046728586608383874, 'max_lr': 0.005688825153497561, 'div_factor': 481, 'final_div_factor': 593, 'weight_decay': 0.0003987351106906617, 'pct_start': 0.31457423593349043}. Best is trial 27 with value: 20.696256968114653.\n",
      "[I 2025-12-16 01:49:59,516] Trial 33 finished with value: 21.367058243439025 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.051215133052229106, 'max_lr': 0.0015125027520224259, 'div_factor': 421, 'final_div_factor': 513, 'weight_decay': 0.00013623416184720935, 'pct_start': 0.37041199662949975}. Best is trial 27 with value: 20.696256968114653.\n",
      "[I 2025-12-16 01:50:19,105] Trial 34 finished with value: 20.178577447207935 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.03362183376308595, 'max_lr': 0.009923163465679907, 'div_factor': 449, 'final_div_factor': 610, 'weight_decay': 0.0003778396434377797, 'pct_start': 0.31936554705694625}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:50:38,730] Trial 35 finished with value: 21.138420582130333 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.04788240767148295, 'max_lr': 0.00810455223587251, 'div_factor': 473, 'final_div_factor': 421, 'weight_decay': 6.969965153114574e-05, 'pct_start': 0.3205807191016027}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:50:58,667] Trial 36 finished with value: 21.62120702036725 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.028907719303530248, 'max_lr': 0.0031576227821123967, 'div_factor': 333, 'final_div_factor': 601, 'weight_decay': 0.00029339404345264094, 'pct_start': 0.3073133810417237}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:51:18,884] Trial 37 finished with value: 21.727405256039116 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.027306593854374124, 'max_lr': 0.006639256536408485, 'div_factor': 480, 'final_div_factor': 527, 'weight_decay': 0.00036276291596031214, 'pct_start': 0.35937749344643793}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:51:38,897] Trial 38 finished with value: 36.10808697940809 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.0628698806178715, 'max_lr': 5.964904494248575e-05, 'div_factor': 578, 'final_div_factor': 266, 'weight_decay': 0.00015965944593096705, 'pct_start': 0.37833492235890265}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:51:59,505] Trial 39 finished with value: 22.033349855602417 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.03463403985468887, 'max_lr': 0.00966306048969454, 'div_factor': 212, 'final_div_factor': 460, 'weight_decay': 7.316624461135039e-05, 'pct_start': 0.3461179074495016}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:52:19,573] Trial 40 finished with value: 21.192581692630753 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.017479000261836436, 'max_lr': 0.003863552176837479, 'div_factor': 442, 'final_div_factor': 593, 'weight_decay': 0.0019597818255159298, 'pct_start': 0.2914676922165697}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:52:39,432] Trial 41 finished with value: 20.572581501093648 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.046947804526090556, 'max_lr': 0.005946275169201334, 'div_factor': 506, 'final_div_factor': 418, 'weight_decay': 8.735118629939312e-05, 'pct_start': 0.32308535482063017}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:53:05,367] Trial 42 finished with value: 20.760014702528963 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.07914856861190815, 'max_lr': 0.0068570607161753195, 'div_factor': 537, 'final_div_factor': 398, 'weight_decay': 0.00017652125013170738, 'pct_start': 0.32419104854559816}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:53:26,385] Trial 43 finished with value: 21.283775146168413 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.07997954624025337, 'max_lr': 0.006940209301116053, 'div_factor': 550, 'final_div_factor': 397, 'weight_decay': 3.858993151267887e-05, 'pct_start': 0.33420034269441606}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:53:46,330] Trial 44 finished with value: 21.460445480344617 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.06236630989339344, 'max_lr': 0.003935974835040899, 'div_factor': 356, 'final_div_factor': 309, 'weight_decay': 9.39304704882297e-05, 'pct_start': 0.2866444455886606}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:54:06,677] Trial 45 finished with value: 23.226441620102534 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.10633630098143793, 'max_lr': 0.00320337592424819, 'div_factor': 592, 'final_div_factor': 217, 'weight_decay': 0.00017323955158090842, 'pct_start': 0.34844109178845906}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:54:27,508] Trial 46 finished with value: 636.4571863055676 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.07972505289801016, 'max_lr': 1.0103502288055663e-05, 'div_factor': 510, 'final_div_factor': 428, 'weight_decay': 1.1559968216383624e-06, 'pct_start': 0.32984540480836044}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:54:48,696] Trial 47 finished with value: 21.766716507313767 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.03598380081018306, 'max_lr': 0.009834257096106991, 'div_factor': 368, 'final_div_factor': 482, 'weight_decay': 0.0010201725122234537, 'pct_start': 0.30545866120179754}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:55:10,094] Trial 48 finished with value: 22.60703774309182 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.014167182467816897, 'max_lr': 0.0017793022126236383, 'div_factor': 247, 'final_div_factor': 367, 'weight_decay': 5.5173285583058316e-06, 'pct_start': 0.3816322072389783}. Best is trial 34 with value: 20.178577447207935.\n",
      "[I 2025-12-16 01:55:31,712] Trial 49 finished with value: 21.065998092536837 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.05858077082852981, 'max_lr': 0.006755908148002902, 'div_factor': 539, 'final_div_factor': 269, 'weight_decay': 5.192174469981468e-05, 'pct_start': 0.2335749839126818}. Best is trial 34 with value: 20.178577447207935.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    attributes=attributes,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    epochs=epochs,\n",
    "    path_vision=path_vision,\n",
    "    path_mlp=path_mlp\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating top-5 trials once at 100 epochs (seed=0)...\n",
      "\n",
      "→ Single-pass full run (Trial 34, ValObjective: 20.1786)\n",
      "\n",
      "Best Trial: 34\n",
      "  Best Score: 20.1786\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.03362183376308595\n",
      "    max_lr: 0.009923163465679907\n",
      "    div_factor: 449\n",
      "    final_div_factor: 610\n",
      "    weight_decay: 0.0003778396434377797\n",
      "    pct_start: 0.31936554705694625\n",
      "  Params: total=577,689  trainable=577,689\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  125.59 K\n",
      "fwd MACs:                                                               51.97 MMACs\n",
      "fwd FLOPs:                                                              105.27 MFLOPS\n",
      "fwd+bwd MACs:                                                           155.92 MMACs\n",
      "fwd+bwd FLOPs:                                                          315.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  125.59 K = 100% Params, 51.97 MMACs = 100% MACs, 105.27 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "      (0): Conv2d(1.3 K = 1.03% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.08% Params, 0 MACs = 0% MACs, 422.4 KFLOPS = 0.4% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 211.2 KFLOPS = 0.2% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    124.2 K = 98.89% Params, 46.27 MMACs = 89.03% MACs, 93.22 MFLOPS = 88.56% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      5.76 K = 4.59% Params, 25.08 MMACs = 48.26% MACs, 50.51 MFLOPS = 47.98% FLOPs\n",
      "      (conv1): Conv2d(4.32 K = 3.44% Params, 19.01 MMACs = 36.57% MACs, 38.02 MFLOPS = 36.11% FLOPs, 48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(900 = 0.72% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        500 = 0.4% Params, 2.11 MMACs = 4.06% MACs, 4.31 MFLOPS = 4.1% FLOPs\n",
      "        (0): Conv2d(480 = 0.38% Params, 2.11 MMACs = 4.06% MACs, 4.22 MFLOPS = 4.01% FLOPs, 48, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      5.72 K = 4.55% Params, 6.16 MMACs = 11.85% MACs, 12.5 MFLOPS = 11.87% FLOPs\n",
      "      (conv1): Conv2d(1.8 K = 1.43% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(3.6 K = 2.87% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        240 = 0.19% Params, 220 KMACs = 0.42% MACs, 484 KFLOPS = 0.46% FLOPs\n",
      "        (0): Conv2d(200 = 0.16% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      22.64 K = 18.03% Params, 6.16 MMACs = 11.85% MACs, 12.41 MFLOPS = 11.79% FLOPs\n",
      "      (conv1): Conv2d(7.2 K = 5.73% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(14.4 K = 11.47% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        880 = 0.7% Params, 220 KMACs = 0.42% MACs, 462 KFLOPS = 0.44% FLOPs\n",
      "        (0): Conv2d(800 = 0.64% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      90.08 K = 71.72% Params, 8.87 MMACs = 17.07% MACs, 17.8 MFLOPS = 16.91% FLOPs\n",
      "      (conv1): Conv2d(28.8 K = 22.93% Params, 2.85 MMACs = 5.49% MACs, 5.7 MFLOPS = 5.42% FLOPs, 40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(57.6 K = 45.86% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        3.36 K = 2.68% Params, 316.8 KMACs = 0.61% MACs, 649.44 KFLOPS = 0.62% FLOPs\n",
      "        (0): Conv2d(3.2 K = 2.55% Params, 316.8 KMACs = 0.61% MACs, 633.6 KFLOPS = 0.6% FLOPs, 40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.92 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  152.06 K\n",
      "fwd MACs:                                                               1.67 MMACs\n",
      "fwd FLOPs:                                                              3.34 MFLOPS\n",
      "fwd+bwd MACs:                                                           5.01 MMACs\n",
      "fwd+bwd FLOPs:                                                          10.03 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  152.06 K = 100% Params, 1.67 MMACs = 100% MACs, 3.34 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(151.81 K = 99.83% Params, 1.67 MMACs = 99.83% MACs, 3.33 MFLOPS = 99.75% FLOPs, in_features=592, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.03362183376308595, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_34/best_model.pth\n",
      "   val_rmse=20.498488\n",
      "   params: total=577,689, trainable=577,689\n",
      "→ Single-pass full run (Trial 41, ValObjective: 20.5726)\n",
      "\n",
      "Best Trial: 41\n",
      "  Best Score: 20.5726\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.046947804526090556\n",
      "    max_lr: 0.005946275169201334\n",
      "    div_factor: 506\n",
      "    final_div_factor: 418\n",
      "    weight_decay: 8.735118629939312e-05\n",
      "    pct_start: 0.32308535482063017\n",
      "  Params: total=577,689  trainable=577,689\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  125.59 K\n",
      "fwd MACs:                                                               51.97 MMACs\n",
      "fwd FLOPs:                                                              105.27 MFLOPS\n",
      "fwd+bwd MACs:                                                           155.92 MMACs\n",
      "fwd+bwd FLOPs:                                                          315.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  125.59 K = 100% Params, 51.97 MMACs = 100% MACs, 105.27 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "      (0): Conv2d(1.3 K = 1.03% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.08% Params, 0 MACs = 0% MACs, 422.4 KFLOPS = 0.4% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 211.2 KFLOPS = 0.2% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    124.2 K = 98.89% Params, 46.27 MMACs = 89.03% MACs, 93.22 MFLOPS = 88.56% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      5.76 K = 4.59% Params, 25.08 MMACs = 48.26% MACs, 50.51 MFLOPS = 47.98% FLOPs\n",
      "      (conv1): Conv2d(4.32 K = 3.44% Params, 19.01 MMACs = 36.57% MACs, 38.02 MFLOPS = 36.11% FLOPs, 48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(900 = 0.72% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        500 = 0.4% Params, 2.11 MMACs = 4.06% MACs, 4.31 MFLOPS = 4.1% FLOPs\n",
      "        (0): Conv2d(480 = 0.38% Params, 2.11 MMACs = 4.06% MACs, 4.22 MFLOPS = 4.01% FLOPs, 48, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      5.72 K = 4.55% Params, 6.16 MMACs = 11.85% MACs, 12.5 MFLOPS = 11.87% FLOPs\n",
      "      (conv1): Conv2d(1.8 K = 1.43% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(3.6 K = 2.87% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        240 = 0.19% Params, 220 KMACs = 0.42% MACs, 484 KFLOPS = 0.46% FLOPs\n",
      "        (0): Conv2d(200 = 0.16% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      22.64 K = 18.03% Params, 6.16 MMACs = 11.85% MACs, 12.41 MFLOPS = 11.79% FLOPs\n",
      "      (conv1): Conv2d(7.2 K = 5.73% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(14.4 K = 11.47% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        880 = 0.7% Params, 220 KMACs = 0.42% MACs, 462 KFLOPS = 0.44% FLOPs\n",
      "        (0): Conv2d(800 = 0.64% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      90.08 K = 71.72% Params, 8.87 MMACs = 17.07% MACs, 17.8 MFLOPS = 16.91% FLOPs\n",
      "      (conv1): Conv2d(28.8 K = 22.93% Params, 2.85 MMACs = 5.49% MACs, 5.7 MFLOPS = 5.42% FLOPs, 40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(57.6 K = 45.86% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        3.36 K = 2.68% Params, 316.8 KMACs = 0.61% MACs, 649.44 KFLOPS = 0.62% FLOPs\n",
      "        (0): Conv2d(3.2 K = 2.55% Params, 316.8 KMACs = 0.61% MACs, 633.6 KFLOPS = 0.6% FLOPs, 40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.92 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  152.06 K\n",
      "fwd MACs:                                                               1.67 MMACs\n",
      "fwd FLOPs:                                                              3.34 MFLOPS\n",
      "fwd+bwd MACs:                                                           5.01 MMACs\n",
      "fwd+bwd FLOPs:                                                          10.03 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  152.06 K = 100% Params, 1.67 MMACs = 100% MACs, 3.34 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(151.81 K = 99.83% Params, 1.67 MMACs = 99.83% MACs, 3.33 MFLOPS = 99.75% FLOPs, in_features=592, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.046947804526090556, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_41/best_model.pth\n",
      "   val_rmse=20.609816\n",
      "   params: total=577,689, trainable=577,689\n",
      "→ Single-pass full run (Trial 27, ValObjective: 20.6963)\n",
      "\n",
      "Best Trial: 27\n",
      "  Best Score: 20.6963\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.07751887561997378\n",
      "    max_lr: 0.0056748821295034306\n",
      "    div_factor: 407\n",
      "    final_div_factor: 382\n",
      "    weight_decay: 0.0003117192337143475\n",
      "    pct_start: 0.3351180030040058\n",
      "  Params: total=577,689  trainable=577,689\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  125.59 K\n",
      "fwd MACs:                                                               51.97 MMACs\n",
      "fwd FLOPs:                                                              105.27 MFLOPS\n",
      "fwd+bwd MACs:                                                           155.92 MMACs\n",
      "fwd+bwd FLOPs:                                                          315.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  125.59 K = 100% Params, 51.97 MMACs = 100% MACs, 105.27 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "      (0): Conv2d(1.3 K = 1.03% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.08% Params, 0 MACs = 0% MACs, 422.4 KFLOPS = 0.4% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 211.2 KFLOPS = 0.2% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    124.2 K = 98.89% Params, 46.27 MMACs = 89.03% MACs, 93.22 MFLOPS = 88.56% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      5.76 K = 4.59% Params, 25.08 MMACs = 48.26% MACs, 50.51 MFLOPS = 47.98% FLOPs\n",
      "      (conv1): Conv2d(4.32 K = 3.44% Params, 19.01 MMACs = 36.57% MACs, 38.02 MFLOPS = 36.11% FLOPs, 48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(900 = 0.72% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        500 = 0.4% Params, 2.11 MMACs = 4.06% MACs, 4.31 MFLOPS = 4.1% FLOPs\n",
      "        (0): Conv2d(480 = 0.38% Params, 2.11 MMACs = 4.06% MACs, 4.22 MFLOPS = 4.01% FLOPs, 48, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      5.72 K = 4.55% Params, 6.16 MMACs = 11.85% MACs, 12.5 MFLOPS = 11.87% FLOPs\n",
      "      (conv1): Conv2d(1.8 K = 1.43% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(3.6 K = 2.87% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        240 = 0.19% Params, 220 KMACs = 0.42% MACs, 484 KFLOPS = 0.46% FLOPs\n",
      "        (0): Conv2d(200 = 0.16% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      22.64 K = 18.03% Params, 6.16 MMACs = 11.85% MACs, 12.41 MFLOPS = 11.79% FLOPs\n",
      "      (conv1): Conv2d(7.2 K = 5.73% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(14.4 K = 11.47% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        880 = 0.7% Params, 220 KMACs = 0.42% MACs, 462 KFLOPS = 0.44% FLOPs\n",
      "        (0): Conv2d(800 = 0.64% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      90.08 K = 71.72% Params, 8.87 MMACs = 17.07% MACs, 17.8 MFLOPS = 16.91% FLOPs\n",
      "      (conv1): Conv2d(28.8 K = 22.93% Params, 2.85 MMACs = 5.49% MACs, 5.7 MFLOPS = 5.42% FLOPs, 40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(57.6 K = 45.86% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        3.36 K = 2.68% Params, 316.8 KMACs = 0.61% MACs, 649.44 KFLOPS = 0.62% FLOPs\n",
      "        (0): Conv2d(3.2 K = 2.55% Params, 316.8 KMACs = 0.61% MACs, 633.6 KFLOPS = 0.6% FLOPs, 40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.92 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  152.06 K\n",
      "fwd MACs:                                                               1.67 MMACs\n",
      "fwd FLOPs:                                                              3.34 MFLOPS\n",
      "fwd+bwd MACs:                                                           5.01 MMACs\n",
      "fwd+bwd FLOPs:                                                          10.03 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  152.06 K = 100% Params, 1.67 MMACs = 100% MACs, 3.34 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(151.81 K = 99.83% Params, 1.67 MMACs = 99.83% MACs, 3.33 MFLOPS = 99.75% FLOPs, in_features=592, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.07751887561997378, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_27/best_model.pth\n",
      "   val_rmse=20.780403\n",
      "   params: total=577,689, trainable=577,689\n",
      "→ Single-pass full run (Trial 42, ValObjective: 20.7600)\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 20.7600\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.07914856861190815\n",
      "    max_lr: 0.0068570607161753195\n",
      "    div_factor: 537\n",
      "    final_div_factor: 398\n",
      "    weight_decay: 0.00017652125013170738\n",
      "    pct_start: 0.32419104854559816\n",
      "  Params: total=577,689  trainable=577,689\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  125.59 K\n",
      "fwd MACs:                                                               51.97 MMACs\n",
      "fwd FLOPs:                                                              105.27 MFLOPS\n",
      "fwd+bwd MACs:                                                           155.92 MMACs\n",
      "fwd+bwd FLOPs:                                                          315.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  125.59 K = 100% Params, 51.97 MMACs = 100% MACs, 105.27 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "      (0): Conv2d(1.3 K = 1.03% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.08% Params, 0 MACs = 0% MACs, 422.4 KFLOPS = 0.4% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 211.2 KFLOPS = 0.2% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    124.2 K = 98.89% Params, 46.27 MMACs = 89.03% MACs, 93.22 MFLOPS = 88.56% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      5.76 K = 4.59% Params, 25.08 MMACs = 48.26% MACs, 50.51 MFLOPS = 47.98% FLOPs\n",
      "      (conv1): Conv2d(4.32 K = 3.44% Params, 19.01 MMACs = 36.57% MACs, 38.02 MFLOPS = 36.11% FLOPs, 48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(900 = 0.72% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        500 = 0.4% Params, 2.11 MMACs = 4.06% MACs, 4.31 MFLOPS = 4.1% FLOPs\n",
      "        (0): Conv2d(480 = 0.38% Params, 2.11 MMACs = 4.06% MACs, 4.22 MFLOPS = 4.01% FLOPs, 48, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      5.72 K = 4.55% Params, 6.16 MMACs = 11.85% MACs, 12.5 MFLOPS = 11.87% FLOPs\n",
      "      (conv1): Conv2d(1.8 K = 1.43% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(3.6 K = 2.87% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        240 = 0.19% Params, 220 KMACs = 0.42% MACs, 484 KFLOPS = 0.46% FLOPs\n",
      "        (0): Conv2d(200 = 0.16% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      22.64 K = 18.03% Params, 6.16 MMACs = 11.85% MACs, 12.41 MFLOPS = 11.79% FLOPs\n",
      "      (conv1): Conv2d(7.2 K = 5.73% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(14.4 K = 11.47% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        880 = 0.7% Params, 220 KMACs = 0.42% MACs, 462 KFLOPS = 0.44% FLOPs\n",
      "        (0): Conv2d(800 = 0.64% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      90.08 K = 71.72% Params, 8.87 MMACs = 17.07% MACs, 17.8 MFLOPS = 16.91% FLOPs\n",
      "      (conv1): Conv2d(28.8 K = 22.93% Params, 2.85 MMACs = 5.49% MACs, 5.7 MFLOPS = 5.42% FLOPs, 40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(57.6 K = 45.86% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        3.36 K = 2.68% Params, 316.8 KMACs = 0.61% MACs, 649.44 KFLOPS = 0.62% FLOPs\n",
      "        (0): Conv2d(3.2 K = 2.55% Params, 316.8 KMACs = 0.61% MACs, 633.6 KFLOPS = 0.6% FLOPs, 40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.92 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  152.06 K\n",
      "fwd MACs:                                                               1.67 MMACs\n",
      "fwd FLOPs:                                                              3.34 MFLOPS\n",
      "fwd+bwd MACs:                                                           5.01 MMACs\n",
      "fwd+bwd FLOPs:                                                          10.03 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  152.06 K = 100% Params, 1.67 MMACs = 100% MACs, 3.34 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(151.81 K = 99.83% Params, 1.67 MMACs = 99.83% MACs, 3.33 MFLOPS = 99.75% FLOPs, in_features=592, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.07914856861190815, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_42/best_model.pth\n",
      "   val_rmse=20.196972\n",
      "   params: total=577,689, trainable=577,689\n",
      "→ Single-pass full run (Trial 31, ValObjective: 20.7642)\n",
      "\n",
      "Best Trial: 31\n",
      "  Best Score: 20.7642\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.0671715477984629\n",
      "    max_lr: 0.003283469175981608\n",
      "    div_factor: 454\n",
      "    final_div_factor: 570\n",
      "    weight_decay: 0.0002568685276166657\n",
      "    pct_start: 0.30709784035026777\n",
      "  Params: total=577,689  trainable=577,689\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  125.59 K\n",
      "fwd MACs:                                                               51.97 MMACs\n",
      "fwd FLOPs:                                                              105.27 MFLOPS\n",
      "fwd+bwd MACs:                                                           155.92 MMACs\n",
      "fwd+bwd FLOPs:                                                          315.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  125.59 K = 100% Params, 51.97 MMACs = 100% MACs, 105.27 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "      (0): Conv2d(1.3 K = 1.03% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.08% Params, 0 MACs = 0% MACs, 422.4 KFLOPS = 0.4% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 211.2 KFLOPS = 0.2% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    124.2 K = 98.89% Params, 46.27 MMACs = 89.03% MACs, 93.22 MFLOPS = 88.56% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      5.76 K = 4.59% Params, 25.08 MMACs = 48.26% MACs, 50.51 MFLOPS = 47.98% FLOPs\n",
      "      (conv1): Conv2d(4.32 K = 3.44% Params, 19.01 MMACs = 36.57% MACs, 38.02 MFLOPS = 36.11% FLOPs, 48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(900 = 0.72% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        500 = 0.4% Params, 2.11 MMACs = 4.06% MACs, 4.31 MFLOPS = 4.1% FLOPs\n",
      "        (0): Conv2d(480 = 0.38% Params, 2.11 MMACs = 4.06% MACs, 4.22 MFLOPS = 4.01% FLOPs, 48, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      5.72 K = 4.55% Params, 6.16 MMACs = 11.85% MACs, 12.5 MFLOPS = 11.87% FLOPs\n",
      "      (conv1): Conv2d(1.8 K = 1.43% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(3.6 K = 2.87% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        240 = 0.19% Params, 220 KMACs = 0.42% MACs, 484 KFLOPS = 0.46% FLOPs\n",
      "        (0): Conv2d(200 = 0.16% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      22.64 K = 18.03% Params, 6.16 MMACs = 11.85% MACs, 12.41 MFLOPS = 11.79% FLOPs\n",
      "      (conv1): Conv2d(7.2 K = 5.73% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(14.4 K = 11.47% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        880 = 0.7% Params, 220 KMACs = 0.42% MACs, 462 KFLOPS = 0.44% FLOPs\n",
      "        (0): Conv2d(800 = 0.64% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      90.08 K = 71.72% Params, 8.87 MMACs = 17.07% MACs, 17.8 MFLOPS = 16.91% FLOPs\n",
      "      (conv1): Conv2d(28.8 K = 22.93% Params, 2.85 MMACs = 5.49% MACs, 5.7 MFLOPS = 5.42% FLOPs, 40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(57.6 K = 45.86% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        3.36 K = 2.68% Params, 316.8 KMACs = 0.61% MACs, 649.44 KFLOPS = 0.62% FLOPs\n",
      "        (0): Conv2d(3.2 K = 2.55% Params, 316.8 KMACs = 0.61% MACs, 633.6 KFLOPS = 0.6% FLOPs, 40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.92 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  152.06 K\n",
      "fwd MACs:                                                               1.67 MMACs\n",
      "fwd FLOPs:                                                              3.34 MFLOPS\n",
      "fwd+bwd MACs:                                                           5.01 MMACs\n",
      "fwd+bwd FLOPs:                                                          10.03 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  152.06 K = 100% Params, 1.67 MMACs = 100% MACs, 3.34 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(151.81 K = 99.83% Params, 1.67 MMACs = 99.83% MACs, 3.33 MFLOPS = 99.75% FLOPs, in_features=592, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0671715477984629, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_31/best_model.pth\n",
      "   val_rmse=20.555531\n",
      "   params: total=577,689, trainable=577,689\n",
      "\n",
      "Winner after single-pass: Trial 42 (trial_42) by val_rmse=20.196972\n",
      "\n",
      "Re-running winner with seeds [0, 1, 2, 3, 4] at 100 epochs...\n",
      "\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 20.7600\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.07914856861190815\n",
      "    max_lr: 0.0068570607161753195\n",
      "    div_factor: 537\n",
      "    final_div_factor: 398\n",
      "    weight_decay: 0.00017652125013170738\n",
      "    pct_start: 0.32419104854559816\n",
      "  Params: total=577,689  trainable=577,689\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  125.59 K\n",
      "fwd MACs:                                                               51.97 MMACs\n",
      "fwd FLOPs:                                                              105.27 MFLOPS\n",
      "fwd+bwd MACs:                                                           155.92 MMACs\n",
      "fwd+bwd FLOPs:                                                          315.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  125.59 K = 100% Params, 51.97 MMACs = 100% MACs, 105.27 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "      (0): Conv2d(1.3 K = 1.03% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.08% Params, 0 MACs = 0% MACs, 422.4 KFLOPS = 0.4% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 211.2 KFLOPS = 0.2% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    124.2 K = 98.89% Params, 46.27 MMACs = 89.03% MACs, 93.22 MFLOPS = 88.56% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      5.76 K = 4.59% Params, 25.08 MMACs = 48.26% MACs, 50.51 MFLOPS = 47.98% FLOPs\n",
      "      (conv1): Conv2d(4.32 K = 3.44% Params, 19.01 MMACs = 36.57% MACs, 38.02 MFLOPS = 36.11% FLOPs, 48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(900 = 0.72% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        500 = 0.4% Params, 2.11 MMACs = 4.06% MACs, 4.31 MFLOPS = 4.1% FLOPs\n",
      "        (0): Conv2d(480 = 0.38% Params, 2.11 MMACs = 4.06% MACs, 4.22 MFLOPS = 4.01% FLOPs, 48, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      5.72 K = 4.55% Params, 6.16 MMACs = 11.85% MACs, 12.5 MFLOPS = 11.87% FLOPs\n",
      "      (conv1): Conv2d(1.8 K = 1.43% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(3.6 K = 2.87% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        240 = 0.19% Params, 220 KMACs = 0.42% MACs, 484 KFLOPS = 0.46% FLOPs\n",
      "        (0): Conv2d(200 = 0.16% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      22.64 K = 18.03% Params, 6.16 MMACs = 11.85% MACs, 12.41 MFLOPS = 11.79% FLOPs\n",
      "      (conv1): Conv2d(7.2 K = 5.73% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(14.4 K = 11.47% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        880 = 0.7% Params, 220 KMACs = 0.42% MACs, 462 KFLOPS = 0.44% FLOPs\n",
      "        (0): Conv2d(800 = 0.64% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      90.08 K = 71.72% Params, 8.87 MMACs = 17.07% MACs, 17.8 MFLOPS = 16.91% FLOPs\n",
      "      (conv1): Conv2d(28.8 K = 22.93% Params, 2.85 MMACs = 5.49% MACs, 5.7 MFLOPS = 5.42% FLOPs, 40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(57.6 K = 45.86% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        3.36 K = 2.68% Params, 316.8 KMACs = 0.61% MACs, 649.44 KFLOPS = 0.62% FLOPs\n",
      "        (0): Conv2d(3.2 K = 2.55% Params, 316.8 KMACs = 0.61% MACs, 633.6 KFLOPS = 0.6% FLOPs, 40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.92 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  152.06 K\n",
      "fwd MACs:                                                               1.67 MMACs\n",
      "fwd FLOPs:                                                              3.34 MFLOPS\n",
      "fwd+bwd MACs:                                                           5.01 MMACs\n",
      "fwd+bwd FLOPs:                                                          10.03 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  152.06 K = 100% Params, 1.67 MMACs = 100% MACs, 3.34 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(151.81 K = 99.83% Params, 1.67 MMACs = 99.83% MACs, 3.33 MFLOPS = 99.75% FLOPs, in_features=592, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.07914856861190815, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_42_seed0/best_model.pth\n",
      "   Seed 0: val_rmse=20.196972, test_loss=571.749868, test_rmse=23.925624, val_loss=409.927032\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 20.7600\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.07914856861190815\n",
      "    max_lr: 0.0068570607161753195\n",
      "    div_factor: 537\n",
      "    final_div_factor: 398\n",
      "    weight_decay: 0.00017652125013170738\n",
      "    pct_start: 0.32419104854559816\n",
      "  Params: total=577,689  trainable=577,689\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  125.59 K\n",
      "fwd MACs:                                                               51.97 MMACs\n",
      "fwd FLOPs:                                                              105.27 MFLOPS\n",
      "fwd+bwd MACs:                                                           155.92 MMACs\n",
      "fwd+bwd FLOPs:                                                          315.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  125.59 K = 100% Params, 51.97 MMACs = 100% MACs, 105.27 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "      (0): Conv2d(1.3 K = 1.03% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.08% Params, 0 MACs = 0% MACs, 422.4 KFLOPS = 0.4% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 211.2 KFLOPS = 0.2% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    124.2 K = 98.89% Params, 46.27 MMACs = 89.03% MACs, 93.22 MFLOPS = 88.56% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      5.76 K = 4.59% Params, 25.08 MMACs = 48.26% MACs, 50.51 MFLOPS = 47.98% FLOPs\n",
      "      (conv1): Conv2d(4.32 K = 3.44% Params, 19.01 MMACs = 36.57% MACs, 38.02 MFLOPS = 36.11% FLOPs, 48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(900 = 0.72% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        500 = 0.4% Params, 2.11 MMACs = 4.06% MACs, 4.31 MFLOPS = 4.1% FLOPs\n",
      "        (0): Conv2d(480 = 0.38% Params, 2.11 MMACs = 4.06% MACs, 4.22 MFLOPS = 4.01% FLOPs, 48, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      5.72 K = 4.55% Params, 6.16 MMACs = 11.85% MACs, 12.5 MFLOPS = 11.87% FLOPs\n",
      "      (conv1): Conv2d(1.8 K = 1.43% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(3.6 K = 2.87% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        240 = 0.19% Params, 220 KMACs = 0.42% MACs, 484 KFLOPS = 0.46% FLOPs\n",
      "        (0): Conv2d(200 = 0.16% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      22.64 K = 18.03% Params, 6.16 MMACs = 11.85% MACs, 12.41 MFLOPS = 11.79% FLOPs\n",
      "      (conv1): Conv2d(7.2 K = 5.73% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(14.4 K = 11.47% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        880 = 0.7% Params, 220 KMACs = 0.42% MACs, 462 KFLOPS = 0.44% FLOPs\n",
      "        (0): Conv2d(800 = 0.64% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      90.08 K = 71.72% Params, 8.87 MMACs = 17.07% MACs, 17.8 MFLOPS = 16.91% FLOPs\n",
      "      (conv1): Conv2d(28.8 K = 22.93% Params, 2.85 MMACs = 5.49% MACs, 5.7 MFLOPS = 5.42% FLOPs, 40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(57.6 K = 45.86% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        3.36 K = 2.68% Params, 316.8 KMACs = 0.61% MACs, 649.44 KFLOPS = 0.62% FLOPs\n",
      "        (0): Conv2d(3.2 K = 2.55% Params, 316.8 KMACs = 0.61% MACs, 633.6 KFLOPS = 0.6% FLOPs, 40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.92 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  152.06 K\n",
      "fwd MACs:                                                               1.67 MMACs\n",
      "fwd FLOPs:                                                              3.34 MFLOPS\n",
      "fwd+bwd MACs:                                                           5.01 MMACs\n",
      "fwd+bwd FLOPs:                                                          10.03 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  152.06 K = 100% Params, 1.67 MMACs = 100% MACs, 3.34 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(151.81 K = 99.83% Params, 1.67 MMACs = 99.83% MACs, 3.33 MFLOPS = 99.75% FLOPs, in_features=592, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.07914856861190815, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_42_seed1/best_model.pth\n",
      "   Seed 1: val_rmse=20.277878, test_loss=530.407196, test_rmse=23.047714, val_loss=415.637548\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 20.7600\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.07914856861190815\n",
      "    max_lr: 0.0068570607161753195\n",
      "    div_factor: 537\n",
      "    final_div_factor: 398\n",
      "    weight_decay: 0.00017652125013170738\n",
      "    pct_start: 0.32419104854559816\n",
      "  Params: total=577,689  trainable=577,689\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  125.59 K\n",
      "fwd MACs:                                                               51.97 MMACs\n",
      "fwd FLOPs:                                                              105.27 MFLOPS\n",
      "fwd+bwd MACs:                                                           155.92 MMACs\n",
      "fwd+bwd FLOPs:                                                          315.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  125.59 K = 100% Params, 51.97 MMACs = 100% MACs, 105.27 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "      (0): Conv2d(1.3 K = 1.03% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.08% Params, 0 MACs = 0% MACs, 422.4 KFLOPS = 0.4% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 211.2 KFLOPS = 0.2% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    124.2 K = 98.89% Params, 46.27 MMACs = 89.03% MACs, 93.22 MFLOPS = 88.56% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      5.76 K = 4.59% Params, 25.08 MMACs = 48.26% MACs, 50.51 MFLOPS = 47.98% FLOPs\n",
      "      (conv1): Conv2d(4.32 K = 3.44% Params, 19.01 MMACs = 36.57% MACs, 38.02 MFLOPS = 36.11% FLOPs, 48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(900 = 0.72% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        500 = 0.4% Params, 2.11 MMACs = 4.06% MACs, 4.31 MFLOPS = 4.1% FLOPs\n",
      "        (0): Conv2d(480 = 0.38% Params, 2.11 MMACs = 4.06% MACs, 4.22 MFLOPS = 4.01% FLOPs, 48, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      5.72 K = 4.55% Params, 6.16 MMACs = 11.85% MACs, 12.5 MFLOPS = 11.87% FLOPs\n",
      "      (conv1): Conv2d(1.8 K = 1.43% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(3.6 K = 2.87% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        240 = 0.19% Params, 220 KMACs = 0.42% MACs, 484 KFLOPS = 0.46% FLOPs\n",
      "        (0): Conv2d(200 = 0.16% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      22.64 K = 18.03% Params, 6.16 MMACs = 11.85% MACs, 12.41 MFLOPS = 11.79% FLOPs\n",
      "      (conv1): Conv2d(7.2 K = 5.73% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(14.4 K = 11.47% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        880 = 0.7% Params, 220 KMACs = 0.42% MACs, 462 KFLOPS = 0.44% FLOPs\n",
      "        (0): Conv2d(800 = 0.64% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      90.08 K = 71.72% Params, 8.87 MMACs = 17.07% MACs, 17.8 MFLOPS = 16.91% FLOPs\n",
      "      (conv1): Conv2d(28.8 K = 22.93% Params, 2.85 MMACs = 5.49% MACs, 5.7 MFLOPS = 5.42% FLOPs, 40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(57.6 K = 45.86% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        3.36 K = 2.68% Params, 316.8 KMACs = 0.61% MACs, 649.44 KFLOPS = 0.62% FLOPs\n",
      "        (0): Conv2d(3.2 K = 2.55% Params, 316.8 KMACs = 0.61% MACs, 633.6 KFLOPS = 0.6% FLOPs, 40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.92 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  152.06 K\n",
      "fwd MACs:                                                               1.67 MMACs\n",
      "fwd FLOPs:                                                              3.34 MFLOPS\n",
      "fwd+bwd MACs:                                                           5.01 MMACs\n",
      "fwd+bwd FLOPs:                                                          10.03 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  152.06 K = 100% Params, 1.67 MMACs = 100% MACs, 3.34 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(151.81 K = 99.83% Params, 1.67 MMACs = 99.83% MACs, 3.33 MFLOPS = 99.75% FLOPs, in_features=592, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.07914856861190815, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_42_seed2/best_model.pth\n",
      "   Seed 2: val_rmse=20.387881, test_loss=568.033132, test_rmse=23.932127, val_loss=417.538839\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 20.7600\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.07914856861190815\n",
      "    max_lr: 0.0068570607161753195\n",
      "    div_factor: 537\n",
      "    final_div_factor: 398\n",
      "    weight_decay: 0.00017652125013170738\n",
      "    pct_start: 0.32419104854559816\n",
      "  Params: total=577,689  trainable=577,689\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  125.59 K\n",
      "fwd MACs:                                                               51.97 MMACs\n",
      "fwd FLOPs:                                                              105.27 MFLOPS\n",
      "fwd+bwd MACs:                                                           155.92 MMACs\n",
      "fwd+bwd FLOPs:                                                          315.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  125.59 K = 100% Params, 51.97 MMACs = 100% MACs, 105.27 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "      (0): Conv2d(1.3 K = 1.03% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.08% Params, 0 MACs = 0% MACs, 422.4 KFLOPS = 0.4% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 211.2 KFLOPS = 0.2% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    124.2 K = 98.89% Params, 46.27 MMACs = 89.03% MACs, 93.22 MFLOPS = 88.56% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      5.76 K = 4.59% Params, 25.08 MMACs = 48.26% MACs, 50.51 MFLOPS = 47.98% FLOPs\n",
      "      (conv1): Conv2d(4.32 K = 3.44% Params, 19.01 MMACs = 36.57% MACs, 38.02 MFLOPS = 36.11% FLOPs, 48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(900 = 0.72% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        500 = 0.4% Params, 2.11 MMACs = 4.06% MACs, 4.31 MFLOPS = 4.1% FLOPs\n",
      "        (0): Conv2d(480 = 0.38% Params, 2.11 MMACs = 4.06% MACs, 4.22 MFLOPS = 4.01% FLOPs, 48, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      5.72 K = 4.55% Params, 6.16 MMACs = 11.85% MACs, 12.5 MFLOPS = 11.87% FLOPs\n",
      "      (conv1): Conv2d(1.8 K = 1.43% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(3.6 K = 2.87% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        240 = 0.19% Params, 220 KMACs = 0.42% MACs, 484 KFLOPS = 0.46% FLOPs\n",
      "        (0): Conv2d(200 = 0.16% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      22.64 K = 18.03% Params, 6.16 MMACs = 11.85% MACs, 12.41 MFLOPS = 11.79% FLOPs\n",
      "      (conv1): Conv2d(7.2 K = 5.73% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(14.4 K = 11.47% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        880 = 0.7% Params, 220 KMACs = 0.42% MACs, 462 KFLOPS = 0.44% FLOPs\n",
      "        (0): Conv2d(800 = 0.64% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      90.08 K = 71.72% Params, 8.87 MMACs = 17.07% MACs, 17.8 MFLOPS = 16.91% FLOPs\n",
      "      (conv1): Conv2d(28.8 K = 22.93% Params, 2.85 MMACs = 5.49% MACs, 5.7 MFLOPS = 5.42% FLOPs, 40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(57.6 K = 45.86% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        3.36 K = 2.68% Params, 316.8 KMACs = 0.61% MACs, 649.44 KFLOPS = 0.62% FLOPs\n",
      "        (0): Conv2d(3.2 K = 2.55% Params, 316.8 KMACs = 0.61% MACs, 633.6 KFLOPS = 0.6% FLOPs, 40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.92 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  152.06 K\n",
      "fwd MACs:                                                               1.67 MMACs\n",
      "fwd FLOPs:                                                              3.34 MFLOPS\n",
      "fwd+bwd MACs:                                                           5.01 MMACs\n",
      "fwd+bwd FLOPs:                                                          10.03 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  152.06 K = 100% Params, 1.67 MMACs = 100% MACs, 3.34 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(151.81 K = 99.83% Params, 1.67 MMACs = 99.83% MACs, 3.33 MFLOPS = 99.75% FLOPs, in_features=592, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.07914856861190815, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_42_seed3/best_model.pth\n",
      "   Seed 3: val_rmse=19.892437, test_loss=553.933411, test_rmse=23.555074, val_loss=396.461639\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 20.7600\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.07914856861190815\n",
      "    max_lr: 0.0068570607161753195\n",
      "    div_factor: 537\n",
      "    final_div_factor: 398\n",
      "    weight_decay: 0.00017652125013170738\n",
      "    pct_start: 0.32419104854559816\n",
      "  Params: total=577,689  trainable=577,689\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  125.59 K\n",
      "fwd MACs:                                                               51.97 MMACs\n",
      "fwd FLOPs:                                                              105.27 MFLOPS\n",
      "fwd+bwd MACs:                                                           155.92 MMACs\n",
      "fwd+bwd FLOPs:                                                          315.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  125.59 K = 100% Params, 51.97 MMACs = 100% MACs, 105.27 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.39 K = 1.11% Params, 5.7 MMACs = 10.97% MACs, 12.04 MFLOPS = 11.44% FLOPs\n",
      "      (0): Conv2d(1.3 K = 1.03% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.08% Params, 0 MACs = 0% MACs, 422.4 KFLOPS = 0.4% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 211.2 KFLOPS = 0.2% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    124.2 K = 98.89% Params, 46.27 MMACs = 89.03% MACs, 93.22 MFLOPS = 88.56% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      5.76 K = 4.59% Params, 25.08 MMACs = 48.26% MACs, 50.51 MFLOPS = 47.98% FLOPs\n",
      "      (conv1): Conv2d(4.32 K = 3.44% Params, 19.01 MMACs = 36.57% MACs, 38.02 MFLOPS = 36.11% FLOPs, 48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(900 = 0.72% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        500 = 0.4% Params, 2.11 MMACs = 4.06% MACs, 4.31 MFLOPS = 4.1% FLOPs\n",
      "        (0): Conv2d(480 = 0.38% Params, 2.11 MMACs = 4.06% MACs, 4.22 MFLOPS = 4.01% FLOPs, 48, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20 = 0.02% Params, 0 MACs = 0% MACs, 88 KFLOPS = 0.08% FLOPs, 10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      5.72 K = 4.55% Params, 6.16 MMACs = 11.85% MACs, 12.5 MFLOPS = 11.87% FLOPs\n",
      "      (conv1): Conv2d(1.8 K = 1.43% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(3.6 K = 2.87% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        240 = 0.19% Params, 220 KMACs = 0.42% MACs, 484 KFLOPS = 0.46% FLOPs\n",
      "        (0): Conv2d(200 = 0.16% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 10, 20, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40 = 0.03% Params, 0 MACs = 0% MACs, 44 KFLOPS = 0.04% FLOPs, 20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      22.64 K = 18.03% Params, 6.16 MMACs = 11.85% MACs, 12.41 MFLOPS = 11.79% FLOPs\n",
      "      (conv1): Conv2d(7.2 K = 5.73% Params, 1.98 MMACs = 3.81% MACs, 3.96 MFLOPS = 3.76% FLOPs, 20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(14.4 K = 11.47% Params, 3.96 MMACs = 7.62% MACs, 7.92 MFLOPS = 7.52% FLOPs, 40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        880 = 0.7% Params, 220 KMACs = 0.42% MACs, 462 KFLOPS = 0.44% FLOPs\n",
      "        (0): Conv2d(800 = 0.64% Params, 220 KMACs = 0.42% MACs, 440 KFLOPS = 0.42% FLOPs, 20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80 = 0.06% Params, 0 MACs = 0% MACs, 22 KFLOPS = 0.02% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      90.08 K = 71.72% Params, 8.87 MMACs = 17.07% MACs, 17.8 MFLOPS = 16.91% FLOPs\n",
      "      (conv1): Conv2d(28.8 K = 22.93% Params, 2.85 MMACs = 5.49% MACs, 5.7 MFLOPS = 5.42% FLOPs, 40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(57.6 K = 45.86% Params, 5.7 MMACs = 10.97% MACs, 11.4 MFLOPS = 10.83% FLOPs, 80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        3.36 K = 2.68% Params, 316.8 KMACs = 0.61% MACs, 649.44 KFLOPS = 0.62% FLOPs\n",
      "        (0): Conv2d(3.2 K = 2.55% Params, 316.8 KMACs = 0.61% MACs, 633.6 KFLOPS = 0.6% FLOPs, 40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160 = 0.13% Params, 0 MACs = 0% MACs, 15.84 KFLOPS = 0.02% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 7.92 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  152.06 K\n",
      "fwd MACs:                                                               1.67 MMACs\n",
      "fwd FLOPs:                                                              3.34 MFLOPS\n",
      "fwd+bwd MACs:                                                           5.01 MMACs\n",
      "fwd+bwd FLOPs:                                                          10.03 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  152.06 K = 100% Params, 1.67 MMACs = 100% MACs, 3.34 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(151.81 K = 99.83% Params, 1.67 MMACs = 99.83% MACs, 3.33 MFLOPS = 99.75% FLOPs, in_features=592, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.07914856861190815, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_42_seed4/best_model.pth\n",
      "   Seed 4: val_rmse=20.738413, test_loss=558.952367, test_rmse=23.741194, val_loss=432.542480\n",
      "\n",
      "Winner aggregated val_rmse: 20.298716 ± 0.307056\n",
      "Model params: total=577,689, trainable=577,689\n",
      "Saved multi-seed summary to: logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_42/winner_multi_seed_summary.txt\n",
      "{'winner_trial_number': 42, 'winner_trial_name': 'trial_42', 'primary_metric': 'val_rmse', 'aggregates': {'train_loss': {'mean': 295.61324451587814, 'std': 13.212568887403929}, 'val_loss': {'mean': 414.42150777180984, 'std': 13.060659793957168}, 'test_loss': {'mean': 556.6151947021485, 'std': 16.270688008783235}, 'min_lr': {'mean': 1.2769200588780855e-05, 'std': 0.0}, 'max_lr': {'mean': 0.00685706071617532, 'std': 9.69739903612216e-19}, 'total_time': {'mean': 26.51054286956787, 'std': 1.362055817774946}, 'average_epoch_time': {'mean': 0.26409380722045894, 'std': 0.013541922054116548}, 'train_mse': {'mean': 295.5695068359375, 'std': 13.207147654953621}, 'train_mae': {'mean': 13.190600204467774, 'std': 0.40857293886578816}, 'train_rmse': {'mean': 17.188709165901418, 'std': 0.3837056912434854}, 'train_r2': {'mean': 0.9656275629997253, 'std': 0.0015358849893638707}, 'val_mse': {'mean': 412.11329956054686, 'std': 12.480346417902084}, 'val_mae': {'mean': 16.2816068649292, 'std': 0.4772091577317891}, 'val_rmse': {'mean': 20.29871604495697, 'std': 0.3070555461831032}, 'val_r2': {'mean': 0.9413561940193176, 'std': 0.00177596851495486}, 'test_mse': {'mean': 558.9730224609375, 'std': 17.18532757202431}, 'test_mae': {'mean': 18.302597427368163, 'std': 0.43078822077596535}, 'test_rmse': {'mean': 23.640346608272075, 'std': 0.3657777701292907}, 'test_r2': {'mean': 0.9349433183670044, 'std': 0.002000124472695579}, 'total_params': {'mean': 577689.0, 'std': 0.0}, 'trainable_params': {'mean': 577689.0, 'std': 0.0}, 'flops': {'mean': 115198512.0, 'std': 0.0}, 'macs': {'mean': 56931776.0, 'std': 0.0}}, 'total_params': 577689, 'trainable_params': 577689, 'flops': 115198512.0, 'macs': 56931776.0, 'summary_path': 'logs/Regression/Moneyball_cleaned/CNN_hybrid/TINTO_blur/best_model/trial_42/winner_multi_seed_summary.txt'}\n"
     ]
    }
   ],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     attributes=attributes,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "     path_vision=path_vision, path_mlp=path_mlp,\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT: IGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "name = f\"IGTD\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes — Train: (862, 72), Val: (185, 72), Test: (185, 72)\n",
      "Numerical features: 8 — ['Year', 'RA', 'W', 'OBP', 'SLG', 'BA', 'OOBP', 'OSLG']\n",
      "Categorical features: 6 — ['Team', 'League', 'Playoffs', 'RankSeason', 'RankPlayoffs', 'G']\n",
      "Total features: 72\n",
      "Images shape (C,H,W): (3, 9, 9)\n",
      "Attributes: 72\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=False, target_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 9]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vision=f\"./logs/{task_type}/{dataset_name}/{vision_name}/{name}/best_model/trial_52\"\n",
    "path_mlp=f\"./logs/{task_type}/{dataset_name}/mlp/best_model/trial_38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-16 02:01:33,071] A new study created in memory with name: no-name-71b26f6c-429a-40b7-bcdf-97fdfcd39cc6\n",
      "[I 2025-12-16 02:01:56,236] Trial 0 finished with value: 21.504423883983296 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.03300149089649712, 'max_lr': 0.00043434159417251293, 'div_factor': 793, 'final_div_factor': 260, 'weight_decay': 0.008548004624615758, 'pct_start': 0.21992522403162432}. Best is trial 0 with value: 21.504423883983296.\n",
      "[I 2025-12-16 02:02:18,804] Trial 1 finished with value: 20.013870007466853 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.16044842147991284, 'max_lr': 0.0007379689221641153, 'div_factor': 589, 'final_div_factor': 202, 'weight_decay': 0.001299019882240533, 'pct_start': 0.11920953511236854}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:02:40,146] Trial 2 finished with value: 20.777877281526422 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.03858005410816033, 'max_lr': 8.120041995510411e-05, 'div_factor': 853, 'final_div_factor': 937, 'weight_decay': 2.617581807207115e-06, 'pct_start': 0.20450446007113393}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:03:02,190] Trial 3 finished with value: 20.946864160634224 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.197906009816364, 'max_lr': 0.00013011778982640847, 'div_factor': 546, 'final_div_factor': 195, 'weight_decay': 1.0394769828327834e-06, 'pct_start': 0.1726570518300583}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:03:23,148] Trial 4 finished with value: 20.2732387996043 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.18830583401446646, 'max_lr': 0.00288571196776685, 'div_factor': 302, 'final_div_factor': 799, 'weight_decay': 1.158641410530314e-06, 'pct_start': 0.200515824997963}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:03:44,568] Trial 5 finished with value: 21.08947801069699 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.04104877405773977, 'max_lr': 0.0005340418075648914, 'div_factor': 306, 'final_div_factor': 304, 'weight_decay': 0.001932735300080477, 'pct_start': 0.3345254908311978}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:04:06,072] Trial 6 finished with value: 21.47392430481276 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.05746633070424756, 'max_lr': 0.0017918670023841466, 'div_factor': 556, 'final_div_factor': 666, 'weight_decay': 0.0013030463054689956, 'pct_start': 0.10964010404044587}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:04:28,183] Trial 7 finished with value: 20.67962183003362 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.14197160456123278, 'max_lr': 0.00013834076418590185, 'div_factor': 685, 'final_div_factor': 624, 'weight_decay': 0.0014691834096586184, 'pct_start': 0.14750865566708166}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:04:49,873] Trial 8 finished with value: 20.947358045396353 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.031061023144281275, 'max_lr': 8.13112555997224e-05, 'div_factor': 572, 'final_div_factor': 608, 'weight_decay': 0.000196782345922413, 'pct_start': 0.20877274170252705}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:05:11,673] Trial 9 finished with value: 21.147325625324722 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.10472035374395158, 'max_lr': 0.003897497650772217, 'div_factor': 126, 'final_div_factor': 680, 'weight_decay': 0.00013523176315888839, 'pct_start': 0.16756278919104087}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:05:33,912] Trial 10 finished with value: 639.9062187148988 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.293730738075961, 'max_lr': 1.0353354810118683e-05, 'div_factor': 996, 'final_div_factor': 420, 'weight_decay': 1.1826030501585412e-05, 'pct_start': 0.2893744330607187}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:05:55,605] Trial 11 finished with value: 20.282586898589344 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.21715635089105675, 'max_lr': 0.005449094138671368, 'div_factor': 313, 'final_div_factor': 943, 'weight_decay': 2.1216855566108796e-05, 'pct_start': 0.11094071232459038}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:06:17,636] Trial 12 finished with value: 20.669712129386774 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.20676411864924887, 'max_lr': 0.001139550569134017, 'div_factor': 333, 'final_div_factor': 820, 'weight_decay': 3.0538326935987646e-05, 'pct_start': 0.2767680160054678}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:06:39,876] Trial 13 finished with value: 21.270308540089985 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.15417448289001562, 'max_lr': 0.009654236911012337, 'div_factor': 53, 'final_div_factor': 780, 'weight_decay': 0.0003784954235668192, 'pct_start': 0.24356146614408583}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:07:02,375] Trial 14 finished with value: 22.082259625897592 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.2540200876701063, 'max_lr': 0.001495911453746236, 'div_factor': 411, 'final_div_factor': 109, 'weight_decay': 0.008058752646426233, 'pct_start': 0.39383277120665466}. Best is trial 1 with value: 20.013870007466853.\n",
      "[I 2025-12-16 02:07:25,096] Trial 15 finished with value: 20.004642710642667 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.14282256825638456, 'max_lr': 0.002665277890928106, 'div_factor': 429, 'final_div_factor': 464, 'weight_decay': 4.720466040544919e-06, 'pct_start': 0.1368103331735112}. Best is trial 15 with value: 20.004642710642667.\n",
      "[I 2025-12-16 02:07:48,094] Trial 16 finished with value: 19.801527577052372 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.12082857120708439, 'max_lr': 0.0007570482845166121, 'div_factor': 695, 'final_div_factor': 433, 'weight_decay': 5.3356671320977265e-05, 'pct_start': 0.14549443957693708}. Best is trial 16 with value: 19.801527577052372.\n",
      "[I 2025-12-16 02:08:10,437] Trial 17 finished with value: 20.085389766695545 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.0885089287906505, 'max_lr': 0.0003110590060484256, 'div_factor': 709, 'final_div_factor': 457, 'weight_decay': 6.835525744511807e-06, 'pct_start': 0.1463936285187383}. Best is trial 16 with value: 19.801527577052372.\n",
      "[I 2025-12-16 02:08:32,885] Trial 18 finished with value: 486.6223831165188 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.11289777373051267, 'max_lr': 2.1317709056718338e-05, 'div_factor': 460, 'final_div_factor': 473, 'weight_decay': 4.695923385720698e-05, 'pct_start': 0.15581541463160362}. Best is trial 16 with value: 19.801527577052372.\n",
      "[I 2025-12-16 02:08:55,531] Trial 19 finished with value: 19.67363335444763 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.004075028244129003, 'max_lr': 0.00810711247531264, 'div_factor': 184, 'final_div_factor': 357, 'weight_decay': 5.853230874234648e-06, 'pct_start': 0.25019812303256056}. Best is trial 19 with value: 19.67363335444763.\n",
      "[I 2025-12-16 02:09:18,146] Trial 20 finished with value: 20.948229963962273 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.0010154288052021355, 'max_lr': 0.009363987181693727, 'div_factor': 172, 'final_div_factor': 314, 'weight_decay': 6.381023491100895e-05, 'pct_start': 0.3067705295615127}. Best is trial 19 with value: 19.67363335444763.\n",
      "[I 2025-12-16 02:09:40,851] Trial 21 finished with value: 19.883203121162893 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.13177319912571542, 'max_lr': 0.002873977645520106, 'div_factor': 200, 'final_div_factor': 384, 'weight_decay': 4.861515238124781e-06, 'pct_start': 0.25695885290249965}. Best is trial 19 with value: 19.67363335444763.\n",
      "[I 2025-12-16 02:10:03,982] Trial 22 finished with value: 24.68263648691753 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.09393188950506222, 'max_lr': 0.004765569166526677, 'div_factor': 192, 'final_div_factor': 358, 'weight_decay': 1.5942688206284828e-05, 'pct_start': 0.2504629794757451}. Best is trial 19 with value: 19.67363335444763.\n",
      "[I 2025-12-16 02:10:27,005] Trial 23 finished with value: 19.620805620147042 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.06622619561932944, 'max_lr': 0.0007949312357869508, 'div_factor': 16, 'final_div_factor': 370, 'weight_decay': 3.300647943144387e-06, 'pct_start': 0.3279145599883264}. Best is trial 23 with value: 19.620805620147042.\n",
      "[I 2025-12-16 02:10:49,844] Trial 24 finished with value: 20.193330566432927 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.06785873659322468, 'max_lr': 0.00027851388110020466, 'div_factor': 17, 'final_div_factor': 551, 'weight_decay': 2.882361270598326e-06, 'pct_start': 0.3514344508725398}. Best is trial 23 with value: 19.620805620147042.\n",
      "[I 2025-12-16 02:11:13,168] Trial 25 finished with value: 19.58709438271593 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.016295302846138338, 'max_lr': 0.0009407855381661249, 'div_factor': 91, 'final_div_factor': 529, 'weight_decay': 9.231471183734755e-06, 'pct_start': 0.32762805488339364}. Best is trial 25 with value: 19.58709438271593.\n",
      "[I 2025-12-16 02:11:36,789] Trial 26 finished with value: 19.56294849600698 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.01041878271830694, 'max_lr': 0.0011721975535196815, 'div_factor': 96, 'final_div_factor': 530, 'weight_decay': 1.0312746387906183e-05, 'pct_start': 0.3690842515553291}. Best is trial 26 with value: 19.56294849600698.\n",
      "[I 2025-12-16 02:12:00,464] Trial 27 finished with value: 20.037732198777434 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.07289961296966369, 'max_lr': 0.0010921475699900004, 'div_factor': 88, 'final_div_factor': 526, 'weight_decay': 1.081749707255342e-05, 'pct_start': 0.39150705658107937}. Best is trial 26 with value: 19.56294849600698.\n",
      "[I 2025-12-16 02:12:24,593] Trial 28 finished with value: 19.30269633220601 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.015863516678051887, 'max_lr': 0.00018948370634940513, 'div_factor': 11, 'final_div_factor': 537, 'weight_decay': 2.3609548800261087e-06, 'pct_start': 0.3453619262098891}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:12:48,747] Trial 29 finished with value: 19.450858772363286 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.025616060224870126, 'max_lr': 0.00041992110052155464, 'div_factor': 124, 'final_div_factor': 555, 'weight_decay': 2.079354445379162e-06, 'pct_start': 0.36699010266077514}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:13:12,714] Trial 30 finished with value: 19.520991055703984 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.023335067489998362, 'max_lr': 0.0002079238381081898, 'div_factor': 250, 'final_div_factor': 725, 'weight_decay': 2.269056346293647e-06, 'pct_start': 0.3657608297080023}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:13:37,137] Trial 31 finished with value: 19.900907317520648 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.02488359675083257, 'max_lr': 0.0001952431415936015, 'div_factor': 238, 'final_div_factor': 700, 'weight_decay': 1.753277774314458e-06, 'pct_start': 0.37191713800770015}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:14:01,551] Trial 32 finished with value: 20.732991428970156 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.04876397968659818, 'max_lr': 0.0003908991312713292, 'div_factor': 130, 'final_div_factor': 746, 'weight_decay': 1.7761114768409165e-06, 'pct_start': 0.3679646295186923}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:14:25,834] Trial 33 finished with value: 21.64852349089742 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.016721082234237482, 'max_lr': 4.600741188829799e-05, 'div_factor': 235, 'final_div_factor': 858, 'weight_decay': 2.321577530039656e-06, 'pct_start': 0.3662025696633956}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:14:50,003] Trial 34 finished with value: 20.83262270923449 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 9.607148758309161e-05, 'max_lr': 0.00020685113603329284, 'div_factor': 116, 'final_div_factor': 594, 'weight_decay': 1.050601220410291e-06, 'pct_start': 0.3462992545040957}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:15:14,144] Trial 35 finished with value: 19.89139748966364 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.03884124687753032, 'max_lr': 0.0004968723484013686, 'div_factor': 65, 'final_div_factor': 731, 'weight_decay': 3.317906142517286e-06, 'pct_start': 0.3083400994663249}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:15:38,789] Trial 36 finished with value: 21.108301549085123 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.08303617307233421, 'max_lr': 7.876175086740619e-05, 'div_factor': 11, 'final_div_factor': 575, 'weight_decay': 1.5809450425334333e-06, 'pct_start': 0.38345704942269926}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:16:03,246] Trial 37 finished with value: 21.76685110172727 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.021328455943552718, 'max_lr': 0.00011260581786456834, 'div_factor': 353, 'final_div_factor': 661, 'weight_decay': 2.122546174414004e-05, 'pct_start': 0.35094706269592246}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:16:28,194] Trial 38 finished with value: 20.0248251384668 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.05211997970917628, 'max_lr': 0.000542623679704308, 'div_factor': 261, 'final_div_factor': 512, 'weight_decay': 1.0046647628443277e-06, 'pct_start': 0.3063643246638373}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:16:53,183] Trial 39 finished with value: 19.71114032049833 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.03613479458431522, 'max_lr': 4.304112612183433e-05, 'div_factor': 140, 'final_div_factor': 865, 'weight_decay': 3.955374086992708e-06, 'pct_start': 0.399068228569589}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:17:18,140] Trial 40 finished with value: 19.56593872657157 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.04905698943147689, 'max_lr': 0.0002194791259782908, 'div_factor': 376, 'final_div_factor': 650, 'weight_decay': 2.121388249445749e-06, 'pct_start': 0.3745904098547064}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:17:43,379] Trial 41 finished with value: 19.37974519764853 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.016945410153823337, 'max_lr': 0.00020741397823681787, 'div_factor': 369, 'final_div_factor': 628, 'weight_decay': 2.321591828874953e-06, 'pct_start': 0.37640417092932255}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:18:08,571] Trial 42 finished with value: 20.162985641770277 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.015792339992826254, 'max_lr': 0.00013058433200950994, 'div_factor': 274, 'final_div_factor': 625, 'weight_decay': 7.323669047732337e-06, 'pct_start': 0.3414451495948724}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:18:33,847] Trial 43 finished with value: 20.083830050371056 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.03217169944705858, 'max_lr': 0.00038866687351140754, 'div_factor': 511, 'final_div_factor': 497, 'weight_decay': 1.674013708625313e-06, 'pct_start': 0.35779572990159714}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:18:59,267] Trial 44 finished with value: 20.186348802557525 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.009810212262826593, 'max_lr': 9.036150994210867e-05, 'div_factor': 156, 'final_div_factor': 561, 'weight_decay': 2.8686952944732884e-06, 'pct_start': 0.380997243454058}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:19:24,961] Trial 45 finished with value: 22.707553712486707 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.1778382426699308, 'max_lr': 5.6576170472867385e-05, 'div_factor': 218, 'final_div_factor': 700, 'weight_decay': 0.0005139551946422963, 'pct_start': 0.32401115860521235}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:19:49,971] Trial 46 finished with value: 23.365966356841792 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.060814707039554536, 'max_lr': 0.0001669315231412562, 'div_factor': 296, 'final_div_factor': 620, 'weight_decay': 1.3389372999162642e-05, 'pct_start': 0.3603767868405718}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:20:15,296] Trial 47 finished with value: 19.938036489892017 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.028784101642004165, 'max_lr': 0.0017894832278524165, 'div_factor': 621, 'final_div_factor': 779, 'weight_decay': 1.424446295216462e-06, 'pct_start': 0.33935044194154046}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:20:40,410] Trial 48 finished with value: 20.424244913177024 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.03834518450601177, 'max_lr': 0.0005876383265339155, 'div_factor': 51, 'final_div_factor': 580, 'weight_decay': 8.067009511650393e-06, 'pct_start': 0.38448626872171293}. Best is trial 28 with value: 19.30269633220601.\n",
      "[I 2025-12-16 02:21:05,932] Trial 49 finished with value: 20.180820930954855 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.24355510811468106, 'max_lr': 0.0003098832959727607, 'div_factor': 379, 'final_div_factor': 739, 'weight_decay': 4.842140032925616e-06, 'pct_start': 0.2958783885283159}. Best is trial 28 with value: 19.30269633220601.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    attributes=attributes,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    epochs=epochs,\n",
    "    path_vision=path_vision,\n",
    "    path_mlp=path_mlp\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating top-5 trials once at 100 epochs (seed=0)...\n",
      "\n",
      "→ Single-pass full run (Trial 28, ValObjective: 19.3027)\n",
      "\n",
      "Best Trial: 28\n",
      "  Best Score: 19.3027\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256,128,64]\n",
      "    fusion_dropout: 0.015863516678051887\n",
      "    max_lr: 0.00018948370634940513\n",
      "    div_factor: 11\n",
      "    final_div_factor: 537\n",
      "    weight_decay: 2.3609548800261087e-06\n",
      "    pct_start: 0.3453619262098891\n",
      "  Params: total=5,501,761  trainable=5,501,761\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.9 M   \n",
      "fwd MACs:                                                               584.47 MMACs\n",
      "fwd FLOPs:                                                              1.17 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.75 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  4.9 M = 100% Params, 584.47 MMACs = 100% MACs, 1.17 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "      (0): Conv2d(1.73 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.08 MFLOPS = 0.26% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    4.9 M = 99.96% Params, 582.93 MMACs = 99.74% MACs, 1.17 GFLOPS = 99.72% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 1.51% Params, 65.69 MMACs = 11.24% MACs, 131.73 MFLOPS = 11.25% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      230.14 K = 4.7% Params, 63.08 MMACs = 10.79% MACs, 126.44 MFLOPS = 10.8% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 1.51% Params, 20.28 MMACs = 3.47% MACs, 40.55 MFLOPS = 3.46% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 3.01% Params, 40.55 MMACs = 6.94% MACs, 81.1 MFLOPS = 6.93% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        8.45 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.58 MFLOPS = 0.39% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.51 MFLOPS = 0.38% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      919.04 K = 18.76% Params, 90.83 MMACs = 15.54% MACs, 181.87 MFLOPS = 15.54% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 6.02% Params, 29.2 MMACs = 5% MACs, 58.39 MFLOPS = 4.99% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 12.04% Params, 58.39 MMACs = 9.99% MACs, 116.79 MFLOPS = 9.98% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        33.28 K = 0.68% Params, 3.24 MMACs = 0.56% MACs, 6.54 MFLOPS = 0.56% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.67% Params, 3.24 MMACs = 0.56% MACs, 6.49 MFLOPS = 0.55% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      3.67 M = 74.99% Params, 363.33 MMACs = 62.16% MACs, 727.07 MFLOPS = 62.12% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 24.08% Params, 116.79 MMACs = 19.98% MACs, 233.57 MFLOPS = 19.96% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 48.17% Params, 233.57 MMACs = 39.96% MACs, 467.14 MFLOPS = 39.91% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        132.1 K = 2.7% Params, 12.98 MMACs = 2.22% MACs, 26.05 MFLOPS = 2.23% FLOPs\n",
      "        (0): Conv2d(131.07 K = 2.68% Params, 12.98 MMACs = 2.22% MACs, 25.95 MFLOPS = 2.22% FLOPs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  303.62 K\n",
      "fwd MACs:                                                               3.33 MMACs\n",
      "fwd FLOPs:                                                              6.67 MFLOPS\n",
      "fwd+bwd MACs:                                                           10 MMACs\n",
      "fwd+bwd FLOPs:                                                          20.02 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  303.62 K = 100% Params, 3.33 MMACs = 100% MACs, 6.67 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(262.4 K = 86.42% Params, 2.88 MMACs = 86.47% MACs, 5.77 MFLOPS = 86.4% FLOPs, in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.04% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.015863516678051887, inplace=False)\n",
      "  (3): Linear(32.9 K = 10.83% Params, 360.45 KMACs = 10.81% MACs, 720.9 KFLOPS = 10.8% FLOPs, in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.02% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.015863516678051887, inplace=False)\n",
      "  (6): Linear(8.26 K = 2.72% Params, 90.11 KMACs = 2.7% MACs, 180.22 KFLOPS = 2.7% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.01% FLOPs)\n",
      "  (8): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.015863516678051887, inplace=False)\n",
      "  (9): Linear(65 = 0.02% Params, 704 MACs = 0.02% MACs, 1.41 KFLOPS = 0.02% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_28/best_model.pth\n",
      "   val_rmse=20.033599\n",
      "   params: total=5,501,761, trainable=5,501,761\n",
      "→ Single-pass full run (Trial 41, ValObjective: 19.3797)\n",
      "\n",
      "Best Trial: 41\n",
      "  Best Score: 19.3797\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256,128,64]\n",
      "    fusion_dropout: 0.016945410153823337\n",
      "    max_lr: 0.00020741397823681787\n",
      "    div_factor: 369\n",
      "    final_div_factor: 628\n",
      "    weight_decay: 2.321591828874953e-06\n",
      "    pct_start: 0.37640417092932255\n",
      "  Params: total=5,501,761  trainable=5,501,761\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.9 M   \n",
      "fwd MACs:                                                               584.47 MMACs\n",
      "fwd FLOPs:                                                              1.17 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.75 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  4.9 M = 100% Params, 584.47 MMACs = 100% MACs, 1.17 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "      (0): Conv2d(1.73 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.08 MFLOPS = 0.26% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    4.9 M = 99.96% Params, 582.93 MMACs = 99.74% MACs, 1.17 GFLOPS = 99.72% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 1.51% Params, 65.69 MMACs = 11.24% MACs, 131.73 MFLOPS = 11.25% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      230.14 K = 4.7% Params, 63.08 MMACs = 10.79% MACs, 126.44 MFLOPS = 10.8% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 1.51% Params, 20.28 MMACs = 3.47% MACs, 40.55 MFLOPS = 3.46% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 3.01% Params, 40.55 MMACs = 6.94% MACs, 81.1 MFLOPS = 6.93% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        8.45 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.58 MFLOPS = 0.39% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.51 MFLOPS = 0.38% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      919.04 K = 18.76% Params, 90.83 MMACs = 15.54% MACs, 181.87 MFLOPS = 15.54% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 6.02% Params, 29.2 MMACs = 5% MACs, 58.39 MFLOPS = 4.99% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 12.04% Params, 58.39 MMACs = 9.99% MACs, 116.79 MFLOPS = 9.98% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        33.28 K = 0.68% Params, 3.24 MMACs = 0.56% MACs, 6.54 MFLOPS = 0.56% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.67% Params, 3.24 MMACs = 0.56% MACs, 6.49 MFLOPS = 0.55% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      3.67 M = 74.99% Params, 363.33 MMACs = 62.16% MACs, 727.07 MFLOPS = 62.12% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 24.08% Params, 116.79 MMACs = 19.98% MACs, 233.57 MFLOPS = 19.96% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 48.17% Params, 233.57 MMACs = 39.96% MACs, 467.14 MFLOPS = 39.91% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        132.1 K = 2.7% Params, 12.98 MMACs = 2.22% MACs, 26.05 MFLOPS = 2.23% FLOPs\n",
      "        (0): Conv2d(131.07 K = 2.68% Params, 12.98 MMACs = 2.22% MACs, 25.95 MFLOPS = 2.22% FLOPs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  303.62 K\n",
      "fwd MACs:                                                               3.33 MMACs\n",
      "fwd FLOPs:                                                              6.67 MFLOPS\n",
      "fwd+bwd MACs:                                                           10 MMACs\n",
      "fwd+bwd FLOPs:                                                          20.02 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  303.62 K = 100% Params, 3.33 MMACs = 100% MACs, 6.67 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(262.4 K = 86.42% Params, 2.88 MMACs = 86.47% MACs, 5.77 MFLOPS = 86.4% FLOPs, in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.04% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.016945410153823337, inplace=False)\n",
      "  (3): Linear(32.9 K = 10.83% Params, 360.45 KMACs = 10.81% MACs, 720.9 KFLOPS = 10.8% FLOPs, in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.02% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.016945410153823337, inplace=False)\n",
      "  (6): Linear(8.26 K = 2.72% Params, 90.11 KMACs = 2.7% MACs, 180.22 KFLOPS = 2.7% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.01% FLOPs)\n",
      "  (8): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.016945410153823337, inplace=False)\n",
      "  (9): Linear(65 = 0.02% Params, 704 MACs = 0.02% MACs, 1.41 KFLOPS = 0.02% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_41/best_model.pth\n",
      "   val_rmse=20.069003\n",
      "   params: total=5,501,761, trainable=5,501,761\n",
      "→ Single-pass full run (Trial 29, ValObjective: 19.4509)\n",
      "\n",
      "Best Trial: 29\n",
      "  Best Score: 19.4509\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256,128,64]\n",
      "    fusion_dropout: 0.025616060224870126\n",
      "    max_lr: 0.00041992110052155464\n",
      "    div_factor: 124\n",
      "    final_div_factor: 555\n",
      "    weight_decay: 2.079354445379162e-06\n",
      "    pct_start: 0.36699010266077514\n",
      "  Params: total=5,501,761  trainable=5,501,761\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.9 M   \n",
      "fwd MACs:                                                               584.47 MMACs\n",
      "fwd FLOPs:                                                              1.17 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.75 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  4.9 M = 100% Params, 584.47 MMACs = 100% MACs, 1.17 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "      (0): Conv2d(1.73 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.08 MFLOPS = 0.26% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    4.9 M = 99.96% Params, 582.93 MMACs = 99.74% MACs, 1.17 GFLOPS = 99.72% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 1.51% Params, 65.69 MMACs = 11.24% MACs, 131.73 MFLOPS = 11.25% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      230.14 K = 4.7% Params, 63.08 MMACs = 10.79% MACs, 126.44 MFLOPS = 10.8% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 1.51% Params, 20.28 MMACs = 3.47% MACs, 40.55 MFLOPS = 3.46% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 3.01% Params, 40.55 MMACs = 6.94% MACs, 81.1 MFLOPS = 6.93% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        8.45 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.58 MFLOPS = 0.39% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.51 MFLOPS = 0.38% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      919.04 K = 18.76% Params, 90.83 MMACs = 15.54% MACs, 181.87 MFLOPS = 15.54% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 6.02% Params, 29.2 MMACs = 5% MACs, 58.39 MFLOPS = 4.99% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 12.04% Params, 58.39 MMACs = 9.99% MACs, 116.79 MFLOPS = 9.98% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        33.28 K = 0.68% Params, 3.24 MMACs = 0.56% MACs, 6.54 MFLOPS = 0.56% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.67% Params, 3.24 MMACs = 0.56% MACs, 6.49 MFLOPS = 0.55% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      3.67 M = 74.99% Params, 363.33 MMACs = 62.16% MACs, 727.07 MFLOPS = 62.12% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 24.08% Params, 116.79 MMACs = 19.98% MACs, 233.57 MFLOPS = 19.96% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 48.17% Params, 233.57 MMACs = 39.96% MACs, 467.14 MFLOPS = 39.91% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        132.1 K = 2.7% Params, 12.98 MMACs = 2.22% MACs, 26.05 MFLOPS = 2.23% FLOPs\n",
      "        (0): Conv2d(131.07 K = 2.68% Params, 12.98 MMACs = 2.22% MACs, 25.95 MFLOPS = 2.22% FLOPs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  303.62 K\n",
      "fwd MACs:                                                               3.33 MMACs\n",
      "fwd FLOPs:                                                              6.67 MFLOPS\n",
      "fwd+bwd MACs:                                                           10 MMACs\n",
      "fwd+bwd FLOPs:                                                          20.02 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  303.62 K = 100% Params, 3.33 MMACs = 100% MACs, 6.67 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(262.4 K = 86.42% Params, 2.88 MMACs = 86.47% MACs, 5.77 MFLOPS = 86.4% FLOPs, in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.04% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.025616060224870126, inplace=False)\n",
      "  (3): Linear(32.9 K = 10.83% Params, 360.45 KMACs = 10.81% MACs, 720.9 KFLOPS = 10.8% FLOPs, in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.02% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.025616060224870126, inplace=False)\n",
      "  (6): Linear(8.26 K = 2.72% Params, 90.11 KMACs = 2.7% MACs, 180.22 KFLOPS = 2.7% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.01% FLOPs)\n",
      "  (8): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.025616060224870126, inplace=False)\n",
      "  (9): Linear(65 = 0.02% Params, 704 MACs = 0.02% MACs, 1.41 KFLOPS = 0.02% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_29/best_model.pth\n",
      "   val_rmse=20.233740\n",
      "   params: total=5,501,761, trainable=5,501,761\n",
      "→ Single-pass full run (Trial 30, ValObjective: 19.5210)\n",
      "\n",
      "Best Trial: 30\n",
      "  Best Score: 19.5210\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256,128,64]\n",
      "    fusion_dropout: 0.023335067489998362\n",
      "    max_lr: 0.0002079238381081898\n",
      "    div_factor: 250\n",
      "    final_div_factor: 725\n",
      "    weight_decay: 2.269056346293647e-06\n",
      "    pct_start: 0.3657608297080023\n",
      "  Params: total=5,501,761  trainable=5,501,761\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.9 M   \n",
      "fwd MACs:                                                               584.47 MMACs\n",
      "fwd FLOPs:                                                              1.17 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.75 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  4.9 M = 100% Params, 584.47 MMACs = 100% MACs, 1.17 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "      (0): Conv2d(1.73 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.08 MFLOPS = 0.26% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    4.9 M = 99.96% Params, 582.93 MMACs = 99.74% MACs, 1.17 GFLOPS = 99.72% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 1.51% Params, 65.69 MMACs = 11.24% MACs, 131.73 MFLOPS = 11.25% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      230.14 K = 4.7% Params, 63.08 MMACs = 10.79% MACs, 126.44 MFLOPS = 10.8% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 1.51% Params, 20.28 MMACs = 3.47% MACs, 40.55 MFLOPS = 3.46% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 3.01% Params, 40.55 MMACs = 6.94% MACs, 81.1 MFLOPS = 6.93% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        8.45 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.58 MFLOPS = 0.39% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.51 MFLOPS = 0.38% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      919.04 K = 18.76% Params, 90.83 MMACs = 15.54% MACs, 181.87 MFLOPS = 15.54% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 6.02% Params, 29.2 MMACs = 5% MACs, 58.39 MFLOPS = 4.99% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 12.04% Params, 58.39 MMACs = 9.99% MACs, 116.79 MFLOPS = 9.98% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        33.28 K = 0.68% Params, 3.24 MMACs = 0.56% MACs, 6.54 MFLOPS = 0.56% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.67% Params, 3.24 MMACs = 0.56% MACs, 6.49 MFLOPS = 0.55% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      3.67 M = 74.99% Params, 363.33 MMACs = 62.16% MACs, 727.07 MFLOPS = 62.12% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 24.08% Params, 116.79 MMACs = 19.98% MACs, 233.57 MFLOPS = 19.96% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 48.17% Params, 233.57 MMACs = 39.96% MACs, 467.14 MFLOPS = 39.91% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        132.1 K = 2.7% Params, 12.98 MMACs = 2.22% MACs, 26.05 MFLOPS = 2.23% FLOPs\n",
      "        (0): Conv2d(131.07 K = 2.68% Params, 12.98 MMACs = 2.22% MACs, 25.95 MFLOPS = 2.22% FLOPs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  303.62 K\n",
      "fwd MACs:                                                               3.33 MMACs\n",
      "fwd FLOPs:                                                              6.67 MFLOPS\n",
      "fwd+bwd MACs:                                                           10 MMACs\n",
      "fwd+bwd FLOPs:                                                          20.02 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  303.62 K = 100% Params, 3.33 MMACs = 100% MACs, 6.67 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(262.4 K = 86.42% Params, 2.88 MMACs = 86.47% MACs, 5.77 MFLOPS = 86.4% FLOPs, in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.04% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (3): Linear(32.9 K = 10.83% Params, 360.45 KMACs = 10.81% MACs, 720.9 KFLOPS = 10.8% FLOPs, in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.02% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (6): Linear(8.26 K = 2.72% Params, 90.11 KMACs = 2.7% MACs, 180.22 KFLOPS = 2.7% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.01% FLOPs)\n",
      "  (8): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (9): Linear(65 = 0.02% Params, 704 MACs = 0.02% MACs, 1.41 KFLOPS = 0.02% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_30/best_model.pth\n",
      "   val_rmse=19.510013\n",
      "   params: total=5,501,761, trainable=5,501,761\n",
      "→ Single-pass full run (Trial 26, ValObjective: 19.5629)\n",
      "\n",
      "Best Trial: 26\n",
      "  Best Score: 19.5629\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256,128,64]\n",
      "    fusion_dropout: 0.01041878271830694\n",
      "    max_lr: 0.0011721975535196815\n",
      "    div_factor: 96\n",
      "    final_div_factor: 530\n",
      "    weight_decay: 1.0312746387906183e-05\n",
      "    pct_start: 0.3690842515553291\n",
      "  Params: total=5,501,761  trainable=5,501,761\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.9 M   \n",
      "fwd MACs:                                                               584.47 MMACs\n",
      "fwd FLOPs:                                                              1.17 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.75 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  4.9 M = 100% Params, 584.47 MMACs = 100% MACs, 1.17 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "      (0): Conv2d(1.73 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.08 MFLOPS = 0.26% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    4.9 M = 99.96% Params, 582.93 MMACs = 99.74% MACs, 1.17 GFLOPS = 99.72% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 1.51% Params, 65.69 MMACs = 11.24% MACs, 131.73 MFLOPS = 11.25% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      230.14 K = 4.7% Params, 63.08 MMACs = 10.79% MACs, 126.44 MFLOPS = 10.8% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 1.51% Params, 20.28 MMACs = 3.47% MACs, 40.55 MFLOPS = 3.46% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 3.01% Params, 40.55 MMACs = 6.94% MACs, 81.1 MFLOPS = 6.93% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        8.45 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.58 MFLOPS = 0.39% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.51 MFLOPS = 0.38% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      919.04 K = 18.76% Params, 90.83 MMACs = 15.54% MACs, 181.87 MFLOPS = 15.54% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 6.02% Params, 29.2 MMACs = 5% MACs, 58.39 MFLOPS = 4.99% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 12.04% Params, 58.39 MMACs = 9.99% MACs, 116.79 MFLOPS = 9.98% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        33.28 K = 0.68% Params, 3.24 MMACs = 0.56% MACs, 6.54 MFLOPS = 0.56% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.67% Params, 3.24 MMACs = 0.56% MACs, 6.49 MFLOPS = 0.55% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      3.67 M = 74.99% Params, 363.33 MMACs = 62.16% MACs, 727.07 MFLOPS = 62.12% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 24.08% Params, 116.79 MMACs = 19.98% MACs, 233.57 MFLOPS = 19.96% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 48.17% Params, 233.57 MMACs = 39.96% MACs, 467.14 MFLOPS = 39.91% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        132.1 K = 2.7% Params, 12.98 MMACs = 2.22% MACs, 26.05 MFLOPS = 2.23% FLOPs\n",
      "        (0): Conv2d(131.07 K = 2.68% Params, 12.98 MMACs = 2.22% MACs, 25.95 MFLOPS = 2.22% FLOPs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  303.62 K\n",
      "fwd MACs:                                                               3.33 MMACs\n",
      "fwd FLOPs:                                                              6.67 MFLOPS\n",
      "fwd+bwd MACs:                                                           10 MMACs\n",
      "fwd+bwd FLOPs:                                                          20.02 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  303.62 K = 100% Params, 3.33 MMACs = 100% MACs, 6.67 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(262.4 K = 86.42% Params, 2.88 MMACs = 86.47% MACs, 5.77 MFLOPS = 86.4% FLOPs, in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.04% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.01041878271830694, inplace=False)\n",
      "  (3): Linear(32.9 K = 10.83% Params, 360.45 KMACs = 10.81% MACs, 720.9 KFLOPS = 10.8% FLOPs, in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.02% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.01041878271830694, inplace=False)\n",
      "  (6): Linear(8.26 K = 2.72% Params, 90.11 KMACs = 2.7% MACs, 180.22 KFLOPS = 2.7% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.01% FLOPs)\n",
      "  (8): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.01041878271830694, inplace=False)\n",
      "  (9): Linear(65 = 0.02% Params, 704 MACs = 0.02% MACs, 1.41 KFLOPS = 0.02% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_26/best_model.pth\n",
      "   val_rmse=20.350950\n",
      "   params: total=5,501,761, trainable=5,501,761\n",
      "\n",
      "Winner after single-pass: Trial 30 (trial_30) by val_rmse=19.510013\n",
      "\n",
      "Re-running winner with seeds [0, 1, 2, 3, 4] at 100 epochs...\n",
      "\n",
      "\n",
      "Best Trial: 30\n",
      "  Best Score: 19.5210\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256,128,64]\n",
      "    fusion_dropout: 0.023335067489998362\n",
      "    max_lr: 0.0002079238381081898\n",
      "    div_factor: 250\n",
      "    final_div_factor: 725\n",
      "    weight_decay: 2.269056346293647e-06\n",
      "    pct_start: 0.3657608297080023\n",
      "  Params: total=5,501,761  trainable=5,501,761\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.9 M   \n",
      "fwd MACs:                                                               584.47 MMACs\n",
      "fwd FLOPs:                                                              1.17 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.75 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  4.9 M = 100% Params, 584.47 MMACs = 100% MACs, 1.17 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "      (0): Conv2d(1.73 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.08 MFLOPS = 0.26% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    4.9 M = 99.96% Params, 582.93 MMACs = 99.74% MACs, 1.17 GFLOPS = 99.72% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 1.51% Params, 65.69 MMACs = 11.24% MACs, 131.73 MFLOPS = 11.25% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      230.14 K = 4.7% Params, 63.08 MMACs = 10.79% MACs, 126.44 MFLOPS = 10.8% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 1.51% Params, 20.28 MMACs = 3.47% MACs, 40.55 MFLOPS = 3.46% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 3.01% Params, 40.55 MMACs = 6.94% MACs, 81.1 MFLOPS = 6.93% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        8.45 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.58 MFLOPS = 0.39% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.51 MFLOPS = 0.38% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      919.04 K = 18.76% Params, 90.83 MMACs = 15.54% MACs, 181.87 MFLOPS = 15.54% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 6.02% Params, 29.2 MMACs = 5% MACs, 58.39 MFLOPS = 4.99% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 12.04% Params, 58.39 MMACs = 9.99% MACs, 116.79 MFLOPS = 9.98% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        33.28 K = 0.68% Params, 3.24 MMACs = 0.56% MACs, 6.54 MFLOPS = 0.56% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.67% Params, 3.24 MMACs = 0.56% MACs, 6.49 MFLOPS = 0.55% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      3.67 M = 74.99% Params, 363.33 MMACs = 62.16% MACs, 727.07 MFLOPS = 62.12% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 24.08% Params, 116.79 MMACs = 19.98% MACs, 233.57 MFLOPS = 19.96% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 48.17% Params, 233.57 MMACs = 39.96% MACs, 467.14 MFLOPS = 39.91% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        132.1 K = 2.7% Params, 12.98 MMACs = 2.22% MACs, 26.05 MFLOPS = 2.23% FLOPs\n",
      "        (0): Conv2d(131.07 K = 2.68% Params, 12.98 MMACs = 2.22% MACs, 25.95 MFLOPS = 2.22% FLOPs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  303.62 K\n",
      "fwd MACs:                                                               3.33 MMACs\n",
      "fwd FLOPs:                                                              6.67 MFLOPS\n",
      "fwd+bwd MACs:                                                           10 MMACs\n",
      "fwd+bwd FLOPs:                                                          20.02 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  303.62 K = 100% Params, 3.33 MMACs = 100% MACs, 6.67 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(262.4 K = 86.42% Params, 2.88 MMACs = 86.47% MACs, 5.77 MFLOPS = 86.4% FLOPs, in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.04% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (3): Linear(32.9 K = 10.83% Params, 360.45 KMACs = 10.81% MACs, 720.9 KFLOPS = 10.8% FLOPs, in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.02% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (6): Linear(8.26 K = 2.72% Params, 90.11 KMACs = 2.7% MACs, 180.22 KFLOPS = 2.7% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.01% FLOPs)\n",
      "  (8): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (9): Linear(65 = 0.02% Params, 704 MACs = 0.02% MACs, 1.41 KFLOPS = 0.02% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_30_seed0/best_model.pth\n",
      "   Seed 0: val_rmse=19.510013, test_loss=787.730835, test_rmse=28.104787, val_loss=385.617956\n",
      "\n",
      "Best Trial: 30\n",
      "  Best Score: 19.5210\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256,128,64]\n",
      "    fusion_dropout: 0.023335067489998362\n",
      "    max_lr: 0.0002079238381081898\n",
      "    div_factor: 250\n",
      "    final_div_factor: 725\n",
      "    weight_decay: 2.269056346293647e-06\n",
      "    pct_start: 0.3657608297080023\n",
      "  Params: total=5,501,761  trainable=5,501,761\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.9 M   \n",
      "fwd MACs:                                                               584.47 MMACs\n",
      "fwd FLOPs:                                                              1.17 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.75 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  4.9 M = 100% Params, 584.47 MMACs = 100% MACs, 1.17 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "      (0): Conv2d(1.73 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.08 MFLOPS = 0.26% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    4.9 M = 99.96% Params, 582.93 MMACs = 99.74% MACs, 1.17 GFLOPS = 99.72% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 1.51% Params, 65.69 MMACs = 11.24% MACs, 131.73 MFLOPS = 11.25% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      230.14 K = 4.7% Params, 63.08 MMACs = 10.79% MACs, 126.44 MFLOPS = 10.8% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 1.51% Params, 20.28 MMACs = 3.47% MACs, 40.55 MFLOPS = 3.46% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 3.01% Params, 40.55 MMACs = 6.94% MACs, 81.1 MFLOPS = 6.93% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        8.45 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.58 MFLOPS = 0.39% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.51 MFLOPS = 0.38% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      919.04 K = 18.76% Params, 90.83 MMACs = 15.54% MACs, 181.87 MFLOPS = 15.54% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 6.02% Params, 29.2 MMACs = 5% MACs, 58.39 MFLOPS = 4.99% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 12.04% Params, 58.39 MMACs = 9.99% MACs, 116.79 MFLOPS = 9.98% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        33.28 K = 0.68% Params, 3.24 MMACs = 0.56% MACs, 6.54 MFLOPS = 0.56% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.67% Params, 3.24 MMACs = 0.56% MACs, 6.49 MFLOPS = 0.55% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      3.67 M = 74.99% Params, 363.33 MMACs = 62.16% MACs, 727.07 MFLOPS = 62.12% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 24.08% Params, 116.79 MMACs = 19.98% MACs, 233.57 MFLOPS = 19.96% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 48.17% Params, 233.57 MMACs = 39.96% MACs, 467.14 MFLOPS = 39.91% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        132.1 K = 2.7% Params, 12.98 MMACs = 2.22% MACs, 26.05 MFLOPS = 2.23% FLOPs\n",
      "        (0): Conv2d(131.07 K = 2.68% Params, 12.98 MMACs = 2.22% MACs, 25.95 MFLOPS = 2.22% FLOPs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  303.62 K\n",
      "fwd MACs:                                                               3.33 MMACs\n",
      "fwd FLOPs:                                                              6.67 MFLOPS\n",
      "fwd+bwd MACs:                                                           10 MMACs\n",
      "fwd+bwd FLOPs:                                                          20.02 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  303.62 K = 100% Params, 3.33 MMACs = 100% MACs, 6.67 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(262.4 K = 86.42% Params, 2.88 MMACs = 86.47% MACs, 5.77 MFLOPS = 86.4% FLOPs, in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.04% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (3): Linear(32.9 K = 10.83% Params, 360.45 KMACs = 10.81% MACs, 720.9 KFLOPS = 10.8% FLOPs, in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.02% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (6): Linear(8.26 K = 2.72% Params, 90.11 KMACs = 2.7% MACs, 180.22 KFLOPS = 2.7% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.01% FLOPs)\n",
      "  (8): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (9): Linear(65 = 0.02% Params, 704 MACs = 0.02% MACs, 1.41 KFLOPS = 0.02% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_30_seed1/best_model.pth\n",
      "   Seed 1: val_rmse=19.692072, test_loss=785.983465, test_rmse=28.033392, val_loss=390.935969\n",
      "\n",
      "Best Trial: 30\n",
      "  Best Score: 19.5210\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256,128,64]\n",
      "    fusion_dropout: 0.023335067489998362\n",
      "    max_lr: 0.0002079238381081898\n",
      "    div_factor: 250\n",
      "    final_div_factor: 725\n",
      "    weight_decay: 2.269056346293647e-06\n",
      "    pct_start: 0.3657608297080023\n",
      "  Params: total=5,501,761  trainable=5,501,761\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.9 M   \n",
      "fwd MACs:                                                               584.47 MMACs\n",
      "fwd FLOPs:                                                              1.17 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.75 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  4.9 M = 100% Params, 584.47 MMACs = 100% MACs, 1.17 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "      (0): Conv2d(1.73 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.08 MFLOPS = 0.26% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    4.9 M = 99.96% Params, 582.93 MMACs = 99.74% MACs, 1.17 GFLOPS = 99.72% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 1.51% Params, 65.69 MMACs = 11.24% MACs, 131.73 MFLOPS = 11.25% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      230.14 K = 4.7% Params, 63.08 MMACs = 10.79% MACs, 126.44 MFLOPS = 10.8% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 1.51% Params, 20.28 MMACs = 3.47% MACs, 40.55 MFLOPS = 3.46% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 3.01% Params, 40.55 MMACs = 6.94% MACs, 81.1 MFLOPS = 6.93% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        8.45 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.58 MFLOPS = 0.39% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.51 MFLOPS = 0.38% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      919.04 K = 18.76% Params, 90.83 MMACs = 15.54% MACs, 181.87 MFLOPS = 15.54% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 6.02% Params, 29.2 MMACs = 5% MACs, 58.39 MFLOPS = 4.99% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 12.04% Params, 58.39 MMACs = 9.99% MACs, 116.79 MFLOPS = 9.98% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        33.28 K = 0.68% Params, 3.24 MMACs = 0.56% MACs, 6.54 MFLOPS = 0.56% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.67% Params, 3.24 MMACs = 0.56% MACs, 6.49 MFLOPS = 0.55% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      3.67 M = 74.99% Params, 363.33 MMACs = 62.16% MACs, 727.07 MFLOPS = 62.12% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 24.08% Params, 116.79 MMACs = 19.98% MACs, 233.57 MFLOPS = 19.96% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 48.17% Params, 233.57 MMACs = 39.96% MACs, 467.14 MFLOPS = 39.91% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        132.1 K = 2.7% Params, 12.98 MMACs = 2.22% MACs, 26.05 MFLOPS = 2.23% FLOPs\n",
      "        (0): Conv2d(131.07 K = 2.68% Params, 12.98 MMACs = 2.22% MACs, 25.95 MFLOPS = 2.22% FLOPs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  303.62 K\n",
      "fwd MACs:                                                               3.33 MMACs\n",
      "fwd FLOPs:                                                              6.67 MFLOPS\n",
      "fwd+bwd MACs:                                                           10 MMACs\n",
      "fwd+bwd FLOPs:                                                          20.02 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  303.62 K = 100% Params, 3.33 MMACs = 100% MACs, 6.67 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(262.4 K = 86.42% Params, 2.88 MMACs = 86.47% MACs, 5.77 MFLOPS = 86.4% FLOPs, in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.04% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (3): Linear(32.9 K = 10.83% Params, 360.45 KMACs = 10.81% MACs, 720.9 KFLOPS = 10.8% FLOPs, in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.02% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (6): Linear(8.26 K = 2.72% Params, 90.11 KMACs = 2.7% MACs, 180.22 KFLOPS = 2.7% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.01% FLOPs)\n",
      "  (8): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (9): Linear(65 = 0.02% Params, 704 MACs = 0.02% MACs, 1.41 KFLOPS = 0.02% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_30_seed2/best_model.pth\n",
      "   Seed 2: val_rmse=19.757219, test_loss=809.839513, test_rmse=28.410573, val_loss=394.206355\n",
      "\n",
      "Best Trial: 30\n",
      "  Best Score: 19.5210\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256,128,64]\n",
      "    fusion_dropout: 0.023335067489998362\n",
      "    max_lr: 0.0002079238381081898\n",
      "    div_factor: 250\n",
      "    final_div_factor: 725\n",
      "    weight_decay: 2.269056346293647e-06\n",
      "    pct_start: 0.3657608297080023\n",
      "  Params: total=5,501,761  trainable=5,501,761\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.9 M   \n",
      "fwd MACs:                                                               584.47 MMACs\n",
      "fwd FLOPs:                                                              1.17 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.75 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  4.9 M = 100% Params, 584.47 MMACs = 100% MACs, 1.17 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "      (0): Conv2d(1.73 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.08 MFLOPS = 0.26% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    4.9 M = 99.96% Params, 582.93 MMACs = 99.74% MACs, 1.17 GFLOPS = 99.72% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 1.51% Params, 65.69 MMACs = 11.24% MACs, 131.73 MFLOPS = 11.25% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      230.14 K = 4.7% Params, 63.08 MMACs = 10.79% MACs, 126.44 MFLOPS = 10.8% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 1.51% Params, 20.28 MMACs = 3.47% MACs, 40.55 MFLOPS = 3.46% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 3.01% Params, 40.55 MMACs = 6.94% MACs, 81.1 MFLOPS = 6.93% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        8.45 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.58 MFLOPS = 0.39% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.51 MFLOPS = 0.38% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      919.04 K = 18.76% Params, 90.83 MMACs = 15.54% MACs, 181.87 MFLOPS = 15.54% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 6.02% Params, 29.2 MMACs = 5% MACs, 58.39 MFLOPS = 4.99% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 12.04% Params, 58.39 MMACs = 9.99% MACs, 116.79 MFLOPS = 9.98% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        33.28 K = 0.68% Params, 3.24 MMACs = 0.56% MACs, 6.54 MFLOPS = 0.56% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.67% Params, 3.24 MMACs = 0.56% MACs, 6.49 MFLOPS = 0.55% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      3.67 M = 74.99% Params, 363.33 MMACs = 62.16% MACs, 727.07 MFLOPS = 62.12% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 24.08% Params, 116.79 MMACs = 19.98% MACs, 233.57 MFLOPS = 19.96% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 48.17% Params, 233.57 MMACs = 39.96% MACs, 467.14 MFLOPS = 39.91% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        132.1 K = 2.7% Params, 12.98 MMACs = 2.22% MACs, 26.05 MFLOPS = 2.23% FLOPs\n",
      "        (0): Conv2d(131.07 K = 2.68% Params, 12.98 MMACs = 2.22% MACs, 25.95 MFLOPS = 2.22% FLOPs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  303.62 K\n",
      "fwd MACs:                                                               3.33 MMACs\n",
      "fwd FLOPs:                                                              6.67 MFLOPS\n",
      "fwd+bwd MACs:                                                           10 MMACs\n",
      "fwd+bwd FLOPs:                                                          20.02 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  303.62 K = 100% Params, 3.33 MMACs = 100% MACs, 6.67 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(262.4 K = 86.42% Params, 2.88 MMACs = 86.47% MACs, 5.77 MFLOPS = 86.4% FLOPs, in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.04% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (3): Linear(32.9 K = 10.83% Params, 360.45 KMACs = 10.81% MACs, 720.9 KFLOPS = 10.8% FLOPs, in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.02% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (6): Linear(8.26 K = 2.72% Params, 90.11 KMACs = 2.7% MACs, 180.22 KFLOPS = 2.7% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.01% FLOPs)\n",
      "  (8): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (9): Linear(65 = 0.02% Params, 704 MACs = 0.02% MACs, 1.41 KFLOPS = 0.02% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_30_seed3/best_model.pth\n",
      "   Seed 3: val_rmse=19.625977, test_loss=788.990397, test_rmse=28.068382, val_loss=387.918472\n",
      "\n",
      "Best Trial: 30\n",
      "  Best Score: 19.5210\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256,128,64]\n",
      "    fusion_dropout: 0.023335067489998362\n",
      "    max_lr: 0.0002079238381081898\n",
      "    div_factor: 250\n",
      "    final_div_factor: 725\n",
      "    weight_decay: 2.269056346293647e-06\n",
      "    pct_start: 0.3657608297080023\n",
      "  Params: total=5,501,761  trainable=5,501,761\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.9 M   \n",
      "fwd MACs:                                                               584.47 MMACs\n",
      "fwd FLOPs:                                                              1.17 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.75 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  4.9 M = 100% Params, 584.47 MMACs = 100% MACs, 1.17 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.25 MFLOPS = 0.28% FLOPs\n",
      "      (0): Conv2d(1.73 K = 0.04% Params, 1.54 MMACs = 0.26% MACs, 3.08 MFLOPS = 0.26% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    4.9 M = 99.96% Params, 582.93 MMACs = 99.74% MACs, 1.17 GFLOPS = 99.72% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 1.51% Params, 65.69 MMACs = 11.24% MACs, 131.73 MFLOPS = 11.25% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.75% Params, 32.85 MMACs = 5.62% MACs, 65.69 MFLOPS = 5.61% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      230.14 K = 4.7% Params, 63.08 MMACs = 10.79% MACs, 126.44 MFLOPS = 10.8% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 1.51% Params, 20.28 MMACs = 3.47% MACs, 40.55 MFLOPS = 3.46% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 3.01% Params, 40.55 MMACs = 6.94% MACs, 81.1 MFLOPS = 6.93% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        8.45 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.58 MFLOPS = 0.39% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.17% Params, 2.25 MMACs = 0.39% MACs, 4.51 MFLOPS = 0.38% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.01% Params, 0 MACs = 0% MACs, 70.4 KFLOPS = 0.01% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      919.04 K = 18.76% Params, 90.83 MMACs = 15.54% MACs, 181.87 MFLOPS = 15.54% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 6.02% Params, 29.2 MMACs = 5% MACs, 58.39 MFLOPS = 4.99% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 12.04% Params, 58.39 MMACs = 9.99% MACs, 116.79 MFLOPS = 9.98% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        33.28 K = 0.68% Params, 3.24 MMACs = 0.56% MACs, 6.54 MFLOPS = 0.56% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.67% Params, 3.24 MMACs = 0.56% MACs, 6.49 MFLOPS = 0.55% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.01% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      3.67 M = 74.99% Params, 363.33 MMACs = 62.16% MACs, 727.07 MFLOPS = 62.12% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 24.08% Params, 116.79 MMACs = 19.98% MACs, 233.57 MFLOPS = 19.96% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 48.17% Params, 233.57 MMACs = 39.96% MACs, 467.14 MFLOPS = 39.91% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        132.1 K = 2.7% Params, 12.98 MMACs = 2.22% MACs, 26.05 MFLOPS = 2.23% FLOPs\n",
      "        (0): Conv2d(131.07 K = 2.68% Params, 12.98 MMACs = 2.22% MACs, 25.95 MFLOPS = 2.22% FLOPs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.02% Params, 0 MACs = 0% MACs, 101.38 KFLOPS = 0.01% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 50.69 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  303.62 K\n",
      "fwd MACs:                                                               3.33 MMACs\n",
      "fwd FLOPs:                                                              6.67 MFLOPS\n",
      "fwd+bwd MACs:                                                           10 MMACs\n",
      "fwd+bwd FLOPs:                                                          20.02 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  303.62 K = 100% Params, 3.33 MMACs = 100% MACs, 6.67 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(262.4 K = 86.42% Params, 2.88 MMACs = 86.47% MACs, 5.77 MFLOPS = 86.4% FLOPs, in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.04% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (3): Linear(32.9 K = 10.83% Params, 360.45 KMACs = 10.81% MACs, 720.9 KFLOPS = 10.8% FLOPs, in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.02% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (6): Linear(8.26 K = 2.72% Params, 90.11 KMACs = 2.7% MACs, 180.22 KFLOPS = 2.7% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.01% FLOPs)\n",
      "  (8): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.023335067489998362, inplace=False)\n",
      "  (9): Linear(65 = 0.02% Params, 704 MACs = 0.02% MACs, 1.41 KFLOPS = 0.02% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_30_seed4/best_model.pth\n",
      "   Seed 4: val_rmse=19.678950, test_loss=860.753062, test_rmse=29.327373, val_loss=388.756159\n",
      "\n",
      "Winner aggregated val_rmse: 19.652846 ± 0.092516\n",
      "Model params: total=5,501,761, trainable=5,501,761\n",
      "Saved multi-seed summary to: logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_30/winner_multi_seed_summary.txt\n",
      "{'winner_trial_number': 30, 'winner_trial_name': 'trial_30', 'primary_metric': 'val_rmse', 'aggregates': {'train_loss': {'mean': 308.2162061903212, 'std': 35.38751801821283}, 'val_loss': {'mean': 389.4869822184245, 'std': 3.253295751345376}, 'test_loss': {'mean': 806.6594543457031, 'std': 31.75773891373198}, 'min_lr': {'mean': 8.316953524327592e-07, 'std': 0.0}, 'max_lr': {'mean': 0.0002079238381081898, 'std': 0.0}, 'total_time': {'mean': 25.530247259140015, 'std': 0.23802775761645914}, 'average_epoch_time': {'mean': 0.2542916927337647, 'std': 0.002324624983629471}, 'train_mse': {'mean': 308.25702514648435, 'std': 35.40103445724767}, 'train_mae': {'mean': 13.722779083251954, 'std': 0.9604551917243477}, 'train_rmse': {'mean': 17.53414906082986, 'std': 1.0066291888547945}, 'train_r2': {'mean': 0.9641521215438843, 'std': 0.004116869000669348}, 'val_mse': {'mean': 386.2412109375, 'std': 3.631771328439357}, 'val_mae': {'mean': 15.396028518676758, 'std': 0.2854036493853457}, 'val_rmse': {'mean': 19.652846194988285, 'std': 0.09251603668049835}, 'val_r2': {'mean': 0.9450377821922302, 'std': 0.0005168076760049482}, 'test_mse': {'mean': 806.167919921875, 'std': 31.314983060595345}, 'test_mae': {'mean': 22.590272903442383, 'std': 0.6278121175060335}, 'test_rmse': {'mean': 28.388901214618073, 'std': 0.5456736072637799}, 'test_r2': {'mean': 0.9061732649803161, 'std': 0.003644634582326709}, 'total_params': {'mean': 5501761.0, 'std': 0.0}, 'trainable_params': {'mean': 5501761.0, 'std': 0.0}, 'flops': {'mean': 1183666176.0, 'std': 0.0}, 'macs': {'mean': 591098112.0, 'std': 0.0}}, 'total_params': 5501761, 'trainable_params': 5501761, 'flops': 1183666176.0, 'macs': 591098112.0, 'summary_path': 'logs/Regression/Moneyball_cleaned/CNN_hybrid/IGTD/best_model/trial_30/winner_multi_seed_summary.txt'}\n"
     ]
    }
   ],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     attributes=attributes,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "     path_vision=path_vision, path_mlp=path_mlp,\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### REFINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "name = f\"REFINED\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes — Train: (862, 72), Val: (185, 72), Test: (185, 72)\n",
      "Numerical features: 8 — ['Year', 'RA', 'W', 'OBP', 'SLG', 'BA', 'OOBP', 'OSLG']\n",
      "Categorical features: 6 — ['Team', 'League', 'Playoffs', 'RankSeason', 'RankPlayoffs', 'G']\n",
      "Total features: 72\n",
      "Images shape (C,H,W): (3, 9, 9)\n",
      "Attributes: 72\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=False, target_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 9]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vision=f\"./logs/{task_type}/{dataset_name}/{vision_name}/{name}/best_model/trial_64\"\n",
    "path_mlp=f\"./logs/{task_type}/{dataset_name}/mlp/best_model/trial_38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-16 02:27:56,210] A new study created in memory with name: no-name-09f399b1-8034-4740-8691-7fc3ee6d508b\n",
      "[I 2025-12-16 02:28:34,398] Trial 0 finished with value: 33.52452897459933 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.26174050067022525, 'max_lr': 4.776127755142375e-05, 'div_factor': 80, 'final_div_factor': 847, 'weight_decay': 0.0007952349191815828, 'pct_start': 0.24799464568528354}. Best is trial 0 with value: 33.52452897459933.\n",
      "[I 2025-12-16 02:29:09,859] Trial 1 finished with value: 61.4334505595791 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.12553688051819545, 'max_lr': 3.1452418124908296e-05, 'div_factor': 57, 'final_div_factor': 640, 'weight_decay': 0.0022708618267781406, 'pct_start': 0.22943556239518603}. Best is trial 0 with value: 33.52452897459933.\n",
      "[I 2025-12-16 02:29:44,645] Trial 2 finished with value: 26.485397058081247 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.006804800453664495, 'max_lr': 0.0001869063931474541, 'div_factor': 291, 'final_div_factor': 992, 'weight_decay': 7.773296593045869e-05, 'pct_start': 0.2839847630582585}. Best is trial 2 with value: 26.485397058081247.\n",
      "[I 2025-12-16 02:30:19,015] Trial 3 finished with value: 20.786911822707093 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.25965000532441007, 'max_lr': 0.0005943753114457669, 'div_factor': 549, 'final_div_factor': 913, 'weight_decay': 0.0007028201004356825, 'pct_start': 0.19965586943484895}. Best is trial 3 with value: 20.786911822707093.\n",
      "[I 2025-12-16 02:30:53,789] Trial 4 finished with value: 21.884648402324387 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.24679843217439829, 'max_lr': 0.0003629537790447069, 'div_factor': 387, 'final_div_factor': 706, 'weight_decay': 1.0020700553637683e-06, 'pct_start': 0.22896612675840491}. Best is trial 3 with value: 20.786911822707093.\n",
      "[I 2025-12-16 02:31:28,384] Trial 5 finished with value: 20.801332519103493 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.2618984697879775, 'max_lr': 0.00972009120482617, 'div_factor': 277, 'final_div_factor': 146, 'weight_decay': 9.838078338279079e-06, 'pct_start': 0.22073896879983324}. Best is trial 3 with value: 20.786911822707093.\n",
      "[I 2025-12-16 02:32:03,491] Trial 6 finished with value: 28.860489539252665 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.2004672755976579, 'max_lr': 7.893787417013926e-05, 'div_factor': 110, 'final_div_factor': 129, 'weight_decay': 1.507992531429914e-06, 'pct_start': 0.3106878142752272}. Best is trial 3 with value: 20.786911822707093.\n",
      "[I 2025-12-16 02:32:39,059] Trial 7 finished with value: 21.569461483800932 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.23587343008039335, 'max_lr': 0.000811900021946068, 'div_factor': 365, 'final_div_factor': 912, 'weight_decay': 0.0007242282664027743, 'pct_start': 0.3353836638340746}. Best is trial 3 with value: 20.786911822707093.\n",
      "[I 2025-12-16 02:33:14,773] Trial 8 finished with value: 22.81114614989512 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.22521001466809004, 'max_lr': 0.0026430388018443426, 'div_factor': 192, 'final_div_factor': 689, 'weight_decay': 1.6229967462161971e-06, 'pct_start': 0.37349428129310613}. Best is trial 3 with value: 20.786911822707093.\n",
      "[I 2025-12-16 02:33:51,086] Trial 9 finished with value: 62.43426230344201 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.12626444576317492, 'max_lr': 5.495100916366309e-05, 'div_factor': 693, 'final_div_factor': 758, 'weight_decay': 5.66073783538371e-06, 'pct_start': 0.34383650155859535}. Best is trial 3 with value: 20.786911822707093.\n",
      "[I 2025-12-16 02:34:26,811] Trial 10 finished with value: 20.9741109484556 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.29743835652855843, 'max_lr': 0.0010853380605271353, 'div_factor': 911, 'final_div_factor': 338, 'weight_decay': 0.009205468874777106, 'pct_start': 0.12817637848936636}. Best is trial 3 with value: 20.786911822707093.\n",
      "[I 2025-12-16 02:35:03,054] Trial 11 finished with value: 19.680707068418315 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.1791748220054451, 'max_lr': 0.00971014354372654, 'div_factor': 597, 'final_div_factor': 375, 'weight_decay': 3.7080369513670005e-05, 'pct_start': 0.16199002819121122}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:35:38,788] Trial 12 finished with value: 20.169661538608704 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.17218796526409164, 'max_lr': 0.006622583850349502, 'div_factor': 638, 'final_div_factor': 393, 'weight_decay': 0.00010659998270836995, 'pct_start': 0.16173522557202233}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:36:15,298] Trial 13 finished with value: 24.724013741072667 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.1746083643742115, 'max_lr': 0.009341730589218236, 'div_factor': 681, 'final_div_factor': 452, 'weight_decay': 6.082535155888238e-05, 'pct_start': 0.13425511057776998}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:36:50,752] Trial 14 finished with value: 21.215260551627395 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.06978626814531741, 'max_lr': 0.003184966461486966, 'div_factor': 816, 'final_div_factor': 352, 'weight_decay': 2.3282516606582343e-05, 'pct_start': 0.17224546107343552}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:37:27,147] Trial 15 finished with value: 708.0552150079823 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.1578645339301815, 'max_lr': 1.260740971774939e-05, 'div_factor': 554, 'final_div_factor': 477, 'weight_decay': 0.00021540816336309038, 'pct_start': 0.10336947400314342}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:38:03,380] Trial 16 finished with value: 20.381975536189138 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.08392503910247688, 'max_lr': 0.003306655545551253, 'div_factor': 717, 'final_div_factor': 273, 'weight_decay': 0.0001752094966722208, 'pct_start': 0.16405023250399323}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:38:39,366] Trial 17 finished with value: 19.80052193618422 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.18630742997958802, 'max_lr': 0.0019927174965027735, 'div_factor': 475, 'final_div_factor': 537, 'weight_decay': 2.931060948164862e-05, 'pct_start': 0.16860170077233635}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:39:15,524] Trial 18 finished with value: 20.816652908911127 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.20133154711627607, 'max_lr': 0.0015017986908647956, 'div_factor': 499, 'final_div_factor': 535, 'weight_decay': 1.787305628799546e-05, 'pct_start': 0.1900400931670816}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:39:52,042] Trial 19 finished with value: 20.41709324397582 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.11120444756778894, 'max_lr': 0.004390055257493105, 'div_factor': 988, 'final_div_factor': 253, 'weight_decay': 4.745049373349887e-06, 'pct_start': 0.2733490123493157}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:40:29,267] Trial 20 finished with value: 21.434324513125773 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.19191414761690445, 'max_lr': 0.00198574343285753, 'div_factor': 446, 'final_div_factor': 587, 'weight_decay': 3.124794112662557e-05, 'pct_start': 0.1300628889773361}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:41:05,605] Trial 21 finished with value: 20.154779046375157 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.1535884547994832, 'max_lr': 0.0061478812017652, 'div_factor': 636, 'final_div_factor': 425, 'weight_decay': 0.00023831058805357945, 'pct_start': 0.16123121471589286}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:41:42,179] Trial 22 finished with value: 20.313717661820068 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.14332126138878523, 'max_lr': 0.005718561936012971, 'div_factor': 825, 'final_div_factor': 462, 'weight_decay': 0.00029252023901791413, 'pct_start': 0.10604473270933615}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:42:19,423] Trial 23 finished with value: 20.04461804690936 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.09219354571228215, 'max_lr': 0.004750183197143942, 'div_factor': 596, 'final_div_factor': 548, 'weight_decay': 4.0967452413004855e-05, 'pct_start': 0.1934928429247328}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:42:56,543] Trial 24 finished with value: 21.906185917345642 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.024048611444286938, 'max_lr': 0.0012900699627912094, 'div_factor': 568, 'final_div_factor': 546, 'weight_decay': 4.2581726853617934e-05, 'pct_start': 0.1993186683427166}. Best is trial 11 with value: 19.680707068418315.\n",
      "[I 2025-12-16 02:43:34,515] Trial 25 finished with value: 19.361614107043817 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.08592854270805945, 'max_lr': 0.0020836507415057464, 'div_factor': 402, 'final_div_factor': 620, 'weight_decay': 1.2573808170279042e-05, 'pct_start': 0.18129438627387234}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:44:12,088] Trial 26 finished with value: 20.6619652763727 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.03736834259703906, 'max_lr': 0.0004132136699635841, 'div_factor': 368, 'final_div_factor': 622, 'weight_decay': 1.0505269988718325e-05, 'pct_start': 0.13852292722276496}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:44:49,313] Trial 27 finished with value: 25.975394710592827 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.07305625249868881, 'max_lr': 0.0001996013159087197, 'div_factor': 456, 'final_div_factor': 749, 'weight_decay': 3.902334381432982e-06, 'pct_start': 0.17896243658364225}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:45:27,063] Trial 28 finished with value: 24.767319293384755 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.04448052170880759, 'max_lr': 0.0022850581838592303, 'div_factor': 306, 'final_div_factor': 277, 'weight_decay': 1.4390104283398517e-05, 'pct_start': 0.1533037383646106}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:46:04,580] Trial 29 finished with value: 20.379760186129293 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.10620447448369755, 'max_lr': 0.0009239581345330345, 'div_factor': 222, 'final_div_factor': 806, 'weight_decay': 7.643087225202324e-06, 'pct_start': 0.25331680515418287}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:46:42,810] Trial 30 finished with value: 20.373556074219714 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.2150715259032954, 'max_lr': 0.0005935064432836974, 'div_factor': 474, 'final_div_factor': 504, 'weight_decay': 3.106812482933709e-06, 'pct_start': 0.21576275330077277}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:47:20,435] Trial 31 finished with value: 21.73969384168227 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.09886358035537984, 'max_lr': 0.003946540691322911, 'div_factor': 593, 'final_div_factor': 612, 'weight_decay': 4.329150304301119e-05, 'pct_start': 0.1929360741222757}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:47:58,821] Trial 32 finished with value: 20.544172380478805 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.06022232193772867, 'max_lr': 0.0021374777720724963, 'div_factor': 752, 'final_div_factor': 556, 'weight_decay': 0.0001193282949555156, 'pct_start': 0.24423957763926918}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:48:36,548] Trial 33 finished with value: 20.2645821519942 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.12966182521747502, 'max_lr': 0.005248292672982696, 'div_factor': 430, 'final_div_factor': 667, 'weight_decay': 1.7637267938609756e-05, 'pct_start': 0.21111123726845565}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:49:14,879] Trial 34 finished with value: 19.927423400157398 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.16808952151098833, 'max_lr': 0.00752560668746115, 'div_factor': 525, 'final_div_factor': 392, 'weight_decay': 6.512326289392243e-05, 'pct_start': 0.1797861221736741}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:49:53,220] Trial 35 finished with value: 20.01785092033483 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.17913820480434023, 'max_lr': 0.009939840044947728, 'div_factor': 513, 'final_div_factor': 376, 'weight_decay': 7.59988795898394e-05, 'pct_start': 0.14841975886038478}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:50:31,783] Trial 36 finished with value: 20.36767371518798 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.14165182173936194, 'max_lr': 0.0017032459133210756, 'div_factor': 12, 'final_div_factor': 310, 'weight_decay': 2.9036191240505e-05, 'pct_start': 0.11718081499766159}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:51:10,813] Trial 37 finished with value: 21.125554004153877 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.1621696773933905, 'max_lr': 0.00021040691662568776, 'div_factor': 330, 'final_div_factor': 220, 'weight_decay': 0.00036250174293815326, 'pct_start': 0.17847133210341865}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:51:49,649] Trial 38 finished with value: 26.126283750979947 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.18376211744582305, 'max_lr': 0.00010987099430852608, 'div_factor': 520, 'final_div_factor': 434, 'weight_decay': 0.0015723750667875989, 'pct_start': 0.23907588981701766}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:52:29,329] Trial 39 finished with value: 20.4017368250479 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.21269837486845652, 'max_lr': 0.007237114515417418, 'div_factor': 408, 'final_div_factor': 214, 'weight_decay': 6.177243418354073e-05, 'pct_start': 0.2607747779523478}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:53:08,732] Trial 40 finished with value: 20.464826228161044 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.0009650306394553415, 'max_lr': 0.0005342005637814158, 'div_factor': 222, 'final_div_factor': 398, 'weight_decay': 1.3532953711956282e-05, 'pct_start': 0.2996033491563907}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:53:48,687] Trial 41 finished with value: 19.710752482534357 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.17758262333899139, 'max_lr': 0.009665043829850686, 'div_factor': 530, 'final_div_factor': 371, 'weight_decay': 0.0001152658498708713, 'pct_start': 0.14357944539224046}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:54:28,316] Trial 42 finished with value: 19.53280306325266 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.2008470802459279, 'max_lr': 0.008227439316735124, 'div_factor': 631, 'final_div_factor': 323, 'weight_decay': 0.0001383163288590549, 'pct_start': 0.1461412508575165}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:55:08,121] Trial 43 finished with value: 20.05679877253479 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.24637428483745322, 'max_lr': 0.002713353232445945, 'div_factor': 608, 'final_div_factor': 102, 'weight_decay': 0.00045451164801159984, 'pct_start': 0.1472403034403377}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:55:48,249] Trial 44 finished with value: 23.31909806301814 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.20372456336759961, 'max_lr': 0.009674427514408276, 'div_factor': 653, 'final_div_factor': 207, 'weight_decay': 0.00012032079420847449, 'pct_start': 0.11365841593015202}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:56:28,849] Trial 45 finished with value: 30.701675776866196 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.23511797591723677, 'max_lr': 0.00376243895424082, 'div_factor': 734, 'final_div_factor': 321, 'weight_decay': 0.0001500855501763085, 'pct_start': 0.11817378952925611}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:57:09,032] Trial 46 finished with value: 24.47162339450532 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.28058229368037685, 'max_lr': 0.007201146297256335, 'div_factor': 797, 'final_div_factor': 495, 'weight_decay': 7.375468697544228e-06, 'pct_start': 0.14047310016771478}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:57:50,091] Trial 47 finished with value: 25.210398826364788 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.1907899470989278, 'max_lr': 0.0007917094126232314, 'div_factor': 397, 'final_div_factor': 999, 'weight_decay': 0.0011937263459608507, 'pct_start': 0.3999160355062168}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:58:31,025] Trial 48 finished with value: 22.895290693448292 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.2247167436383572, 'max_lr': 0.0029111058851938997, 'div_factor': 480, 'final_div_factor': 173, 'weight_decay': 0.005578318050923392, 'pct_start': 0.16530894891586695}. Best is trial 25 with value: 19.361614107043817.\n",
      "[I 2025-12-16 02:59:12,551] Trial 49 finished with value: 19.61432410778098 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.13278280954652802, 'max_lr': 0.005062310756465027, 'div_factor': 549, 'final_div_factor': 661, 'weight_decay': 0.0005645289481422312, 'pct_start': 0.12430856209337601}. Best is trial 25 with value: 19.361614107043817.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    attributes=attributes,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    epochs=epochs,\n",
    "    path_vision=path_vision,\n",
    "    path_mlp=path_mlp\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating top-5 trials once at 100 epochs (seed=0)...\n",
      "\n",
      "→ Single-pass full run (Trial 25, ValObjective: 19.3616)\n",
      "\n",
      "Best Trial: 25\n",
      "  Best Score: 19.3616\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512,256]\n",
      "    fusion_dropout: 0.08592854270805945\n",
      "    max_lr: 0.0020836507415057464\n",
      "    div_factor: 402\n",
      "    final_div_factor: 620\n",
      "    weight_decay: 1.2573808170279042e-05\n",
      "    pct_start: 0.18129438627387234\n",
      "  Params: total=880,041  trainable=880,041\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  161.19 K\n",
      "fwd MACs:                                                               33.48 MMACs\n",
      "fwd FLOPs:                                                              67.58 MFLOPS\n",
      "fwd+bwd MACs:                                                           100.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          202.74 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  161.19 K = 100% Params, 33.48 MMACs = 100% MACs, 67.58 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "      (0): Conv2d(1.73 K = 1.07% Params, 1.54 MMACs = 4.6% MACs, 3.08 MFLOPS = 4.56% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.08% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    159.34 K = 98.85% Params, 31.94 MMACs = 95.4% MACs, 64.32 MFLOPS = 95.18% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      9.05 K = 5.61% Params, 8 MMACs = 23.89% MACs, 16.08 MFLOPS = 23.8% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 4.29% Params, 6.16 MMACs = 18.39% MACs, 12.32 MFLOPS = 18.23% FLOPs, 64, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        792 = 0.49% Params, 684.29 KMACs = 2.04% MACs, 1.39 MFLOPS = 2.06% FLOPs\n",
      "        (0): Conv2d(768 = 0.48% Params, 684.29 KMACs = 2.04% MACs, 1.37 MFLOPS = 2.03% FLOPs, 64, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      8.21 K = 5.09% Params, 2.22 MMACs = 6.62% MACs, 4.49 MFLOPS = 6.64% FLOPs\n",
      "      (conv1): Conv2d(2.59 K = 1.61% Params, 712.8 KMACs = 2.13% MACs, 1.43 MFLOPS = 2.11% FLOPs, 12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        336 = 0.21% Params, 79.2 KMACs = 0.24% MACs, 171.6 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(288 = 0.18% Params, 79.2 KMACs = 0.24% MACs, 158.4 KFLOPS = 0.23% FLOPs, 12, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      32.54 K = 20.19% Params, 3.19 MMACs = 9.54% MACs, 6.42 MFLOPS = 9.51% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 6.43% Params, 1.03 MMACs = 3.07% MACs, 2.05 MFLOPS = 3.04% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.77% Params, 114.05 KMACs = 0.34% MACs, 237.6 KFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.71% Params, 114.05 KMACs = 0.34% MACs, 228.1 KFLOPS = 0.34% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.75 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  418.82 K\n",
      "fwd MACs:                                                               4.6 MMACs\n",
      "fwd FLOPs:                                                              9.21 MFLOPS\n",
      "fwd+bwd MACs:                                                           13.8 MMACs\n",
      "fwd+bwd FLOPs:                                                          27.62 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  418.82 K = 100% Params, 4.6 MMACs = 100% MACs, 9.21 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(287.23 K = 68.58% Params, 3.15 MMACs = 68.59% MACs, 6.31 MFLOPS = 68.52% FLOPs, in_features=560, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.08592854270805945, inplace=False)\n",
      "  (3): Linear(131.33 K = 31.36% Params, 1.44 MMACs = 31.35% MACs, 2.88 MFLOPS = 31.32% FLOPs, in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.08592854270805945, inplace=False)\n",
      "  (6): Linear(257 = 0.06% Params, 2.82 KMACs = 0.06% MACs, 5.63 KFLOPS = 0.06% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_25/best_model.pth\n",
      "   val_rmse=20.455280\n",
      "   params: total=880,041, trainable=880,041\n",
      "→ Single-pass full run (Trial 42, ValObjective: 19.5328)\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 19.5328\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.2008470802459279\n",
      "    max_lr: 0.008227439316735124\n",
      "    div_factor: 631\n",
      "    final_div_factor: 323\n",
      "    weight_decay: 0.0001383163288590549\n",
      "    pct_start: 0.1461412508575165\n",
      "  Params: total=605,097  trainable=605,097\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  161.19 K\n",
      "fwd MACs:                                                               33.48 MMACs\n",
      "fwd FLOPs:                                                              67.58 MFLOPS\n",
      "fwd+bwd MACs:                                                           100.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          202.74 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  161.19 K = 100% Params, 33.48 MMACs = 100% MACs, 67.58 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "      (0): Conv2d(1.73 K = 1.07% Params, 1.54 MMACs = 4.6% MACs, 3.08 MFLOPS = 4.56% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.08% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    159.34 K = 98.85% Params, 31.94 MMACs = 95.4% MACs, 64.32 MFLOPS = 95.18% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      9.05 K = 5.61% Params, 8 MMACs = 23.89% MACs, 16.08 MFLOPS = 23.8% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 4.29% Params, 6.16 MMACs = 18.39% MACs, 12.32 MFLOPS = 18.23% FLOPs, 64, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        792 = 0.49% Params, 684.29 KMACs = 2.04% MACs, 1.39 MFLOPS = 2.06% FLOPs\n",
      "        (0): Conv2d(768 = 0.48% Params, 684.29 KMACs = 2.04% MACs, 1.37 MFLOPS = 2.03% FLOPs, 64, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      8.21 K = 5.09% Params, 2.22 MMACs = 6.62% MACs, 4.49 MFLOPS = 6.64% FLOPs\n",
      "      (conv1): Conv2d(2.59 K = 1.61% Params, 712.8 KMACs = 2.13% MACs, 1.43 MFLOPS = 2.11% FLOPs, 12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        336 = 0.21% Params, 79.2 KMACs = 0.24% MACs, 171.6 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(288 = 0.18% Params, 79.2 KMACs = 0.24% MACs, 158.4 KFLOPS = 0.23% FLOPs, 12, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      32.54 K = 20.19% Params, 3.19 MMACs = 9.54% MACs, 6.42 MFLOPS = 9.51% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 6.43% Params, 1.03 MMACs = 3.07% MACs, 2.05 MFLOPS = 3.04% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.77% Params, 114.05 KMACs = 0.34% MACs, 237.6 KFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.71% Params, 114.05 KMACs = 0.34% MACs, 228.1 KFLOPS = 0.34% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.75 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  143.87 K\n",
      "fwd MACs:                                                               1.58 MMACs\n",
      "fwd FLOPs:                                                              3.16 MFLOPS\n",
      "fwd+bwd MACs:                                                           4.74 MMACs\n",
      "fwd+bwd FLOPs:                                                          9.49 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  143.87 K = 100% Params, 1.58 MMACs = 100% MACs, 3.16 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(143.62 K = 99.82% Params, 1.58 MMACs = 99.82% MACs, 3.15 MFLOPS = 99.73% FLOPs, in_features=560, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2008470802459279, inplace=False)\n",
      "  (3): Linear(257 = 0.18% Params, 2.82 KMACs = 0.18% MACs, 5.63 KFLOPS = 0.18% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_42/best_model.pth\n",
      "   val_rmse=19.534029\n",
      "   params: total=605,097, trainable=605,097\n",
      "→ Single-pass full run (Trial 49, ValObjective: 19.6143)\n",
      "\n",
      "Best Trial: 49\n",
      "  Best Score: 19.6143\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,128]\n",
      "    fusion_dropout: 0.13278280954652802\n",
      "    max_lr: 0.005062310756465027\n",
      "    div_factor: 549\n",
      "    final_div_factor: 661\n",
      "    weight_decay: 0.0005645289481422312\n",
      "    pct_start: 0.12430856209337601\n",
      "  Params: total=549,673  trainable=549,673\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  161.19 K\n",
      "fwd MACs:                                                               33.48 MMACs\n",
      "fwd FLOPs:                                                              67.58 MFLOPS\n",
      "fwd+bwd MACs:                                                           100.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          202.74 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  161.19 K = 100% Params, 33.48 MMACs = 100% MACs, 67.58 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "      (0): Conv2d(1.73 K = 1.07% Params, 1.54 MMACs = 4.6% MACs, 3.08 MFLOPS = 4.56% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.08% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    159.34 K = 98.85% Params, 31.94 MMACs = 95.4% MACs, 64.32 MFLOPS = 95.18% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      9.05 K = 5.61% Params, 8 MMACs = 23.89% MACs, 16.08 MFLOPS = 23.8% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 4.29% Params, 6.16 MMACs = 18.39% MACs, 12.32 MFLOPS = 18.23% FLOPs, 64, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        792 = 0.49% Params, 684.29 KMACs = 2.04% MACs, 1.39 MFLOPS = 2.06% FLOPs\n",
      "        (0): Conv2d(768 = 0.48% Params, 684.29 KMACs = 2.04% MACs, 1.37 MFLOPS = 2.03% FLOPs, 64, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      8.21 K = 5.09% Params, 2.22 MMACs = 6.62% MACs, 4.49 MFLOPS = 6.64% FLOPs\n",
      "      (conv1): Conv2d(2.59 K = 1.61% Params, 712.8 KMACs = 2.13% MACs, 1.43 MFLOPS = 2.11% FLOPs, 12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        336 = 0.21% Params, 79.2 KMACs = 0.24% MACs, 171.6 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(288 = 0.18% Params, 79.2 KMACs = 0.24% MACs, 158.4 KFLOPS = 0.23% FLOPs, 12, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      32.54 K = 20.19% Params, 3.19 MMACs = 9.54% MACs, 6.42 MFLOPS = 9.51% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 6.43% Params, 1.03 MMACs = 3.07% MACs, 2.05 MFLOPS = 3.04% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.77% Params, 114.05 KMACs = 0.34% MACs, 237.6 KFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.71% Params, 114.05 KMACs = 0.34% MACs, 228.1 KFLOPS = 0.34% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.75 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  88.45 K \n",
      "fwd MACs:                                                               970.11 KMACs\n",
      "fwd FLOPs:                                                              1.94 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.91 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.83 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  88.45 K = 100% Params, 970.11 KMACs = 100% MACs, 1.94 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(71.81 K = 81.19% Params, 788.48 KMACs = 81.28% MACs, 1.58 MFLOPS = 81.16% FLOPs, in_features=560, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.13278280954652802, inplace=False)\n",
      "  (3): Linear(16.51 K = 18.67% Params, 180.22 KMACs = 18.58% MACs, 360.45 KFLOPS = 18.55% FLOPs, in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.13278280954652802, inplace=False)\n",
      "  (6): Linear(129 = 0.15% Params, 1.41 KMACs = 0.15% MACs, 2.82 KFLOPS = 0.14% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_49/best_model.pth\n",
      "   val_rmse=20.242114\n",
      "   params: total=549,673, trainable=549,673\n",
      "→ Single-pass full run (Trial 11, ValObjective: 19.6807)\n",
      "\n",
      "Best Trial: 11\n",
      "  Best Score: 19.6807\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512,256]\n",
      "    fusion_dropout: 0.1791748220054451\n",
      "    max_lr: 0.00971014354372654\n",
      "    div_factor: 597\n",
      "    final_div_factor: 375\n",
      "    weight_decay: 3.7080369513670005e-05\n",
      "    pct_start: 0.16199002819121122\n",
      "  Params: total=880,041  trainable=880,041\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  161.19 K\n",
      "fwd MACs:                                                               33.48 MMACs\n",
      "fwd FLOPs:                                                              67.58 MFLOPS\n",
      "fwd+bwd MACs:                                                           100.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          202.74 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  161.19 K = 100% Params, 33.48 MMACs = 100% MACs, 67.58 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "      (0): Conv2d(1.73 K = 1.07% Params, 1.54 MMACs = 4.6% MACs, 3.08 MFLOPS = 4.56% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.08% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    159.34 K = 98.85% Params, 31.94 MMACs = 95.4% MACs, 64.32 MFLOPS = 95.18% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      9.05 K = 5.61% Params, 8 MMACs = 23.89% MACs, 16.08 MFLOPS = 23.8% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 4.29% Params, 6.16 MMACs = 18.39% MACs, 12.32 MFLOPS = 18.23% FLOPs, 64, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        792 = 0.49% Params, 684.29 KMACs = 2.04% MACs, 1.39 MFLOPS = 2.06% FLOPs\n",
      "        (0): Conv2d(768 = 0.48% Params, 684.29 KMACs = 2.04% MACs, 1.37 MFLOPS = 2.03% FLOPs, 64, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      8.21 K = 5.09% Params, 2.22 MMACs = 6.62% MACs, 4.49 MFLOPS = 6.64% FLOPs\n",
      "      (conv1): Conv2d(2.59 K = 1.61% Params, 712.8 KMACs = 2.13% MACs, 1.43 MFLOPS = 2.11% FLOPs, 12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        336 = 0.21% Params, 79.2 KMACs = 0.24% MACs, 171.6 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(288 = 0.18% Params, 79.2 KMACs = 0.24% MACs, 158.4 KFLOPS = 0.23% FLOPs, 12, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      32.54 K = 20.19% Params, 3.19 MMACs = 9.54% MACs, 6.42 MFLOPS = 9.51% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 6.43% Params, 1.03 MMACs = 3.07% MACs, 2.05 MFLOPS = 3.04% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.77% Params, 114.05 KMACs = 0.34% MACs, 237.6 KFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.71% Params, 114.05 KMACs = 0.34% MACs, 228.1 KFLOPS = 0.34% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.75 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  418.82 K\n",
      "fwd MACs:                                                               4.6 MMACs\n",
      "fwd FLOPs:                                                              9.21 MFLOPS\n",
      "fwd+bwd MACs:                                                           13.8 MMACs\n",
      "fwd+bwd FLOPs:                                                          27.62 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  418.82 K = 100% Params, 4.6 MMACs = 100% MACs, 9.21 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(287.23 K = 68.58% Params, 3.15 MMACs = 68.59% MACs, 6.31 MFLOPS = 68.52% FLOPs, in_features=560, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1791748220054451, inplace=False)\n",
      "  (3): Linear(131.33 K = 31.36% Params, 1.44 MMACs = 31.35% MACs, 2.88 MFLOPS = 31.32% FLOPs, in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1791748220054451, inplace=False)\n",
      "  (6): Linear(257 = 0.06% Params, 2.82 KMACs = 0.06% MACs, 5.63 KFLOPS = 0.06% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_11/best_model.pth\n",
      "   val_rmse=20.803188\n",
      "   params: total=880,041, trainable=880,041\n",
      "→ Single-pass full run (Trial 41, ValObjective: 19.7108)\n",
      "\n",
      "Best Trial: 41\n",
      "  Best Score: 19.7108\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.17758262333899139\n",
      "    max_lr: 0.009665043829850686\n",
      "    div_factor: 530\n",
      "    final_div_factor: 371\n",
      "    weight_decay: 0.0001152658498708713\n",
      "    pct_start: 0.14357944539224046\n",
      "  Params: total=605,097  trainable=605,097\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  161.19 K\n",
      "fwd MACs:                                                               33.48 MMACs\n",
      "fwd FLOPs:                                                              67.58 MFLOPS\n",
      "fwd+bwd MACs:                                                           100.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          202.74 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  161.19 K = 100% Params, 33.48 MMACs = 100% MACs, 67.58 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "      (0): Conv2d(1.73 K = 1.07% Params, 1.54 MMACs = 4.6% MACs, 3.08 MFLOPS = 4.56% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.08% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    159.34 K = 98.85% Params, 31.94 MMACs = 95.4% MACs, 64.32 MFLOPS = 95.18% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      9.05 K = 5.61% Params, 8 MMACs = 23.89% MACs, 16.08 MFLOPS = 23.8% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 4.29% Params, 6.16 MMACs = 18.39% MACs, 12.32 MFLOPS = 18.23% FLOPs, 64, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        792 = 0.49% Params, 684.29 KMACs = 2.04% MACs, 1.39 MFLOPS = 2.06% FLOPs\n",
      "        (0): Conv2d(768 = 0.48% Params, 684.29 KMACs = 2.04% MACs, 1.37 MFLOPS = 2.03% FLOPs, 64, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      8.21 K = 5.09% Params, 2.22 MMACs = 6.62% MACs, 4.49 MFLOPS = 6.64% FLOPs\n",
      "      (conv1): Conv2d(2.59 K = 1.61% Params, 712.8 KMACs = 2.13% MACs, 1.43 MFLOPS = 2.11% FLOPs, 12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        336 = 0.21% Params, 79.2 KMACs = 0.24% MACs, 171.6 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(288 = 0.18% Params, 79.2 KMACs = 0.24% MACs, 158.4 KFLOPS = 0.23% FLOPs, 12, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      32.54 K = 20.19% Params, 3.19 MMACs = 9.54% MACs, 6.42 MFLOPS = 9.51% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 6.43% Params, 1.03 MMACs = 3.07% MACs, 2.05 MFLOPS = 3.04% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.77% Params, 114.05 KMACs = 0.34% MACs, 237.6 KFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.71% Params, 114.05 KMACs = 0.34% MACs, 228.1 KFLOPS = 0.34% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.75 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  143.87 K\n",
      "fwd MACs:                                                               1.58 MMACs\n",
      "fwd FLOPs:                                                              3.16 MFLOPS\n",
      "fwd+bwd MACs:                                                           4.74 MMACs\n",
      "fwd+bwd FLOPs:                                                          9.49 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  143.87 K = 100% Params, 1.58 MMACs = 100% MACs, 3.16 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(143.62 K = 99.82% Params, 1.58 MMACs = 99.82% MACs, 3.15 MFLOPS = 99.73% FLOPs, in_features=560, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.17758262333899139, inplace=False)\n",
      "  (3): Linear(257 = 0.18% Params, 2.82 KMACs = 0.18% MACs, 5.63 KFLOPS = 0.18% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_41/best_model.pth\n",
      "   val_rmse=19.802116\n",
      "   params: total=605,097, trainable=605,097\n",
      "\n",
      "Winner after single-pass: Trial 42 (trial_42) by val_rmse=19.534029\n",
      "\n",
      "Re-running winner with seeds [0, 1, 2, 3, 4] at 100 epochs...\n",
      "\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 19.5328\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.2008470802459279\n",
      "    max_lr: 0.008227439316735124\n",
      "    div_factor: 631\n",
      "    final_div_factor: 323\n",
      "    weight_decay: 0.0001383163288590549\n",
      "    pct_start: 0.1461412508575165\n",
      "  Params: total=605,097  trainable=605,097\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  161.19 K\n",
      "fwd MACs:                                                               33.48 MMACs\n",
      "fwd FLOPs:                                                              67.58 MFLOPS\n",
      "fwd+bwd MACs:                                                           100.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          202.74 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  161.19 K = 100% Params, 33.48 MMACs = 100% MACs, 67.58 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "      (0): Conv2d(1.73 K = 1.07% Params, 1.54 MMACs = 4.6% MACs, 3.08 MFLOPS = 4.56% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.08% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    159.34 K = 98.85% Params, 31.94 MMACs = 95.4% MACs, 64.32 MFLOPS = 95.18% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      9.05 K = 5.61% Params, 8 MMACs = 23.89% MACs, 16.08 MFLOPS = 23.8% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 4.29% Params, 6.16 MMACs = 18.39% MACs, 12.32 MFLOPS = 18.23% FLOPs, 64, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        792 = 0.49% Params, 684.29 KMACs = 2.04% MACs, 1.39 MFLOPS = 2.06% FLOPs\n",
      "        (0): Conv2d(768 = 0.48% Params, 684.29 KMACs = 2.04% MACs, 1.37 MFLOPS = 2.03% FLOPs, 64, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      8.21 K = 5.09% Params, 2.22 MMACs = 6.62% MACs, 4.49 MFLOPS = 6.64% FLOPs\n",
      "      (conv1): Conv2d(2.59 K = 1.61% Params, 712.8 KMACs = 2.13% MACs, 1.43 MFLOPS = 2.11% FLOPs, 12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        336 = 0.21% Params, 79.2 KMACs = 0.24% MACs, 171.6 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(288 = 0.18% Params, 79.2 KMACs = 0.24% MACs, 158.4 KFLOPS = 0.23% FLOPs, 12, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      32.54 K = 20.19% Params, 3.19 MMACs = 9.54% MACs, 6.42 MFLOPS = 9.51% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 6.43% Params, 1.03 MMACs = 3.07% MACs, 2.05 MFLOPS = 3.04% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.77% Params, 114.05 KMACs = 0.34% MACs, 237.6 KFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.71% Params, 114.05 KMACs = 0.34% MACs, 228.1 KFLOPS = 0.34% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.75 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  143.87 K\n",
      "fwd MACs:                                                               1.58 MMACs\n",
      "fwd FLOPs:                                                              3.16 MFLOPS\n",
      "fwd+bwd MACs:                                                           4.74 MMACs\n",
      "fwd+bwd FLOPs:                                                          9.49 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  143.87 K = 100% Params, 1.58 MMACs = 100% MACs, 3.16 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(143.62 K = 99.82% Params, 1.58 MMACs = 99.82% MACs, 3.15 MFLOPS = 99.73% FLOPs, in_features=560, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2008470802459279, inplace=False)\n",
      "  (3): Linear(257 = 0.18% Params, 2.82 KMACs = 0.18% MACs, 5.63 KFLOPS = 0.18% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_42_seed0/best_model.pth\n",
      "   Seed 0: val_rmse=19.534029, test_loss=500.221715, test_rmse=22.423865, val_loss=382.604810\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 19.5328\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.2008470802459279\n",
      "    max_lr: 0.008227439316735124\n",
      "    div_factor: 631\n",
      "    final_div_factor: 323\n",
      "    weight_decay: 0.0001383163288590549\n",
      "    pct_start: 0.1461412508575165\n",
      "  Params: total=605,097  trainable=605,097\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  161.19 K\n",
      "fwd MACs:                                                               33.48 MMACs\n",
      "fwd FLOPs:                                                              67.58 MFLOPS\n",
      "fwd+bwd MACs:                                                           100.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          202.74 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  161.19 K = 100% Params, 33.48 MMACs = 100% MACs, 67.58 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "      (0): Conv2d(1.73 K = 1.07% Params, 1.54 MMACs = 4.6% MACs, 3.08 MFLOPS = 4.56% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.08% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    159.34 K = 98.85% Params, 31.94 MMACs = 95.4% MACs, 64.32 MFLOPS = 95.18% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      9.05 K = 5.61% Params, 8 MMACs = 23.89% MACs, 16.08 MFLOPS = 23.8% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 4.29% Params, 6.16 MMACs = 18.39% MACs, 12.32 MFLOPS = 18.23% FLOPs, 64, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        792 = 0.49% Params, 684.29 KMACs = 2.04% MACs, 1.39 MFLOPS = 2.06% FLOPs\n",
      "        (0): Conv2d(768 = 0.48% Params, 684.29 KMACs = 2.04% MACs, 1.37 MFLOPS = 2.03% FLOPs, 64, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      8.21 K = 5.09% Params, 2.22 MMACs = 6.62% MACs, 4.49 MFLOPS = 6.64% FLOPs\n",
      "      (conv1): Conv2d(2.59 K = 1.61% Params, 712.8 KMACs = 2.13% MACs, 1.43 MFLOPS = 2.11% FLOPs, 12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        336 = 0.21% Params, 79.2 KMACs = 0.24% MACs, 171.6 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(288 = 0.18% Params, 79.2 KMACs = 0.24% MACs, 158.4 KFLOPS = 0.23% FLOPs, 12, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      32.54 K = 20.19% Params, 3.19 MMACs = 9.54% MACs, 6.42 MFLOPS = 9.51% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 6.43% Params, 1.03 MMACs = 3.07% MACs, 2.05 MFLOPS = 3.04% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.77% Params, 114.05 KMACs = 0.34% MACs, 237.6 KFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.71% Params, 114.05 KMACs = 0.34% MACs, 228.1 KFLOPS = 0.34% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.75 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  143.87 K\n",
      "fwd MACs:                                                               1.58 MMACs\n",
      "fwd FLOPs:                                                              3.16 MFLOPS\n",
      "fwd+bwd MACs:                                                           4.74 MMACs\n",
      "fwd+bwd FLOPs:                                                          9.49 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  143.87 K = 100% Params, 1.58 MMACs = 100% MACs, 3.16 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(143.62 K = 99.82% Params, 1.58 MMACs = 99.82% MACs, 3.15 MFLOPS = 99.73% FLOPs, in_features=560, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2008470802459279, inplace=False)\n",
      "  (3): Linear(257 = 0.18% Params, 2.82 KMACs = 0.18% MACs, 5.63 KFLOPS = 0.18% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_42_seed1/best_model.pth\n",
      "   Seed 1: val_rmse=19.809174, test_loss=543.951004, test_rmse=23.366123, val_loss=392.733027\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 19.5328\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.2008470802459279\n",
      "    max_lr: 0.008227439316735124\n",
      "    div_factor: 631\n",
      "    final_div_factor: 323\n",
      "    weight_decay: 0.0001383163288590549\n",
      "    pct_start: 0.1461412508575165\n",
      "  Params: total=605,097  trainable=605,097\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  161.19 K\n",
      "fwd MACs:                                                               33.48 MMACs\n",
      "fwd FLOPs:                                                              67.58 MFLOPS\n",
      "fwd+bwd MACs:                                                           100.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          202.74 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  161.19 K = 100% Params, 33.48 MMACs = 100% MACs, 67.58 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "      (0): Conv2d(1.73 K = 1.07% Params, 1.54 MMACs = 4.6% MACs, 3.08 MFLOPS = 4.56% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.08% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    159.34 K = 98.85% Params, 31.94 MMACs = 95.4% MACs, 64.32 MFLOPS = 95.18% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      9.05 K = 5.61% Params, 8 MMACs = 23.89% MACs, 16.08 MFLOPS = 23.8% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 4.29% Params, 6.16 MMACs = 18.39% MACs, 12.32 MFLOPS = 18.23% FLOPs, 64, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        792 = 0.49% Params, 684.29 KMACs = 2.04% MACs, 1.39 MFLOPS = 2.06% FLOPs\n",
      "        (0): Conv2d(768 = 0.48% Params, 684.29 KMACs = 2.04% MACs, 1.37 MFLOPS = 2.03% FLOPs, 64, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      8.21 K = 5.09% Params, 2.22 MMACs = 6.62% MACs, 4.49 MFLOPS = 6.64% FLOPs\n",
      "      (conv1): Conv2d(2.59 K = 1.61% Params, 712.8 KMACs = 2.13% MACs, 1.43 MFLOPS = 2.11% FLOPs, 12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        336 = 0.21% Params, 79.2 KMACs = 0.24% MACs, 171.6 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(288 = 0.18% Params, 79.2 KMACs = 0.24% MACs, 158.4 KFLOPS = 0.23% FLOPs, 12, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      32.54 K = 20.19% Params, 3.19 MMACs = 9.54% MACs, 6.42 MFLOPS = 9.51% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 6.43% Params, 1.03 MMACs = 3.07% MACs, 2.05 MFLOPS = 3.04% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.77% Params, 114.05 KMACs = 0.34% MACs, 237.6 KFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.71% Params, 114.05 KMACs = 0.34% MACs, 228.1 KFLOPS = 0.34% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.75 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  143.87 K\n",
      "fwd MACs:                                                               1.58 MMACs\n",
      "fwd FLOPs:                                                              3.16 MFLOPS\n",
      "fwd+bwd MACs:                                                           4.74 MMACs\n",
      "fwd+bwd FLOPs:                                                          9.49 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  143.87 K = 100% Params, 1.58 MMACs = 100% MACs, 3.16 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(143.62 K = 99.82% Params, 1.58 MMACs = 99.82% MACs, 3.15 MFLOPS = 99.73% FLOPs, in_features=560, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2008470802459279, inplace=False)\n",
      "  (3): Linear(257 = 0.18% Params, 2.82 KMACs = 0.18% MACs, 5.63 KFLOPS = 0.18% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_42_seed2/best_model.pth\n",
      "   Seed 2: val_rmse=19.649599, test_loss=539.438263, test_rmse=23.328116, val_loss=387.781871\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 19.5328\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.2008470802459279\n",
      "    max_lr: 0.008227439316735124\n",
      "    div_factor: 631\n",
      "    final_div_factor: 323\n",
      "    weight_decay: 0.0001383163288590549\n",
      "    pct_start: 0.1461412508575165\n",
      "  Params: total=605,097  trainable=605,097\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  161.19 K\n",
      "fwd MACs:                                                               33.48 MMACs\n",
      "fwd FLOPs:                                                              67.58 MFLOPS\n",
      "fwd+bwd MACs:                                                           100.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          202.74 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  161.19 K = 100% Params, 33.48 MMACs = 100% MACs, 67.58 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "      (0): Conv2d(1.73 K = 1.07% Params, 1.54 MMACs = 4.6% MACs, 3.08 MFLOPS = 4.56% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.08% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    159.34 K = 98.85% Params, 31.94 MMACs = 95.4% MACs, 64.32 MFLOPS = 95.18% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      9.05 K = 5.61% Params, 8 MMACs = 23.89% MACs, 16.08 MFLOPS = 23.8% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 4.29% Params, 6.16 MMACs = 18.39% MACs, 12.32 MFLOPS = 18.23% FLOPs, 64, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        792 = 0.49% Params, 684.29 KMACs = 2.04% MACs, 1.39 MFLOPS = 2.06% FLOPs\n",
      "        (0): Conv2d(768 = 0.48% Params, 684.29 KMACs = 2.04% MACs, 1.37 MFLOPS = 2.03% FLOPs, 64, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      8.21 K = 5.09% Params, 2.22 MMACs = 6.62% MACs, 4.49 MFLOPS = 6.64% FLOPs\n",
      "      (conv1): Conv2d(2.59 K = 1.61% Params, 712.8 KMACs = 2.13% MACs, 1.43 MFLOPS = 2.11% FLOPs, 12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        336 = 0.21% Params, 79.2 KMACs = 0.24% MACs, 171.6 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(288 = 0.18% Params, 79.2 KMACs = 0.24% MACs, 158.4 KFLOPS = 0.23% FLOPs, 12, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      32.54 K = 20.19% Params, 3.19 MMACs = 9.54% MACs, 6.42 MFLOPS = 9.51% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 6.43% Params, 1.03 MMACs = 3.07% MACs, 2.05 MFLOPS = 3.04% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.77% Params, 114.05 KMACs = 0.34% MACs, 237.6 KFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.71% Params, 114.05 KMACs = 0.34% MACs, 228.1 KFLOPS = 0.34% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.75 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  143.87 K\n",
      "fwd MACs:                                                               1.58 MMACs\n",
      "fwd FLOPs:                                                              3.16 MFLOPS\n",
      "fwd+bwd MACs:                                                           4.74 MMACs\n",
      "fwd+bwd FLOPs:                                                          9.49 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  143.87 K = 100% Params, 1.58 MMACs = 100% MACs, 3.16 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(143.62 K = 99.82% Params, 1.58 MMACs = 99.82% MACs, 3.15 MFLOPS = 99.73% FLOPs, in_features=560, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2008470802459279, inplace=False)\n",
      "  (3): Linear(257 = 0.18% Params, 2.82 KMACs = 0.18% MACs, 5.63 KFLOPS = 0.18% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_42_seed3/best_model.pth\n",
      "   Seed 3: val_rmse=19.853131, test_loss=540.759206, test_rmse=23.292254, val_loss=396.285248\n",
      "\n",
      "Best Trial: 42\n",
      "  Best Score: 19.5328\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.2008470802459279\n",
      "    max_lr: 0.008227439316735124\n",
      "    div_factor: 631\n",
      "    final_div_factor: 323\n",
      "    weight_decay: 0.0001383163288590549\n",
      "    pct_start: 0.1461412508575165\n",
      "  Params: total=605,097  trainable=605,097\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  161.19 K\n",
      "fwd MACs:                                                               33.48 MMACs\n",
      "fwd FLOPs:                                                              67.58 MFLOPS\n",
      "fwd+bwd MACs:                                                           100.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          202.74 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  161.19 K = 100% Params, 33.48 MMACs = 100% MACs, 67.58 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "    (net): Sequential(\n",
      "      1.86 K = 1.15% Params, 1.54 MMACs = 4.6% MACs, 3.25 MFLOPS = 4.81% FLOPs\n",
      "      (0): Conv2d(1.73 K = 1.07% Params, 1.54 MMACs = 4.6% MACs, 3.08 MFLOPS = 4.56% FLOPs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.08% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    159.34 K = 98.85% Params, 31.94 MMACs = 95.4% MACs, 64.32 MFLOPS = 95.18% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      9.05 K = 5.61% Params, 8 MMACs = 23.89% MACs, 16.08 MFLOPS = 23.8% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 4.29% Params, 6.16 MMACs = 18.39% MACs, 12.32 MFLOPS = 18.23% FLOPs, 64, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        792 = 0.49% Params, 684.29 KMACs = 2.04% MACs, 1.39 MFLOPS = 2.06% FLOPs\n",
      "        (0): Conv2d(768 = 0.48% Params, 684.29 KMACs = 2.04% MACs, 1.37 MFLOPS = 2.03% FLOPs, 64, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      2.64 K = 1.64% Params, 2.31 MMACs = 6.9% MACs, 4.68 MFLOPS = 6.93% FLOPs\n",
      "      (conv1): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.3 K = 0.8% Params, 1.15 MMACs = 3.45% MACs, 2.31 MFLOPS = 3.42% FLOPs, 12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24 = 0.01% Params, 0 MACs = 0% MACs, 21.38 KFLOPS = 0.03% FLOPs, 12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      8.21 K = 5.09% Params, 2.22 MMACs = 6.62% MACs, 4.49 MFLOPS = 6.64% FLOPs\n",
      "      (conv1): Conv2d(2.59 K = 1.61% Params, 712.8 KMACs = 2.13% MACs, 1.43 MFLOPS = 2.11% FLOPs, 12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        336 = 0.21% Params, 79.2 KMACs = 0.24% MACs, 171.6 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(288 = 0.18% Params, 79.2 KMACs = 0.24% MACs, 158.4 KFLOPS = 0.23% FLOPs, 12, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      10.46 K = 6.49% Params, 2.85 MMACs = 8.52% MACs, 5.74 MFLOPS = 8.5% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 3.22% Params, 1.43 MMACs = 4.26% MACs, 2.85 MFLOPS = 4.22% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0.03% Params, 0 MACs = 0% MACs, 13.2 KFLOPS = 0.02% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      32.54 K = 20.19% Params, 3.19 MMACs = 9.54% MACs, 6.42 MFLOPS = 9.51% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 6.43% Params, 1.03 MMACs = 3.07% MACs, 2.05 MFLOPS = 3.04% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.77% Params, 114.05 KMACs = 0.34% MACs, 237.6 KFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.71% Params, 114.05 KMACs = 0.34% MACs, 228.1 KFLOPS = 0.34% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      41.66 K = 25.85% Params, 4.11 MMACs = 12.26% MACs, 8.24 MFLOPS = 12.19% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 12.86% Params, 2.05 MMACs = 6.13% MACs, 4.11 MFLOPS = 6.08% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0.06% Params, 0 MACs = 0% MACs, 9.5 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 4.75 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  143.87 K\n",
      "fwd MACs:                                                               1.58 MMACs\n",
      "fwd FLOPs:                                                              3.16 MFLOPS\n",
      "fwd+bwd MACs:                                                           4.74 MMACs\n",
      "fwd+bwd FLOPs:                                                          9.49 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  143.87 K = 100% Params, 1.58 MMACs = 100% MACs, 3.16 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(143.62 K = 99.82% Params, 1.58 MMACs = 99.82% MACs, 3.15 MFLOPS = 99.73% FLOPs, in_features=560, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2008470802459279, inplace=False)\n",
      "  (3): Linear(257 = 0.18% Params, 2.82 KMACs = 0.18% MACs, 5.63 KFLOPS = 0.18% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_42_seed4/best_model.pth\n",
      "   Seed 4: val_rmse=20.452179, test_loss=521.183670, test_rmse=22.953944, val_loss=418.129145\n",
      "\n",
      "Winner aggregated val_rmse: 19.859623 ± 0.354905\n",
      "Model params: total=605,097, trainable=605,097\n",
      "Saved multi-seed summary to: logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_42/winner_multi_seed_summary.txt\n",
      "{'winner_trial_number': 42, 'winner_trial_name': 'trial_42', 'primary_metric': 'val_rmse', 'aggregates': {'train_loss': {'mean': 288.59157793963396, 'std': 8.374593704164884}, 'val_loss': {'mean': 395.50682017008467, 'std': 13.658387379715796}, 'test_loss': {'mean': 529.1107716878255, 'std': 18.439035471784937}, 'min_lr': {'mean': 1.3038731088328248e-05, 'std': 0.0}, 'max_lr': {'mean': 0.008227439316735124, 'std': 0.0}, 'total_time': {'mean': 36.830158138275145, 'std': 0.6384937927596819}, 'average_epoch_time': {'mean': 0.3670030808448791, 'std': 0.0064090409571441595}, 'train_mse': {'mean': 288.66710205078124, 'std': 8.407748607170262}, 'train_mae': {'mean': 13.208725738525391, 'std': 0.23627162789895947}, 'train_rmse': {'mean': 16.988757834374304, 'std': 0.2480153618402243}, 'train_r2': {'mean': 0.9664302587509155, 'std': 0.000977749102051088}, 'val_mse': {'mean': 394.5053771972656, 'std': 14.212482424422047}, 'val_mae': {'mean': 15.631882286071777, 'std': 0.2736040341773418}, 'val_rmse': {'mean': 19.85962263000285, 'std': 0.3549052534555689}, 'val_r2': {'mean': 0.9438617944717407, 'std': 0.002022432261260146}, 'test_mse': {'mean': 532.4838195800781, 'std': 18.241151349943337}, 'test_mae': {'mean': 17.764634704589845, 'std': 0.2812569579716994}, 'test_rmse': {'mean': 23.072860552198176, 'std': 0.39831758512423787}, 'test_r2': {'mean': 0.9380262851715088, 'std': 0.0021230119571095767}, 'total_params': {'mean': 605097.0, 'std': 0.0}, 'trainable_params': {'mean': 605097.0, 'std': 0.0}, 'flops': {'mean': 77330528.0, 'std': 0.0}, 'macs': {'mean': 38349872.0, 'std': 0.0}}, 'total_params': 605097, 'trainable_params': 605097, 'flops': 77330528.0, 'macs': 38349872.0, 'summary_path': 'logs/Regression/Moneyball_cleaned/CNN_hybrid/REFINED/best_model/trial_42/winner_multi_seed_summary.txt'}\n"
     ]
    }
   ],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     attributes=attributes,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "     path_vision=path_vision, path_mlp=path_mlp,\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT: BarGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"BarGraph\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes — Train: (862, 72), Val: (185, 72), Test: (185, 72)\n",
      "Numerical features: 8 — ['Year', 'RA', 'W', 'OBP', 'SLG', 'BA', 'OOBP', 'OSLG']\n",
      "Categorical features: 6 — ['Team', 'League', 'Playoffs', 'RankSeason', 'RankPlayoffs', 'G']\n",
      "Total features: 72\n",
      "Images shape (C,H,W): (3, 72, 72)\n",
      "Attributes: 72\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=False, target_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6, 8, 9, 12, 18, 24, 36, 72]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = [2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vision=f\"./logs/{task_type}/{dataset_name}/{vision_name}/{name}/best_model/trial_56\"\n",
    "path_mlp=f\"./logs/{task_type}/{dataset_name}/mlp/best_model/trial_38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-16 03:09:47,672] A new study created in memory with name: no-name-fdb2c8d4-7fe1-4b22-90b5-b6e84b944aba\n",
      "[I 2025-12-16 03:10:36,200] Trial 0 finished with value: 37.33165501046414 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.2626508011548726, 'max_lr': 0.0007031910841264855, 'div_factor': 840, 'final_div_factor': 717, 'weight_decay': 0.00028016279905767526, 'pct_start': 0.2160919273967954}. Best is trial 0 with value: 37.33165501046414.\n",
      "[I 2025-12-16 03:11:20,662] Trial 1 finished with value: 20.565744068755755 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.1378821235215841, 'max_lr': 0.003851951475482363, 'div_factor': 353, 'final_div_factor': 506, 'weight_decay': 0.005518502763427382, 'pct_start': 0.12404499282863476}. Best is trial 1 with value: 20.565744068755755.\n",
      "[I 2025-12-16 03:12:06,240] Trial 2 finished with value: 69.25297898037492 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.07397171687064497, 'max_lr': 3.0206471113065787e-05, 'div_factor': 120, 'final_div_factor': 304, 'weight_decay': 2.1197213148407707e-05, 'pct_start': 0.270004107566004}. Best is trial 1 with value: 20.565744068755755.\n",
      "[I 2025-12-16 03:12:50,390] Trial 3 finished with value: 25.605635308859505 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.19030583903255968, 'max_lr': 0.0008170215482689946, 'div_factor': 12, 'final_div_factor': 645, 'weight_decay': 0.00048574614856200033, 'pct_start': 0.35160432873668035}. Best is trial 1 with value: 20.565744068755755.\n",
      "[I 2025-12-16 03:13:34,702] Trial 4 finished with value: 21.658693382183458 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.015268136214939764, 'max_lr': 0.0010812836551063931, 'div_factor': 340, 'final_div_factor': 683, 'weight_decay': 0.0005774316024311228, 'pct_start': 0.3873644124911604}. Best is trial 1 with value: 20.565744068755755.\n",
      "[I 2025-12-16 03:14:17,696] Trial 5 finished with value: 24.709130654381845 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.28917245841694006, 'max_lr': 0.0002631671685277677, 'div_factor': 530, 'final_div_factor': 168, 'weight_decay': 2.691637065578572e-06, 'pct_start': 0.3072528924298895}. Best is trial 1 with value: 20.565744068755755.\n",
      "[I 2025-12-16 03:15:00,877] Trial 6 finished with value: 74.08386097545976 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.2974955287583378, 'max_lr': 0.00017893263251211845, 'div_factor': 533, 'final_div_factor': 659, 'weight_decay': 0.006210678847601073, 'pct_start': 0.2353742140683585}. Best is trial 1 with value: 20.565744068755755.\n",
      "[I 2025-12-16 03:15:45,112] Trial 7 finished with value: 29.935377583299122 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.07894274405999939, 'max_lr': 5.322996383936633e-05, 'div_factor': 627, 'final_div_factor': 386, 'weight_decay': 5.3134457342037325e-05, 'pct_start': 0.17826368056454783}. Best is trial 1 with value: 20.565744068755755.\n",
      "[I 2025-12-16 03:16:29,462] Trial 8 finished with value: 20.3474151544317 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.06236841666566525, 'max_lr': 0.008241051400210803, 'div_factor': 545, 'final_div_factor': 603, 'weight_decay': 0.0004482805852937021, 'pct_start': 0.16473359371411456}. Best is trial 8 with value: 20.3474151544317.\n",
      "[I 2025-12-16 03:17:14,432] Trial 9 finished with value: 29.418218497894188 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.11333018694333788, 'max_lr': 8.581985972239385e-05, 'div_factor': 233, 'final_div_factor': 400, 'weight_decay': 0.0012306971252669888, 'pct_start': 0.27563845043963586}. Best is trial 8 with value: 20.3474151544317.\n",
      "[I 2025-12-16 03:17:58,987] Trial 10 finished with value: 20.822922854655577 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.02303234160762837, 'max_lr': 0.00622205141407734, 'div_factor': 920, 'final_div_factor': 976, 'weight_decay': 8.349086868577162e-06, 'pct_start': 0.10984053015602924}. Best is trial 8 with value: 20.3474151544317.\n",
      "[I 2025-12-16 03:18:43,187] Trial 11 finished with value: 20.46792176878745 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.17991765476504673, 'max_lr': 0.008275341255228807, 'div_factor': 348, 'final_div_factor': 501, 'weight_decay': 0.009943947117250761, 'pct_start': 0.10573374743949876}. Best is trial 8 with value: 20.3474151544317.\n",
      "[I 2025-12-16 03:19:27,532] Trial 12 finished with value: 21.53524588170627 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.202261770668603, 'max_lr': 0.00927908083750223, 'div_factor': 691, 'final_div_factor': 930, 'weight_decay': 0.002082557352454418, 'pct_start': 0.169327276109015}. Best is trial 8 with value: 20.3474151544317.\n",
      "[I 2025-12-16 03:20:11,566] Trial 13 finished with value: 24.318460317360376 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.19424253908928976, 'max_lr': 0.0022070128585515137, 'div_factor': 376, 'final_div_factor': 839, 'weight_decay': 0.00010069729520851212, 'pct_start': 0.1566617114774776}. Best is trial 8 with value: 20.3474151544317.\n",
      "[I 2025-12-16 03:20:56,978] Trial 14 finished with value: 673.11528544522 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.07517917370792791, 'max_lr': 1.0345899914539248e-05, 'div_factor': 697, 'final_div_factor': 530, 'weight_decay': 0.006792931126126452, 'pct_start': 0.10702978006343555}. Best is trial 8 with value: 20.3474151544317.\n",
      "[I 2025-12-16 03:21:41,989] Trial 15 finished with value: 21.557583367222904 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.15596464814339542, 'max_lr': 0.002077036602239542, 'div_factor': 427, 'final_div_factor': 119, 'weight_decay': 0.00013728285026666975, 'pct_start': 0.20318616559600833}. Best is trial 8 with value: 20.3474151544317.\n",
      "[I 2025-12-16 03:22:28,323] Trial 16 finished with value: 22.32572070898902 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.2459684219247368, 'max_lr': 0.004055013027038329, 'div_factor': 224, 'final_div_factor': 456, 'weight_decay': 1.1188782546901162e-06, 'pct_start': 0.1410486047134128}. Best is trial 8 with value: 20.3474151544317.\n",
      "[I 2025-12-16 03:23:13,012] Trial 17 finished with value: 21.38049959517909 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.11408325190132429, 'max_lr': 0.009184931268381174, 'div_factor': 241, 'final_div_factor': 273, 'weight_decay': 0.0017273777173287463, 'pct_start': 0.1859326925613277}. Best is trial 8 with value: 20.3474151544317.\n",
      "[I 2025-12-16 03:23:57,907] Trial 18 finished with value: 19.755534881325428 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.04377821931181073, 'max_lr': 0.0018178552104879883, 'div_factor': 800, 'final_div_factor': 788, 'weight_decay': 0.009744240826576232, 'pct_start': 0.1414015012083698}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:24:42,641] Trial 19 finished with value: 20.440511592427335 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.04498481584714731, 'max_lr': 0.0017764430232150169, 'div_factor': 992, 'final_div_factor': 820, 'weight_decay': 4.796066027166788e-05, 'pct_start': 0.23827164096474834}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:25:27,782] Trial 20 finished with value: 27.084605714302334 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.040081390387650774, 'max_lr': 0.000448523107816788, 'div_factor': 813, 'final_div_factor': 765, 'weight_decay': 0.0022774063576821214, 'pct_start': 0.1460412220207879}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:26:15,144] Trial 21 finished with value: 20.806642000052758 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.05075340815463613, 'max_lr': 0.0015422761290046546, 'div_factor': 1000, 'final_div_factor': 847, 'weight_decay': 3.3863633461610394e-05, 'pct_start': 0.24280399897146582}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:27:26,700] Trial 22 finished with value: 21.331083099579633 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.05154552014601722, 'max_lr': 0.00353131561489533, 'div_factor': 794, 'final_div_factor': 823, 'weight_decay': 1.1074171803823984e-05, 'pct_start': 0.2006514483890566}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:28:21,756] Trial 23 finished with value: 21.70687343889459 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.0016834715842282341, 'max_lr': 0.00045353964275153834, 'div_factor': 972, 'final_div_factor': 906, 'weight_decay': 0.00027259380852965285, 'pct_start': 0.30642689136834733}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:29:14,221] Trial 24 finished with value: 21.829252526578657 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.09613550596399628, 'max_lr': 0.002651271423119883, 'div_factor': 912, 'final_div_factor': 579, 'weight_decay': 0.0007734069703031254, 'pct_start': 0.22543353392727733}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:30:08,569] Trial 25 finished with value: 20.15324135733583 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.030975640762085956, 'max_lr': 0.001247986450721206, 'div_factor': 730, 'final_div_factor': 747, 'weight_decay': 0.00020448491226101654, 'pct_start': 0.14327320502167243}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:31:10,963] Trial 26 finished with value: 20.421701653816708 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.01649575741395759, 'max_lr': 0.0010814079485726282, 'div_factor': 635, 'final_div_factor': 598, 'weight_decay': 0.00018571969148894083, 'pct_start': 0.15963245822449987}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:32:06,594] Trial 27 finished with value: 21.593704775640052 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.06746773826791444, 'max_lr': 0.0050402599005989895, 'div_factor': 730, 'final_div_factor': 756, 'weight_decay': 0.0029339021483085247, 'pct_start': 0.13207890072928838}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:33:01,287] Trial 28 finished with value: 22.152216663914245 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.09901989707672906, 'max_lr': 0.0005544014555565793, 'div_factor': 631, 'final_div_factor': 758, 'weight_decay': 0.0008071221462538473, 'pct_start': 0.18576169086252725}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:34:10,077] Trial 29 finished with value: 20.569889677101454 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.03222041521657416, 'max_lr': 0.0011806661946005396, 'div_factor': 471, 'final_div_factor': 722, 'weight_decay': 0.00043289497278346616, 'pct_start': 0.2055919176576346}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:35:02,053] Trial 30 finished with value: 22.718625074880055 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.12440949953237641, 'max_lr': 0.00018047263866046692, 'div_factor': 862, 'final_div_factor': 636, 'weight_decay': 0.00029657832473974256, 'pct_start': 0.12361319867716879}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:35:57,728] Trial 31 finished with value: 21.077641418655872 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.011031562752647037, 'max_lr': 0.0008356447550242102, 'div_factor': 599, 'final_div_factor': 566, 'weight_decay': 0.00015291089969334028, 'pct_start': 0.14749007878672007}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:36:53,937] Trial 32 finished with value: 20.02181427530982 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.026746409606518015, 'max_lr': 0.001222083325852992, 'div_factor': 775, 'final_div_factor': 602, 'weight_decay': 0.00024162029979551416, 'pct_start': 0.17089918471947727}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:37:46,578] Trial 33 finished with value: 19.90238553418087 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.05538076577766719, 'max_lr': 0.0028403296523698105, 'div_factor': 776, 'final_div_factor': 707, 'weight_decay': 7.123878287448641e-05, 'pct_start': 0.12468336728269078}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:38:38,349] Trial 34 finished with value: 20.03554885131519 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.029842892097352046, 'max_lr': 0.0027777945284527625, 'div_factor': 751, 'final_div_factor': 686, 'weight_decay': 7.34167368868041e-05, 'pct_start': 0.1272655867896294}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:39:30,811] Trial 35 finished with value: 20.679212310369653 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.08925920750720868, 'max_lr': 0.003134494311439523, 'div_factor': 798, 'final_div_factor': 681, 'weight_decay': 1.3759822246929127e-05, 'pct_start': 0.1261264273546016}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:40:23,734] Trial 36 finished with value: 22.017021530183115 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.0034668502489911338, 'max_lr': 0.0007102830877181858, 'div_factor': 873, 'final_div_factor': 708, 'weight_decay': 5.998650313705698e-05, 'pct_start': 0.1213746311559326}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:41:15,405] Trial 37 finished with value: 20.58005356495284 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.059010605484576026, 'max_lr': 0.00530455460995292, 'div_factor': 750, 'final_div_factor': 627, 'weight_decay': 2.6431894540408888e-05, 'pct_start': 0.10191148085749666}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:42:08,581] Trial 38 finished with value: 24.157959239739316 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.030731489598875728, 'max_lr': 0.0017181589635301407, 'div_factor': 919, 'final_div_factor': 885, 'weight_decay': 1.876159003656842e-05, 'pct_start': 0.2660208004355269}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:42:59,974] Trial 39 finished with value: 22.84769331560081 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.13849101560312382, 'max_lr': 0.002909632612674195, 'div_factor': 770, 'final_div_factor': 498, 'weight_decay': 6.2858230303884924e-06, 'pct_start': 0.17477270843727918}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:43:52,485] Trial 40 finished with value: 22.527936757952613 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.08216367981903722, 'max_lr': 0.00034664004140826706, 'div_factor': 680, 'final_div_factor': 803, 'weight_decay': 7.961235028279005e-05, 'pct_start': 0.3645448969292095}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:44:44,913] Trial 41 finished with value: 20.800830032219793 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.028453814278818657, 'max_lr': 0.0008125102640585983, 'div_factor': 846, 'final_div_factor': 712, 'weight_decay': 9.282852773358822e-05, 'pct_start': 0.1384514626655025}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:45:37,785] Trial 42 finished with value: 20.47313138937761 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.0429491019699387, 'max_lr': 0.001259322520854827, 'div_factor': 734, 'final_div_factor': 664, 'weight_decay': 0.0002263267155312516, 'pct_start': 0.1500717533419168}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:46:31,204] Trial 43 finished with value: 22.114992108896683 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.06349382876701695, 'max_lr': 0.004224272476727187, 'div_factor': 574, 'final_div_factor': 791, 'weight_decay': 3.7600095033425864e-05, 'pct_start': 0.11845575510550892}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:47:26,645] Trial 44 finished with value: 20.120431322210685 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.016626657731722935, 'max_lr': 0.0021283654653220675, 'div_factor': 689, 'final_div_factor': 722, 'weight_decay': 5.3118506701604246e-06, 'pct_start': 0.13246637180560436}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:48:27,781] Trial 45 finished with value: 20.728167342693823 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.020478031020016434, 'max_lr': 0.002625229867335303, 'div_factor': 673, 'final_div_factor': 670, 'weight_decay': 2.351653715747157e-06, 'pct_start': 0.16347790764986225}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:49:21,063] Trial 46 finished with value: 20.14705004431868 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.011550803510878623, 'max_lr': 0.0015307761409674946, 'div_factor': 834, 'final_div_factor': 548, 'weight_decay': 4.726225516888734e-06, 'pct_start': 0.13349399267053363}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:50:12,663] Trial 47 finished with value: 22.026644437715586 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.055062461745042965, 'max_lr': 0.0023519710051411002, 'div_factor': 768, 'final_div_factor': 455, 'weight_decay': 3.4994840186834022e-06, 'pct_start': 0.11592107943138691}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:51:09,659] Trial 48 finished with value: 19.91117819441742 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.07351092088513697, 'max_lr': 0.006131162411951024, 'div_factor': 883, 'final_div_factor': 868, 'weight_decay': 0.003990153178342957, 'pct_start': 0.10087679332500787}. Best is trial 18 with value: 19.755534881325428.\n",
      "[I 2025-12-16 03:52:02,534] Trial 49 finished with value: 20.750732408949148 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.10575531816436748, 'max_lr': 0.0053039498664329415, 'div_factor': 887, 'final_div_factor': 968, 'weight_decay': 0.009762517104084873, 'pct_start': 0.10596597230171119}. Best is trial 18 with value: 19.755534881325428.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    attributes=attributes,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    epochs=epochs,\n",
    "    path_vision=path_vision,\n",
    "    path_mlp=path_mlp\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating top-5 trials once at 100 epochs (seed=0)...\n",
      "\n",
      "→ Single-pass full run (Trial 18, ValObjective: 19.7555)\n",
      "\n",
      "Best Trial: 18\n",
      "  Best Score: 19.7555\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.04377821931181073\n",
      "    max_lr: 0.0018178552104879883\n",
      "    div_factor: 800\n",
      "    final_div_factor: 788\n",
      "    weight_decay: 0.009744240826576232\n",
      "    pct_start: 0.1414015012083698\n",
      "  Params: total=621,249  trainable=621,249\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  173.25 K\n",
      "fwd MACs:                                                               1.41 GMACs\n",
      "fwd FLOPs:                                                              2.84 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.22 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  173.25 K = 100% Params, 1.41 GMACs = 100% MACs, 2.84 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "      (0): Conv2d(864 = 0.5% Params, 49.27 MMACs = 3.5% MACs, 98.54 MFLOPS = 3.47% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 3.65 MFLOPS = 0.13% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    172.32 K = 99.46% Params, 1.36 GMACs = 96.5% MACs, 2.73 GFLOPS = 96.32% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      7.52 K = 4.34% Params, 423.35 MMACs = 30.09% MACs, 853.99 MFLOPS = 30.11% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 262.77 MMACs = 18.68% MACs, 525.53 MFLOPS = 18.53% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.33% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        544 = 0.31% Params, 29.2 MMACs = 2.08% MACs, 60.22 MFLOPS = 2.12% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 29.2 MMACs = 2.08% MACs, 58.39 MFLOPS = 2.06% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 8.39% Params, 204.37 MMACs = 14.53% MACs, 412.4 MFLOPS = 14.54% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.33% Params, 7.3 MMACs = 0.52% MACs, 15.51 MFLOPS = 0.55% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 10.71% Params, 262.77 MMACs = 18.68% MACs, 528.27 MFLOPS = 18.62% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 33.32% Params, 204.37 MMACs = 14.53% MACs, 410.57 MFLOPS = 14.48% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 10.64% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.26% Params, 7.3 MMACs = 0.52% MACs, 15.05 MFLOPS = 0.53% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.18% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 42.7% Params, 262.77 MMACs = 18.68% MACs, 526.9 MFLOPS = 18.58% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  147.97 K\n",
      "fwd MACs:                                                               1.62 MMACs\n",
      "fwd FLOPs:                                                              3.25 MFLOPS\n",
      "fwd+bwd MACs:                                                           4.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          9.76 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  147.97 K = 100% Params, 1.62 MMACs = 100% MACs, 3.25 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(147.71 K = 99.83% Params, 1.62 MMACs = 99.83% MACs, 3.24 MFLOPS = 99.74% FLOPs, in_features=576, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.04377821931181073, inplace=False)\n",
      "  (3): Linear(257 = 0.17% Params, 2.82 KMACs = 0.17% MACs, 5.63 KFLOPS = 0.17% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_18/best_model.pth\n",
      "   val_rmse=20.492585\n",
      "   params: total=621,249, trainable=621,249\n",
      "→ Single-pass full run (Trial 33, ValObjective: 19.9024)\n",
      "\n",
      "Best Trial: 33\n",
      "  Best Score: 19.9024\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,128]\n",
      "    fusion_dropout: 0.05538076577766719\n",
      "    max_lr: 0.0028403296523698105\n",
      "    div_factor: 776\n",
      "    final_div_factor: 707\n",
      "    weight_decay: 7.123878287448641e-05\n",
      "    pct_start: 0.12468336728269078\n",
      "  Params: total=563,777  trainable=563,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  173.25 K\n",
      "fwd MACs:                                                               1.41 GMACs\n",
      "fwd FLOPs:                                                              2.84 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.22 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  173.25 K = 100% Params, 1.41 GMACs = 100% MACs, 2.84 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "      (0): Conv2d(864 = 0.5% Params, 49.27 MMACs = 3.5% MACs, 98.54 MFLOPS = 3.47% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 3.65 MFLOPS = 0.13% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    172.32 K = 99.46% Params, 1.36 GMACs = 96.5% MACs, 2.73 GFLOPS = 96.32% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      7.52 K = 4.34% Params, 423.35 MMACs = 30.09% MACs, 853.99 MFLOPS = 30.11% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 262.77 MMACs = 18.68% MACs, 525.53 MFLOPS = 18.53% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.33% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        544 = 0.31% Params, 29.2 MMACs = 2.08% MACs, 60.22 MFLOPS = 2.12% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 29.2 MMACs = 2.08% MACs, 58.39 MFLOPS = 2.06% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 8.39% Params, 204.37 MMACs = 14.53% MACs, 412.4 MFLOPS = 14.54% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.33% Params, 7.3 MMACs = 0.52% MACs, 15.51 MFLOPS = 0.55% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 10.71% Params, 262.77 MMACs = 18.68% MACs, 528.27 MFLOPS = 18.62% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 33.32% Params, 204.37 MMACs = 14.53% MACs, 410.57 MFLOPS = 14.48% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 10.64% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.26% Params, 7.3 MMACs = 0.52% MACs, 15.05 MFLOPS = 0.53% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.18% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 42.7% Params, 262.77 MMACs = 18.68% MACs, 526.9 MFLOPS = 18.58% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  90.5 K  \n",
      "fwd MACs:                                                               992.64 KMACs\n",
      "fwd FLOPs:                                                              1.99 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.98 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.96 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  90.5 K = 100% Params, 992.64 KMACs = 100% MACs, 1.99 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 81.61% Params, 811.01 KMACs = 81.7% MACs, 1.62 MFLOPS = 81.59% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.05538076577766719, inplace=False)\n",
      "  (3): Linear(16.51 K = 18.25% Params, 180.22 KMACs = 18.16% MACs, 360.45 KFLOPS = 18.13% FLOPs, in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.05538076577766719, inplace=False)\n",
      "  (6): Linear(129 = 0.14% Params, 1.41 KMACs = 0.14% MACs, 2.82 KFLOPS = 0.14% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_33/best_model.pth\n",
      "   val_rmse=20.752574\n",
      "   params: total=563,777, trainable=563,777\n",
      "→ Single-pass full run (Trial 48, ValObjective: 19.9112)\n",
      "\n",
      "Best Trial: 48\n",
      "  Best Score: 19.9112\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128]\n",
      "    fusion_dropout: 0.07351092088513697\n",
      "    max_lr: 0.006131162411951024\n",
      "    div_factor: 883\n",
      "    final_div_factor: 868\n",
      "    weight_decay: 0.003990153178342957\n",
      "    pct_start: 0.10087679332500787\n",
      "  Params: total=547,265  trainable=547,265\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  173.25 K\n",
      "fwd MACs:                                                               1.41 GMACs\n",
      "fwd FLOPs:                                                              2.84 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.22 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  173.25 K = 100% Params, 1.41 GMACs = 100% MACs, 2.84 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "      (0): Conv2d(864 = 0.5% Params, 49.27 MMACs = 3.5% MACs, 98.54 MFLOPS = 3.47% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 3.65 MFLOPS = 0.13% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    172.32 K = 99.46% Params, 1.36 GMACs = 96.5% MACs, 2.73 GFLOPS = 96.32% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      7.52 K = 4.34% Params, 423.35 MMACs = 30.09% MACs, 853.99 MFLOPS = 30.11% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 262.77 MMACs = 18.68% MACs, 525.53 MFLOPS = 18.53% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.33% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        544 = 0.31% Params, 29.2 MMACs = 2.08% MACs, 60.22 MFLOPS = 2.12% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 29.2 MMACs = 2.08% MACs, 58.39 MFLOPS = 2.06% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 8.39% Params, 204.37 MMACs = 14.53% MACs, 412.4 MFLOPS = 14.54% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.33% Params, 7.3 MMACs = 0.52% MACs, 15.51 MFLOPS = 0.55% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 10.71% Params, 262.77 MMACs = 18.68% MACs, 528.27 MFLOPS = 18.62% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 33.32% Params, 204.37 MMACs = 14.53% MACs, 410.57 MFLOPS = 14.48% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 10.64% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.26% Params, 7.3 MMACs = 0.52% MACs, 15.05 MFLOPS = 0.53% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.18% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 42.7% Params, 262.77 MMACs = 18.68% MACs, 526.9 MFLOPS = 18.58% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  73.98 K \n",
      "fwd MACs:                                                               812.42 KMACs\n",
      "fwd FLOPs:                                                              1.63 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          4.88 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  73.98 K = 100% Params, 812.42 KMACs = 100% MACs, 1.63 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 99.83% Params, 811.01 KMACs = 99.83% MACs, 1.62 MFLOPS = 99.74% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.07351092088513697, inplace=False)\n",
      "  (3): Linear(129 = 0.17% Params, 1.41 KMACs = 0.17% MACs, 2.82 KFLOPS = 0.17% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_48/best_model.pth\n",
      "   val_rmse=20.794685\n",
      "   params: total=547,265, trainable=547,265\n",
      "→ Single-pass full run (Trial 32, ValObjective: 20.0218)\n",
      "\n",
      "Best Trial: 32\n",
      "  Best Score: 20.0218\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,128]\n",
      "    fusion_dropout: 0.026746409606518015\n",
      "    max_lr: 0.001222083325852992\n",
      "    div_factor: 775\n",
      "    final_div_factor: 602\n",
      "    weight_decay: 0.00024162029979551416\n",
      "    pct_start: 0.17089918471947727\n",
      "  Params: total=563,777  trainable=563,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  173.25 K\n",
      "fwd MACs:                                                               1.41 GMACs\n",
      "fwd FLOPs:                                                              2.84 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.22 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  173.25 K = 100% Params, 1.41 GMACs = 100% MACs, 2.84 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "      (0): Conv2d(864 = 0.5% Params, 49.27 MMACs = 3.5% MACs, 98.54 MFLOPS = 3.47% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 3.65 MFLOPS = 0.13% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    172.32 K = 99.46% Params, 1.36 GMACs = 96.5% MACs, 2.73 GFLOPS = 96.32% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      7.52 K = 4.34% Params, 423.35 MMACs = 30.09% MACs, 853.99 MFLOPS = 30.11% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 262.77 MMACs = 18.68% MACs, 525.53 MFLOPS = 18.53% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.33% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        544 = 0.31% Params, 29.2 MMACs = 2.08% MACs, 60.22 MFLOPS = 2.12% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 29.2 MMACs = 2.08% MACs, 58.39 MFLOPS = 2.06% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 8.39% Params, 204.37 MMACs = 14.53% MACs, 412.4 MFLOPS = 14.54% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.33% Params, 7.3 MMACs = 0.52% MACs, 15.51 MFLOPS = 0.55% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 10.71% Params, 262.77 MMACs = 18.68% MACs, 528.27 MFLOPS = 18.62% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 33.32% Params, 204.37 MMACs = 14.53% MACs, 410.57 MFLOPS = 14.48% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 10.64% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.26% Params, 7.3 MMACs = 0.52% MACs, 15.05 MFLOPS = 0.53% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.18% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 42.7% Params, 262.77 MMACs = 18.68% MACs, 526.9 MFLOPS = 18.58% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  90.5 K  \n",
      "fwd MACs:                                                               992.64 KMACs\n",
      "fwd FLOPs:                                                              1.99 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.98 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.96 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  90.5 K = 100% Params, 992.64 KMACs = 100% MACs, 1.99 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 81.61% Params, 811.01 KMACs = 81.7% MACs, 1.62 MFLOPS = 81.59% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.026746409606518015, inplace=False)\n",
      "  (3): Linear(16.51 K = 18.25% Params, 180.22 KMACs = 18.16% MACs, 360.45 KFLOPS = 18.13% FLOPs, in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.026746409606518015, inplace=False)\n",
      "  (6): Linear(129 = 0.14% Params, 1.41 KMACs = 0.14% MACs, 2.82 KFLOPS = 0.14% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_32/best_model.pth\n",
      "   val_rmse=20.765601\n",
      "   params: total=563,777, trainable=563,777\n",
      "→ Single-pass full run (Trial 34, ValObjective: 20.0355)\n",
      "\n",
      "Best Trial: 34\n",
      "  Best Score: 20.0355\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,128]\n",
      "    fusion_dropout: 0.029842892097352046\n",
      "    max_lr: 0.0027777945284527625\n",
      "    div_factor: 751\n",
      "    final_div_factor: 686\n",
      "    weight_decay: 7.34167368868041e-05\n",
      "    pct_start: 0.1272655867896294\n",
      "  Params: total=563,777  trainable=563,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  173.25 K\n",
      "fwd MACs:                                                               1.41 GMACs\n",
      "fwd FLOPs:                                                              2.84 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.22 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  173.25 K = 100% Params, 1.41 GMACs = 100% MACs, 2.84 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "      (0): Conv2d(864 = 0.5% Params, 49.27 MMACs = 3.5% MACs, 98.54 MFLOPS = 3.47% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 3.65 MFLOPS = 0.13% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    172.32 K = 99.46% Params, 1.36 GMACs = 96.5% MACs, 2.73 GFLOPS = 96.32% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      7.52 K = 4.34% Params, 423.35 MMACs = 30.09% MACs, 853.99 MFLOPS = 30.11% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 262.77 MMACs = 18.68% MACs, 525.53 MFLOPS = 18.53% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.33% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        544 = 0.31% Params, 29.2 MMACs = 2.08% MACs, 60.22 MFLOPS = 2.12% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 29.2 MMACs = 2.08% MACs, 58.39 MFLOPS = 2.06% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 8.39% Params, 204.37 MMACs = 14.53% MACs, 412.4 MFLOPS = 14.54% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.33% Params, 7.3 MMACs = 0.52% MACs, 15.51 MFLOPS = 0.55% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 10.71% Params, 262.77 MMACs = 18.68% MACs, 528.27 MFLOPS = 18.62% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 33.32% Params, 204.37 MMACs = 14.53% MACs, 410.57 MFLOPS = 14.48% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 10.64% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.26% Params, 7.3 MMACs = 0.52% MACs, 15.05 MFLOPS = 0.53% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.18% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 42.7% Params, 262.77 MMACs = 18.68% MACs, 526.9 MFLOPS = 18.58% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  90.5 K  \n",
      "fwd MACs:                                                               992.64 KMACs\n",
      "fwd FLOPs:                                                              1.99 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.98 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.96 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  90.5 K = 100% Params, 992.64 KMACs = 100% MACs, 1.99 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 81.61% Params, 811.01 KMACs = 81.7% MACs, 1.62 MFLOPS = 81.59% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (3): Linear(16.51 K = 18.25% Params, 180.22 KMACs = 18.16% MACs, 360.45 KFLOPS = 18.13% FLOPs, in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (6): Linear(129 = 0.14% Params, 1.41 KMACs = 0.14% MACs, 2.82 KFLOPS = 0.14% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_34/best_model.pth\n",
      "   val_rmse=20.179888\n",
      "   params: total=563,777, trainable=563,777\n",
      "\n",
      "Winner after single-pass: Trial 34 (trial_34) by val_rmse=20.179888\n",
      "\n",
      "Re-running winner with seeds [0, 1, 2, 3, 4] at 100 epochs...\n",
      "\n",
      "\n",
      "Best Trial: 34\n",
      "  Best Score: 20.0355\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,128]\n",
      "    fusion_dropout: 0.029842892097352046\n",
      "    max_lr: 0.0027777945284527625\n",
      "    div_factor: 751\n",
      "    final_div_factor: 686\n",
      "    weight_decay: 7.34167368868041e-05\n",
      "    pct_start: 0.1272655867896294\n",
      "  Params: total=563,777  trainable=563,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  173.25 K\n",
      "fwd MACs:                                                               1.41 GMACs\n",
      "fwd FLOPs:                                                              2.84 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.22 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  173.25 K = 100% Params, 1.41 GMACs = 100% MACs, 2.84 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "      (0): Conv2d(864 = 0.5% Params, 49.27 MMACs = 3.5% MACs, 98.54 MFLOPS = 3.47% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 3.65 MFLOPS = 0.13% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    172.32 K = 99.46% Params, 1.36 GMACs = 96.5% MACs, 2.73 GFLOPS = 96.32% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      7.52 K = 4.34% Params, 423.35 MMACs = 30.09% MACs, 853.99 MFLOPS = 30.11% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 262.77 MMACs = 18.68% MACs, 525.53 MFLOPS = 18.53% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.33% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        544 = 0.31% Params, 29.2 MMACs = 2.08% MACs, 60.22 MFLOPS = 2.12% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 29.2 MMACs = 2.08% MACs, 58.39 MFLOPS = 2.06% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 8.39% Params, 204.37 MMACs = 14.53% MACs, 412.4 MFLOPS = 14.54% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.33% Params, 7.3 MMACs = 0.52% MACs, 15.51 MFLOPS = 0.55% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 10.71% Params, 262.77 MMACs = 18.68% MACs, 528.27 MFLOPS = 18.62% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 33.32% Params, 204.37 MMACs = 14.53% MACs, 410.57 MFLOPS = 14.48% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 10.64% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.26% Params, 7.3 MMACs = 0.52% MACs, 15.05 MFLOPS = 0.53% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.18% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 42.7% Params, 262.77 MMACs = 18.68% MACs, 526.9 MFLOPS = 18.58% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  90.5 K  \n",
      "fwd MACs:                                                               992.64 KMACs\n",
      "fwd FLOPs:                                                              1.99 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.98 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.96 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  90.5 K = 100% Params, 992.64 KMACs = 100% MACs, 1.99 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 81.61% Params, 811.01 KMACs = 81.7% MACs, 1.62 MFLOPS = 81.59% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (3): Linear(16.51 K = 18.25% Params, 180.22 KMACs = 18.16% MACs, 360.45 KFLOPS = 18.13% FLOPs, in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (6): Linear(129 = 0.14% Params, 1.41 KMACs = 0.14% MACs, 2.82 KFLOPS = 0.14% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_34_seed0/best_model.pth\n",
      "   Seed 0: val_rmse=20.179888, test_loss=621.949371, test_rmse=25.001218, val_loss=410.216754\n",
      "\n",
      "Best Trial: 34\n",
      "  Best Score: 20.0355\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,128]\n",
      "    fusion_dropout: 0.029842892097352046\n",
      "    max_lr: 0.0027777945284527625\n",
      "    div_factor: 751\n",
      "    final_div_factor: 686\n",
      "    weight_decay: 7.34167368868041e-05\n",
      "    pct_start: 0.1272655867896294\n",
      "  Params: total=563,777  trainable=563,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  173.25 K\n",
      "fwd MACs:                                                               1.41 GMACs\n",
      "fwd FLOPs:                                                              2.84 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.22 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  173.25 K = 100% Params, 1.41 GMACs = 100% MACs, 2.84 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "      (0): Conv2d(864 = 0.5% Params, 49.27 MMACs = 3.5% MACs, 98.54 MFLOPS = 3.47% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 3.65 MFLOPS = 0.13% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    172.32 K = 99.46% Params, 1.36 GMACs = 96.5% MACs, 2.73 GFLOPS = 96.32% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      7.52 K = 4.34% Params, 423.35 MMACs = 30.09% MACs, 853.99 MFLOPS = 30.11% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 262.77 MMACs = 18.68% MACs, 525.53 MFLOPS = 18.53% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.33% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        544 = 0.31% Params, 29.2 MMACs = 2.08% MACs, 60.22 MFLOPS = 2.12% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 29.2 MMACs = 2.08% MACs, 58.39 MFLOPS = 2.06% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 8.39% Params, 204.37 MMACs = 14.53% MACs, 412.4 MFLOPS = 14.54% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.33% Params, 7.3 MMACs = 0.52% MACs, 15.51 MFLOPS = 0.55% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 10.71% Params, 262.77 MMACs = 18.68% MACs, 528.27 MFLOPS = 18.62% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 33.32% Params, 204.37 MMACs = 14.53% MACs, 410.57 MFLOPS = 14.48% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 10.64% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.26% Params, 7.3 MMACs = 0.52% MACs, 15.05 MFLOPS = 0.53% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.18% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 42.7% Params, 262.77 MMACs = 18.68% MACs, 526.9 MFLOPS = 18.58% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  90.5 K  \n",
      "fwd MACs:                                                               992.64 KMACs\n",
      "fwd FLOPs:                                                              1.99 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.98 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.96 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  90.5 K = 100% Params, 992.64 KMACs = 100% MACs, 1.99 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 81.61% Params, 811.01 KMACs = 81.7% MACs, 1.62 MFLOPS = 81.59% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (3): Linear(16.51 K = 18.25% Params, 180.22 KMACs = 18.16% MACs, 360.45 KFLOPS = 18.13% FLOPs, in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (6): Linear(129 = 0.14% Params, 1.41 KMACs = 0.14% MACs, 2.82 KFLOPS = 0.14% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_34_seed1/best_model.pth\n",
      "   Seed 1: val_rmse=20.513764, test_loss=581.734065, test_rmse=24.175164, val_loss=422.917384\n",
      "\n",
      "Best Trial: 34\n",
      "  Best Score: 20.0355\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,128]\n",
      "    fusion_dropout: 0.029842892097352046\n",
      "    max_lr: 0.0027777945284527625\n",
      "    div_factor: 751\n",
      "    final_div_factor: 686\n",
      "    weight_decay: 7.34167368868041e-05\n",
      "    pct_start: 0.1272655867896294\n",
      "  Params: total=563,777  trainable=563,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  173.25 K\n",
      "fwd MACs:                                                               1.41 GMACs\n",
      "fwd FLOPs:                                                              2.84 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.22 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  173.25 K = 100% Params, 1.41 GMACs = 100% MACs, 2.84 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "      (0): Conv2d(864 = 0.5% Params, 49.27 MMACs = 3.5% MACs, 98.54 MFLOPS = 3.47% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 3.65 MFLOPS = 0.13% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    172.32 K = 99.46% Params, 1.36 GMACs = 96.5% MACs, 2.73 GFLOPS = 96.32% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      7.52 K = 4.34% Params, 423.35 MMACs = 30.09% MACs, 853.99 MFLOPS = 30.11% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 262.77 MMACs = 18.68% MACs, 525.53 MFLOPS = 18.53% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.33% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        544 = 0.31% Params, 29.2 MMACs = 2.08% MACs, 60.22 MFLOPS = 2.12% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 29.2 MMACs = 2.08% MACs, 58.39 MFLOPS = 2.06% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 8.39% Params, 204.37 MMACs = 14.53% MACs, 412.4 MFLOPS = 14.54% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.33% Params, 7.3 MMACs = 0.52% MACs, 15.51 MFLOPS = 0.55% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 10.71% Params, 262.77 MMACs = 18.68% MACs, 528.27 MFLOPS = 18.62% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 33.32% Params, 204.37 MMACs = 14.53% MACs, 410.57 MFLOPS = 14.48% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 10.64% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.26% Params, 7.3 MMACs = 0.52% MACs, 15.05 MFLOPS = 0.53% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.18% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 42.7% Params, 262.77 MMACs = 18.68% MACs, 526.9 MFLOPS = 18.58% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  90.5 K  \n",
      "fwd MACs:                                                               992.64 KMACs\n",
      "fwd FLOPs:                                                              1.99 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.98 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.96 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  90.5 K = 100% Params, 992.64 KMACs = 100% MACs, 1.99 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 81.61% Params, 811.01 KMACs = 81.7% MACs, 1.62 MFLOPS = 81.59% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (3): Linear(16.51 K = 18.25% Params, 180.22 KMACs = 18.16% MACs, 360.45 KFLOPS = 18.13% FLOPs, in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (6): Linear(129 = 0.14% Params, 1.41 KMACs = 0.14% MACs, 2.82 KFLOPS = 0.14% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_34_seed2/best_model.pth\n",
      "   Seed 2: val_rmse=20.323655, test_loss=572.742432, test_rmse=23.969600, val_loss=415.038620\n",
      "\n",
      "Best Trial: 34\n",
      "  Best Score: 20.0355\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,128]\n",
      "    fusion_dropout: 0.029842892097352046\n",
      "    max_lr: 0.0027777945284527625\n",
      "    div_factor: 751\n",
      "    final_div_factor: 686\n",
      "    weight_decay: 7.34167368868041e-05\n",
      "    pct_start: 0.1272655867896294\n",
      "  Params: total=563,777  trainable=563,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  173.25 K\n",
      "fwd MACs:                                                               1.41 GMACs\n",
      "fwd FLOPs:                                                              2.84 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.22 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  173.25 K = 100% Params, 1.41 GMACs = 100% MACs, 2.84 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "      (0): Conv2d(864 = 0.5% Params, 49.27 MMACs = 3.5% MACs, 98.54 MFLOPS = 3.47% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 3.65 MFLOPS = 0.13% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    172.32 K = 99.46% Params, 1.36 GMACs = 96.5% MACs, 2.73 GFLOPS = 96.32% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      7.52 K = 4.34% Params, 423.35 MMACs = 30.09% MACs, 853.99 MFLOPS = 30.11% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 262.77 MMACs = 18.68% MACs, 525.53 MFLOPS = 18.53% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.33% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        544 = 0.31% Params, 29.2 MMACs = 2.08% MACs, 60.22 MFLOPS = 2.12% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 29.2 MMACs = 2.08% MACs, 58.39 MFLOPS = 2.06% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 8.39% Params, 204.37 MMACs = 14.53% MACs, 412.4 MFLOPS = 14.54% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.33% Params, 7.3 MMACs = 0.52% MACs, 15.51 MFLOPS = 0.55% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 10.71% Params, 262.77 MMACs = 18.68% MACs, 528.27 MFLOPS = 18.62% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 33.32% Params, 204.37 MMACs = 14.53% MACs, 410.57 MFLOPS = 14.48% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 10.64% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.26% Params, 7.3 MMACs = 0.52% MACs, 15.05 MFLOPS = 0.53% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.18% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 42.7% Params, 262.77 MMACs = 18.68% MACs, 526.9 MFLOPS = 18.58% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  90.5 K  \n",
      "fwd MACs:                                                               992.64 KMACs\n",
      "fwd FLOPs:                                                              1.99 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.98 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.96 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  90.5 K = 100% Params, 992.64 KMACs = 100% MACs, 1.99 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 81.61% Params, 811.01 KMACs = 81.7% MACs, 1.62 MFLOPS = 81.59% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (3): Linear(16.51 K = 18.25% Params, 180.22 KMACs = 18.16% MACs, 360.45 KFLOPS = 18.13% FLOPs, in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (6): Linear(129 = 0.14% Params, 1.41 KMACs = 0.14% MACs, 2.82 KFLOPS = 0.14% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_34_seed3/best_model.pth\n",
      "   Seed 3: val_rmse=20.397221, test_loss=601.782939, test_rmse=24.585281, val_loss=420.918864\n",
      "\n",
      "Best Trial: 34\n",
      "  Best Score: 20.0355\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,128]\n",
      "    fusion_dropout: 0.029842892097352046\n",
      "    max_lr: 0.0027777945284527625\n",
      "    div_factor: 751\n",
      "    final_div_factor: 686\n",
      "    weight_decay: 7.34167368868041e-05\n",
      "    pct_start: 0.1272655867896294\n",
      "  Params: total=563,777  trainable=563,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  173.25 K\n",
      "fwd MACs:                                                               1.41 GMACs\n",
      "fwd FLOPs:                                                              2.84 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.22 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.51 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  173.25 K = 100% Params, 1.41 GMACs = 100% MACs, 2.84 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.54% Params, 49.27 MMACs = 3.5% MACs, 104.01 MFLOPS = 3.67% FLOPs\n",
      "      (0): Conv2d(864 = 0.5% Params, 49.27 MMACs = 3.5% MACs, 98.54 MFLOPS = 3.47% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 3.65 MFLOPS = 0.13% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    172.32 K = 99.46% Params, 1.36 GMACs = 96.5% MACs, 2.73 GFLOPS = 96.32% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      7.52 K = 4.34% Params, 423.35 MMACs = 30.09% MACs, 853.99 MFLOPS = 30.11% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 262.77 MMACs = 18.68% MACs, 525.53 MFLOPS = 18.53% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.33% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        544 = 0.31% Params, 29.2 MMACs = 2.08% MACs, 60.22 MFLOPS = 2.12% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 29.2 MMACs = 2.08% MACs, 58.39 MFLOPS = 2.06% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.06% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 8.39% Params, 204.37 MMACs = 14.53% MACs, 412.4 MFLOPS = 14.54% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.66% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.33% Params, 7.3 MMACs = 0.52% MACs, 15.51 MFLOPS = 0.55% FLOPs\n",
      "        (0): Conv2d(512 = 0.3% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 10.71% Params, 262.77 MMACs = 18.68% MACs, 528.27 MFLOPS = 18.62% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 5.32% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.04% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.03% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 33.32% Params, 204.37 MMACs = 14.53% MACs, 410.57 MFLOPS = 14.48% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 10.64% Params, 65.69 MMACs = 4.67% MACs, 131.38 MFLOPS = 4.63% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.26% Params, 7.3 MMACs = 0.52% MACs, 15.05 MFLOPS = 0.53% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.18% Params, 7.3 MMACs = 0.52% MACs, 14.6 MFLOPS = 0.51% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 42.7% Params, 262.77 MMACs = 18.68% MACs, 526.9 MFLOPS = 18.58% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 21.28% Params, 131.38 MMACs = 9.34% MACs, 262.77 MFLOPS = 9.26% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.02% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  90.5 K  \n",
      "fwd MACs:                                                               992.64 KMACs\n",
      "fwd FLOPs:                                                              1.99 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.98 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.96 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  90.5 K = 100% Params, 992.64 KMACs = 100% MACs, 1.99 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 81.61% Params, 811.01 KMACs = 81.7% MACs, 1.62 MFLOPS = 81.59% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (3): Linear(16.51 K = 18.25% Params, 180.22 KMACs = 18.16% MACs, 360.45 KFLOPS = 18.13% FLOPs, in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.07% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.029842892097352046, inplace=False)\n",
      "  (6): Linear(129 = 0.14% Params, 1.41 KMACs = 0.14% MACs, 2.82 KFLOPS = 0.14% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_34_seed4/best_model.pth\n",
      "   Seed 4: val_rmse=20.706746, test_loss=590.761190, test_rmse=24.356778, val_loss=430.576782\n",
      "\n",
      "Winner aggregated val_rmse: 20.424255 ± 0.198985\n",
      "Model params: total=563,777, trainable=563,777\n",
      "Saved multi-seed summary to: logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_34/winner_multi_seed_summary.txt\n",
      "{'winner_trial_number': 34, 'winner_trial_name': 'trial_34', 'primary_metric': 'val_rmse', 'aggregates': {'train_loss': {'mean': 294.7582733719438, 'std': 29.453512775633428}, 'val_loss': {'mean': 419.93368072509764, 'std': 7.770601187037612}, 'test_loss': {'mean': 593.793999226888, 'std': 19.06801367808315}, 'min_lr': {'mean': 3.6987943121874337e-06, 'std': 4.735058123106524e-22}, 'max_lr': {'mean': 0.0027777945284527625, 'std': 0.0}, 'total_time': {'mean': 33.27095637321472, 'std': 0.48904262207777316}, 'average_epoch_time': {'mean': 0.3317849407196045, 'std': 0.0050078815174588445}, 'train_mse': {'mean': 294.83421630859374, 'std': 29.517438796355755}, 'train_mae': {'mean': 13.365910720825195, 'std': 0.6798162047871513}, 'train_rmse': {'mean': 17.153703479795574, 'std': 0.8548927095277056}, 'train_r2': {'mean': 0.9657130837440491, 'std': 0.003432656351770483}, 'val_mse': {'mean': 417.18186645507814, 'std': 8.137888608463578}, 'val_mae': {'mean': 16.173081398010254, 'std': 0.24072196277827787}, 'val_rmse': {'mean': 20.42425495618628, 'std': 0.19898473489890744}, 'val_r2': {'mean': 0.9406349420547485, 'std': 0.001158043238648404}, 'test_mse': {'mean': 596.3459716796875, 'std': 19.472910275356764}, 'test_mae': {'mean': 19.365872192382813, 'std': 0.40450302239652564}, 'test_rmse': {'mean': 24.41760820804836, 'std': 0.3974623849132655}, 'test_r2': {'mean': 0.9305936098098755, 'std': 0.0022663870622095417}, 'total_params': {'mean': 563777.0, 'std': 0.0}, 'trainable_params': {'mean': 563777.0, 'std': 0.0}, 'flops': {'mean': 2844951296.0, 'std': 0.0}, 'macs': {'mean': 1411177856.0, 'std': 0.0}}, 'total_params': 563777, 'trainable_params': 563777, 'flops': 2844951296.0, 'macs': 1411177856.0, 'summary_path': 'logs/Regression/Moneyball_cleaned/CNN_hybrid/BarGraph/best_model/trial_34/winner_multi_seed_summary.txt'}\n"
     ]
    }
   ],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     attributes=attributes,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "     path_vision=path_vision, path_mlp=path_mlp,\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT: DistanceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"DistanceMatrix\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes — Train: (862, 72), Val: (185, 72), Test: (185, 72)\n",
      "Numerical features: 8 — ['Year', 'RA', 'W', 'OBP', 'SLG', 'BA', 'OOBP', 'OSLG']\n",
      "Categorical features: 6 — ['Team', 'League', 'Playoffs', 'RankSeason', 'RankPlayoffs', 'G']\n",
      "Total features: 72\n",
      "Images shape (C,H,W): (3, 72, 72)\n",
      "Attributes: 72\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=False, target_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6, 8, 9, 12, 18, 24, 36, 72]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vision=f\"./logs/{task_type}/{dataset_name}/{vision_name}/{name}/best_model/trial_60\"\n",
    "path_mlp=f\"./logs/{task_type}/{dataset_name}/mlp/best_model/trial_38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = [2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-16 04:04:13,948] A new study created in memory with name: no-name-b623e1f5-6d97-48ab-a57a-1892e6c22370\n",
      "[I 2025-12-16 04:05:05,877] Trial 0 finished with value: 30.654077793200084 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.2191149287526321, 'max_lr': 9.46519678801181e-05, 'div_factor': 975, 'final_div_factor': 460, 'weight_decay': 0.0008418483970265773, 'pct_start': 0.3799860201700934}. Best is trial 0 with value: 30.654077793200084.\n",
      "[I 2025-12-16 04:05:59,650] Trial 1 finished with value: 24.025840835068212 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.10041174618472498, 'max_lr': 0.0006746871952544025, 'div_factor': 224, 'final_div_factor': 459, 'weight_decay': 1.7062528550646727e-06, 'pct_start': 0.36656110711226475}. Best is trial 1 with value: 24.025840835068212.\n",
      "[I 2025-12-16 04:06:52,370] Trial 2 finished with value: 21.4450300243236 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.2253035339782762, 'max_lr': 0.0005347377105781023, 'div_factor': 183, 'final_div_factor': 963, 'weight_decay': 0.0038523119062611787, 'pct_start': 0.14201587788201955}. Best is trial 2 with value: 21.4450300243236.\n",
      "[I 2025-12-16 04:07:45,124] Trial 3 finished with value: 21.88586992765502 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.07419407081689113, 'max_lr': 0.006431664802764606, 'div_factor': 328, 'final_div_factor': 273, 'weight_decay': 0.001201807208367059, 'pct_start': 0.24112053690479693}. Best is trial 2 with value: 21.4450300243236.\n",
      "[I 2025-12-16 04:08:37,448] Trial 4 finished with value: 431.46996201937395 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.20900206618011638, 'max_lr': 1.034492405673011e-05, 'div_factor': 417, 'final_div_factor': 185, 'weight_decay': 2.144993346227007e-06, 'pct_start': 0.20062187561839145}. Best is trial 2 with value: 21.4450300243236.\n",
      "[I 2025-12-16 04:09:29,835] Trial 5 finished with value: 116.75925920189371 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.11561435485979729, 'max_lr': 1.7470879295513747e-05, 'div_factor': 677, 'final_div_factor': 973, 'weight_decay': 0.00540188864041435, 'pct_start': 0.3238209174267482}. Best is trial 2 with value: 21.4450300243236.\n",
      "[I 2025-12-16 04:10:21,526] Trial 6 finished with value: 711.6713822052984 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.2768421186381019, 'max_lr': 1.0760576288554541e-05, 'div_factor': 35, 'final_div_factor': 335, 'weight_decay': 3.579542203439816e-05, 'pct_start': 0.350293434185168}. Best is trial 2 with value: 21.4450300243236.\n",
      "[I 2025-12-16 04:11:14,754] Trial 7 finished with value: 74.78175929621808 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.28433644341011666, 'max_lr': 2.0788167625216413e-05, 'div_factor': 590, 'final_div_factor': 845, 'weight_decay': 3.834350104742697e-05, 'pct_start': 0.163762779804076}. Best is trial 2 with value: 21.4450300243236.\n",
      "[I 2025-12-16 04:12:07,247] Trial 8 finished with value: 36.230257959292466 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.2688553209347616, 'max_lr': 0.00025712952742408676, 'div_factor': 835, 'final_div_factor': 829, 'weight_decay': 0.001267853999766261, 'pct_start': 0.19168028489834354}. Best is trial 2 with value: 21.4450300243236.\n",
      "[I 2025-12-16 04:12:59,511] Trial 9 finished with value: 20.28693027037166 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.13262414147259577, 'max_lr': 0.002028841322985166, 'div_factor': 486, 'final_div_factor': 810, 'weight_decay': 2.2428314043979347e-06, 'pct_start': 0.3446215181392688}. Best is trial 9 with value: 20.28693027037166.\n",
      "[I 2025-12-16 04:13:51,615] Trial 10 finished with value: 19.720821410862698 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.011677551179096468, 'max_lr': 0.00597098242675848, 'div_factor': 697, 'final_div_factor': 663, 'weight_decay': 8.461616338130991e-06, 'pct_start': 0.29396798702175353}. Best is trial 10 with value: 19.720821410862698.\n",
      "[I 2025-12-16 04:14:43,931] Trial 11 finished with value: 19.72111310869885 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.013631807691164158, 'max_lr': 0.007134672781472697, 'div_factor': 712, 'final_div_factor': 684, 'weight_decay': 6.839091825058127e-06, 'pct_start': 0.29581710687514834}. Best is trial 10 with value: 19.720821410862698.\n",
      "[I 2025-12-16 04:15:36,579] Trial 12 finished with value: 19.078899995776176 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.019636401844129847, 'max_lr': 0.009639740944443876, 'div_factor': 738, 'final_div_factor': 659, 'weight_decay': 1.4538626796014097e-05, 'pct_start': 0.2904933893752771}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:16:29,749] Trial 13 finished with value: 20.344124270510747 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.001651840558773432, 'max_lr': 0.003204637555384198, 'div_factor': 834, 'final_div_factor': 628, 'weight_decay': 1.2967011263340214e-05, 'pct_start': 0.2744262108745425}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:17:24,096] Trial 14 finished with value: 20.50521114453668 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.050596184989049435, 'max_lr': 0.0018037397151100287, 'div_factor': 798, 'final_div_factor': 660, 'weight_decay': 0.00020594351687775172, 'pct_start': 0.2388088803433971}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:18:17,056] Trial 15 finished with value: 19.96274530096429 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.044232964173960804, 'max_lr': 0.007672942440566201, 'div_factor': 963, 'final_div_factor': 524, 'weight_decay': 0.00014292173042478571, 'pct_start': 0.30224710767493435}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:19:10,833] Trial 16 finished with value: 26.709605297284323 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.1569612620201814, 'max_lr': 0.003041919688519024, 'div_factor': 569, 'final_div_factor': 747, 'weight_decay': 9.036167362109981e-06, 'pct_start': 0.2637557148911315}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:20:04,009] Trial 17 finished with value: 20.531098386997286 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.03742697782059823, 'max_lr': 0.001107658814210787, 'div_factor': 702, 'final_div_factor': 565, 'weight_decay': 3.29788295233288e-05, 'pct_start': 0.21740567100794106}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:20:58,609] Trial 18 finished with value: 20.63316599706647 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.16612385268738467, 'max_lr': 0.00022295735204422689, 'div_factor': 593, 'final_div_factor': 397, 'weight_decay': 5.428309254853653e-06, 'pct_start': 0.31040061240422223}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:21:53,014] Trial 19 finished with value: 25.472153961559794 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.08543627544369857, 'max_lr': 8.601273469018895e-05, 'div_factor': 898, 'final_div_factor': 575, 'weight_decay': 1.9213199005112316e-05, 'pct_start': 0.3937202694116345}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:22:48,281] Trial 20 finished with value: 19.938376284867157 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.023407139234860735, 'max_lr': 0.004037871965998367, 'div_factor': 764, 'final_div_factor': 722, 'weight_decay': 7.349800399267729e-05, 'pct_start': 0.27823225670019186}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:23:42,447] Trial 21 finished with value: 19.874213820928865 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.002582223784650001, 'max_lr': 0.009784596148753495, 'div_factor': 693, 'final_div_factor': 691, 'weight_decay': 5.103795418946625e-06, 'pct_start': 0.2973805875071181}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:24:36,626] Trial 22 finished with value: 19.649453254062664 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.020961948668823643, 'max_lr': 0.005564584249640402, 'div_factor': 668, 'final_div_factor': 769, 'weight_decay': 4.633005704545458e-06, 'pct_start': 0.3310202894505647}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:25:30,015] Trial 23 finished with value: 19.961824986386123 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.06695116341023241, 'max_lr': 0.004693636009083888, 'div_factor': 503, 'final_div_factor': 904, 'weight_decay': 3.541452034670583e-06, 'pct_start': 0.10181611974942711}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:26:24,234] Trial 24 finished with value: 19.941019447154574 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.036452812818688035, 'max_lr': 0.001584656446550022, 'div_factor': 622, 'final_div_factor': 768, 'weight_decay': 1.6167258121550212e-05, 'pct_start': 0.3322053814109196}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:27:17,705] Trial 25 finished with value: 20.55945432421238 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.06517269514349662, 'max_lr': 0.0009106491423071453, 'div_factor': 416, 'final_div_factor': 629, 'weight_decay': 1.0922719246222839e-06, 'pct_start': 0.3554138124906883}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:28:11,903] Trial 26 finished with value: 19.541568368150735 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.022726789505825278, 'max_lr': 0.002890191944371373, 'div_factor': 894, 'final_div_factor': 897, 'weight_decay': 0.0004062165049882049, 'pct_start': 0.2555932442125664}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:29:06,180] Trial 27 finished with value: 20.012936032541006 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.09961939273155211, 'max_lr': 0.002694395766691202, 'div_factor': 863, 'final_div_factor': 865, 'weight_decay': 0.00033791276297443173, 'pct_start': 0.2495428414375069}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:29:59,346] Trial 28 finished with value: 20.07157374272794 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.03113253256059583, 'max_lr': 0.004391062703666476, 'div_factor': 946, 'final_div_factor': 910, 'weight_decay': 0.00044764567807241824, 'pct_start': 0.22238021065079458}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:30:53,444] Trial 29 finished with value: 32.99872748493493 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.18146001537465425, 'max_lr': 5.953058235306392e-05, 'div_factor': 772, 'final_div_factor': 1000, 'weight_decay': 6.645101690466313e-05, 'pct_start': 0.3757006339666693}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:31:46,782] Trial 30 finished with value: 19.833851954413934 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.05413463823091701, 'max_lr': 0.009920298868152084, 'div_factor': 994, 'final_div_factor': 778, 'weight_decay': 0.002415338167280144, 'pct_start': 0.2667796998326825}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:32:45,419] Trial 31 finished with value: 20.027589460531924 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.017424752179118052, 'max_lr': 0.0059327018125032, 'div_factor': 904, 'final_div_factor': 471, 'weight_decay': 0.0004880596611677359, 'pct_start': 0.32384115124665364}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:33:41,438] Trial 32 finished with value: 20.560659587091315 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.0004732704404021241, 'max_lr': 0.0021493869336784406, 'div_factor': 748, 'final_div_factor': 902, 'weight_decay': 3.312986830230338e-06, 'pct_start': 0.2808825200780724}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:34:36,934] Trial 33 finished with value: 21.36202377047438 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.08455714019796176, 'max_lr': 0.00046575844459442034, 'div_factor': 664, 'final_div_factor': 616, 'weight_decay': 1.065454174614756e-06, 'pct_start': 0.3113719221238075}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:35:32,766] Trial 34 finished with value: 21.672008071193833 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.028739745155001384, 'max_lr': 0.0013388295717624908, 'div_factor': 629, 'final_div_factor': 501, 'weight_decay': 1.1663889893364342e-05, 'pct_start': 0.2843838155665069}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:36:33,197] Trial 35 finished with value: 20.65575728066852 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.05412483564331668, 'max_lr': 0.005467994985115212, 'div_factor': 541, 'final_div_factor': 710, 'weight_decay': 2.1984722167069588e-05, 'pct_start': 0.26112471604851806}. Best is trial 12 with value: 19.078899995776176.\n",
      "[I 2025-12-16 04:37:28,919] Trial 36 finished with value: 18.90376726983771 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.09978174634647179, 'max_lr': 0.0036171957405806074, 'div_factor': 813, 'final_div_factor': 796, 'weight_decay': 0.009951159409282585, 'pct_start': 0.2411762114666556}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:38:26,869] Trial 37 finished with value: 19.69581607596998 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.1125423187454259, 'max_lr': 0.0006643909865591377, 'div_factor': 895, 'final_div_factor': 795, 'weight_decay': 0.006743930010561362, 'pct_start': 0.23038063722882474}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:39:22,856] Trial 38 finished with value: 19.969664646746466 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.08063806888187577, 'max_lr': 0.003517821655988755, 'div_factor': 815, 'final_div_factor': 949, 'weight_decay': 0.009174154131810417, 'pct_start': 0.1993984852411019}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:40:17,779] Trial 39 finished with value: 20.745366693741822 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.12946132906786267, 'max_lr': 0.0008651604219706788, 'div_factor': 759, 'final_div_factor': 157, 'weight_decay': 0.0018604220342729298, 'pct_start': 0.1752798660274245}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:41:13,952] Trial 40 finished with value: 34.76716639841389 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.22728816195234935, 'max_lr': 0.002502842589812933, 'div_factor': 926, 'final_div_factor': 839, 'weight_decay': 0.0009125527526301827, 'pct_start': 0.24698666381654236}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:42:08,825] Trial 41 finished with value: 20.31081348166807 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.10268709719377427, 'max_lr': 0.00040865903706814046, 'div_factor': 861, 'final_div_factor': 794, 'weight_decay': 0.005854010562735535, 'pct_start': 0.22937407711575142}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:43:04,322] Trial 42 finished with value: 20.031470906055706 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.13269870641191733, 'max_lr': 0.009560064495321837, 'div_factor': 892, 'final_div_factor': 875, 'weight_decay': 0.0036210276146443235, 'pct_start': 0.21337397204511796}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:43:59,569] Trial 43 finished with value: 21.666576835250474 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.10831880241474683, 'max_lr': 0.0006200717209862051, 'div_factor': 798, 'final_div_factor': 736, 'weight_decay': 0.009365479681497108, 'pct_start': 0.23683986126643136}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:44:55,431] Trial 44 finished with value: 20.21459201232143 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.19249926008070084, 'max_lr': 0.00023589706452622477, 'div_factor': 272, 'final_div_factor': 792, 'weight_decay': 0.004636166608524246, 'pct_start': 0.2575766579977946}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:45:51,086] Trial 45 finished with value: 21.487224242885876 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.12234386455632405, 'max_lr': 0.0039470955383275426, 'div_factor': 857, 'final_div_factor': 926, 'weight_decay': 0.002589110395165651, 'pct_start': 0.18552880590003568}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:46:47,036] Trial 46 finished with value: 20.647520405049928 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.06545941231982505, 'max_lr': 0.00014147522057412636, 'div_factor': 736, 'final_div_factor': 828, 'weight_decay': 0.006329616468993429, 'pct_start': 0.15463597251926248}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:47:42,630] Trial 47 finished with value: 19.974563444902948 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.01716512593590174, 'max_lr': 0.00733816213651484, 'div_factor': 109, 'final_div_factor': 237, 'weight_decay': 0.00016156732628333636, 'pct_start': 0.340681880937898}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:48:39,695] Trial 48 finished with value: 20.712274658598055 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.045080829138117576, 'max_lr': 0.0017603594762782577, 'div_factor': 974, 'final_div_factor': 862, 'weight_decay': 4.777324023334811e-05, 'pct_start': 0.3561256569659351}. Best is trial 36 with value: 18.90376726983771.\n",
      "[I 2025-12-16 04:49:36,770] Trial 49 finished with value: 20.159085616916165 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.09356570977744608, 'max_lr': 0.0013738178742878806, 'div_factor': 805, 'final_div_factor': 744, 'weight_decay': 0.0012732325838598693, 'pct_start': 0.2311663497817934}. Best is trial 36 with value: 18.90376726983771.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    attributes=attributes,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    epochs=epochs,\n",
    "    path_vision=path_vision,\n",
    "    path_mlp=path_mlp\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating top-5 trials once at 100 epochs (seed=0)...\n",
      "\n",
      "→ Single-pass full run (Trial 36, ValObjective: 18.9038)\n",
      "\n",
      "Best Trial: 36\n",
      "  Best Score: 18.9038\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512,256]\n",
      "    fusion_dropout: 0.09978174634647179\n",
      "    max_lr: 0.0036171957405806074\n",
      "    div_factor: 813\n",
      "    final_div_factor: 796\n",
      "    weight_decay: 0.009951159409282585\n",
      "    pct_start: 0.2411762114666556\n",
      "  Params: total=814,593  trainable=814,593\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  87.55 K \n",
      "fwd MACs:                                                               204.86 MMACs\n",
      "fwd FLOPs:                                                              413.53 MFLOPS\n",
      "fwd+bwd MACs:                                                           614.58 MMACs\n",
      "fwd+bwd FLOPs:                                                          1.24 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  87.55 K = 100% Params, 204.86 MMACs = 100% MACs, 413.53 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "    (net): Sequential(\n",
      "      7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "      (0): Conv2d(7.06 K = 8.06% Params, 100.59 MMACs = 49.1% MACs, 201.18 MFLOPS = 48.65% FLOPs, 3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.11% Params, 0 MACs = 0% MACs, 1.37 MFLOPS = 0.33% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 684.29 KFLOPS = 0.17% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    80.4 K = 91.83% Params, 104.27 MMACs = 50.9% MACs, 210.28 MFLOPS = 50.85% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      4.46 K = 5.1% Params, 62.95 MMACs = 30.73% MACs, 126.82 MFLOPS = 30.67% FLOPs\n",
      "      (conv1): Conv2d(3.46 K = 3.95% Params, 49.27 MMACs = 24.05% MACs, 98.54 MFLOPS = 23.83% FLOPs, 48, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(576 = 0.66% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        400 = 0.46% Params, 5.47 MMACs = 2.67% MACs, 11.18 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(384 = 0.44% Params, 5.47 MMACs = 2.67% MACs, 10.95 MFLOPS = 2.65% FLOPs, 48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      3.68 K = 4.2% Params, 12.77 MMACs = 6.24% MACs, 26 MFLOPS = 6.29% FLOPs\n",
      "      (conv1): Conv2d(1.15 K = 1.32% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 2.63% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        160 = 0.18% Params, 456.19 KMACs = 0.22% MACs, 1.03 MFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(128 = 0.15% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      14.53 K = 16.59% Params, 12.77 MMACs = 6.24% MACs, 25.77 MFLOPS = 6.23% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 5.26% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 10.53% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.66% Params, 456.19 KMACs = 0.22% MACs, 969.41 KFLOPS = 0.23% FLOPs\n",
      "        (0): Conv2d(512 = 0.58% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 65.94% Params, 15.77 MMACs = 7.7% MACs, 31.68 MFLOPS = 7.66% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 21.05% Params, 5.07 MMACs = 2.47% MACs, 10.14 MFLOPS = 2.45% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 42.11% Params, 10.14 MMACs = 4.95% MACs, 20.28 MFLOPS = 4.9% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 2.49% Params, 563.2 KMACs = 0.27% MACs, 1.16 MFLOPS = 0.28% FLOPs\n",
      "        (0): Conv2d(2.05 K = 2.34% Params, 563.2 KMACs = 0.27% MACs, 1.13 MFLOPS = 0.27% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 17.6 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  427.01 K\n",
      "fwd MACs:                                                               4.69 MMACs\n",
      "fwd FLOPs:                                                              9.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           14.07 MMACs\n",
      "fwd+bwd FLOPs:                                                          28.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  427.01 K = 100% Params, 4.69 MMACs = 100% MACs, 9.39 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(295.42 K = 69.18% Params, 3.24 MMACs = 69.19% MACs, 6.49 MFLOPS = 69.13% FLOPs, in_features=576, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.09978174634647179, inplace=False)\n",
      "  (3): Linear(131.33 K = 30.76% Params, 1.44 MMACs = 30.75% MACs, 2.88 MFLOPS = 30.72% FLOPs, in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.09978174634647179, inplace=False)\n",
      "  (6): Linear(257 = 0.06% Params, 2.82 KMACs = 0.06% MACs, 5.63 KFLOPS = 0.06% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_36/best_model.pth\n",
      "   val_rmse=20.226980\n",
      "   params: total=814,593, trainable=814,593\n",
      "→ Single-pass full run (Trial 12, ValObjective: 19.0789)\n",
      "\n",
      "Best Trial: 12\n",
      "  Best Score: 19.0789\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: []\n",
      "    fusion_dropout: 0.019636401844129847\n",
      "    max_lr: 0.009639740944443876\n",
      "    div_factor: 738\n",
      "    final_div_factor: 659\n",
      "    weight_decay: 1.4538626796014097e-05\n",
      "    pct_start: 0.2904933893752771\n",
      "  Params: total=388,161  trainable=388,161\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  87.55 K \n",
      "fwd MACs:                                                               204.86 MMACs\n",
      "fwd FLOPs:                                                              413.53 MFLOPS\n",
      "fwd+bwd MACs:                                                           614.58 MMACs\n",
      "fwd+bwd FLOPs:                                                          1.24 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  87.55 K = 100% Params, 204.86 MMACs = 100% MACs, 413.53 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "    (net): Sequential(\n",
      "      7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "      (0): Conv2d(7.06 K = 8.06% Params, 100.59 MMACs = 49.1% MACs, 201.18 MFLOPS = 48.65% FLOPs, 3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.11% Params, 0 MACs = 0% MACs, 1.37 MFLOPS = 0.33% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 684.29 KFLOPS = 0.17% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    80.4 K = 91.83% Params, 104.27 MMACs = 50.9% MACs, 210.28 MFLOPS = 50.85% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      4.46 K = 5.1% Params, 62.95 MMACs = 30.73% MACs, 126.82 MFLOPS = 30.67% FLOPs\n",
      "      (conv1): Conv2d(3.46 K = 3.95% Params, 49.27 MMACs = 24.05% MACs, 98.54 MFLOPS = 23.83% FLOPs, 48, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(576 = 0.66% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        400 = 0.46% Params, 5.47 MMACs = 2.67% MACs, 11.18 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(384 = 0.44% Params, 5.47 MMACs = 2.67% MACs, 10.95 MFLOPS = 2.65% FLOPs, 48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      3.68 K = 4.2% Params, 12.77 MMACs = 6.24% MACs, 26 MFLOPS = 6.29% FLOPs\n",
      "      (conv1): Conv2d(1.15 K = 1.32% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 2.63% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        160 = 0.18% Params, 456.19 KMACs = 0.22% MACs, 1.03 MFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(128 = 0.15% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      14.53 K = 16.59% Params, 12.77 MMACs = 6.24% MACs, 25.77 MFLOPS = 6.23% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 5.26% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 10.53% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.66% Params, 456.19 KMACs = 0.22% MACs, 969.41 KFLOPS = 0.23% FLOPs\n",
      "        (0): Conv2d(512 = 0.58% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 65.94% Params, 15.77 MMACs = 7.7% MACs, 31.68 MFLOPS = 7.66% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 21.05% Params, 5.07 MMACs = 2.47% MACs, 10.14 MFLOPS = 2.45% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 42.11% Params, 10.14 MMACs = 4.95% MACs, 20.28 MFLOPS = 4.9% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 2.49% Params, 563.2 KMACs = 0.27% MACs, 1.16 MFLOPS = 0.28% FLOPs\n",
      "        (0): Conv2d(2.05 K = 2.34% Params, 563.2 KMACs = 0.27% MACs, 1.13 MFLOPS = 0.27% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 17.6 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  577     \n",
      "fwd MACs:                                                               6.34 KMACs\n",
      "fwd FLOPs:                                                              12.67 KFLOPS\n",
      "fwd+bwd MACs:                                                           19.01 KMACs\n",
      "fwd+bwd FLOPs:                                                          38.02 KFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  577 = 100% Params, 6.34 KMACs = 100% MACs, 12.67 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(577 = 100% Params, 6.34 KMACs = 100% MACs, 12.67 KFLOPS = 100% FLOPs, in_features=576, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_12/best_model.pth\n",
      "   val_rmse=19.292495\n",
      "   params: total=388,161, trainable=388,161\n",
      "→ Single-pass full run (Trial 26, ValObjective: 19.5416)\n",
      "\n",
      "Best Trial: 26\n",
      "  Best Score: 19.5416\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128]\n",
      "    fusion_dropout: 0.022726789505825278\n",
      "    max_lr: 0.002890191944371373\n",
      "    div_factor: 894\n",
      "    final_div_factor: 897\n",
      "    weight_decay: 0.0004062165049882049\n",
      "    pct_start: 0.2555932442125664\n",
      "  Params: total=461,569  trainable=461,569\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  87.55 K \n",
      "fwd MACs:                                                               204.86 MMACs\n",
      "fwd FLOPs:                                                              413.53 MFLOPS\n",
      "fwd+bwd MACs:                                                           614.58 MMACs\n",
      "fwd+bwd FLOPs:                                                          1.24 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  87.55 K = 100% Params, 204.86 MMACs = 100% MACs, 413.53 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "    (net): Sequential(\n",
      "      7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "      (0): Conv2d(7.06 K = 8.06% Params, 100.59 MMACs = 49.1% MACs, 201.18 MFLOPS = 48.65% FLOPs, 3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.11% Params, 0 MACs = 0% MACs, 1.37 MFLOPS = 0.33% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 684.29 KFLOPS = 0.17% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    80.4 K = 91.83% Params, 104.27 MMACs = 50.9% MACs, 210.28 MFLOPS = 50.85% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      4.46 K = 5.1% Params, 62.95 MMACs = 30.73% MACs, 126.82 MFLOPS = 30.67% FLOPs\n",
      "      (conv1): Conv2d(3.46 K = 3.95% Params, 49.27 MMACs = 24.05% MACs, 98.54 MFLOPS = 23.83% FLOPs, 48, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(576 = 0.66% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        400 = 0.46% Params, 5.47 MMACs = 2.67% MACs, 11.18 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(384 = 0.44% Params, 5.47 MMACs = 2.67% MACs, 10.95 MFLOPS = 2.65% FLOPs, 48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      3.68 K = 4.2% Params, 12.77 MMACs = 6.24% MACs, 26 MFLOPS = 6.29% FLOPs\n",
      "      (conv1): Conv2d(1.15 K = 1.32% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 2.63% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        160 = 0.18% Params, 456.19 KMACs = 0.22% MACs, 1.03 MFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(128 = 0.15% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      14.53 K = 16.59% Params, 12.77 MMACs = 6.24% MACs, 25.77 MFLOPS = 6.23% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 5.26% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 10.53% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.66% Params, 456.19 KMACs = 0.22% MACs, 969.41 KFLOPS = 0.23% FLOPs\n",
      "        (0): Conv2d(512 = 0.58% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 65.94% Params, 15.77 MMACs = 7.7% MACs, 31.68 MFLOPS = 7.66% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 21.05% Params, 5.07 MMACs = 2.47% MACs, 10.14 MFLOPS = 2.45% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 42.11% Params, 10.14 MMACs = 4.95% MACs, 20.28 MFLOPS = 4.9% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 2.49% Params, 563.2 KMACs = 0.27% MACs, 1.16 MFLOPS = 0.28% FLOPs\n",
      "        (0): Conv2d(2.05 K = 2.34% Params, 563.2 KMACs = 0.27% MACs, 1.13 MFLOPS = 0.27% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 17.6 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  73.98 K \n",
      "fwd MACs:                                                               812.42 KMACs\n",
      "fwd FLOPs:                                                              1.63 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          4.88 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  73.98 K = 100% Params, 812.42 KMACs = 100% MACs, 1.63 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 99.83% Params, 811.01 KMACs = 99.83% MACs, 1.62 MFLOPS = 99.74% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.022726789505825278, inplace=False)\n",
      "  (3): Linear(129 = 0.17% Params, 1.41 KMACs = 0.17% MACs, 2.82 KFLOPS = 0.17% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_26/best_model.pth\n",
      "   val_rmse=19.806047\n",
      "   params: total=461,569, trainable=461,569\n",
      "→ Single-pass full run (Trial 22, ValObjective: 19.6495)\n",
      "\n",
      "Best Trial: 22\n",
      "  Best Score: 19.6495\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128]\n",
      "    fusion_dropout: 0.020961948668823643\n",
      "    max_lr: 0.005564584249640402\n",
      "    div_factor: 668\n",
      "    final_div_factor: 769\n",
      "    weight_decay: 4.633005704545458e-06\n",
      "    pct_start: 0.3310202894505647\n",
      "  Params: total=461,569  trainable=461,569\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  87.55 K \n",
      "fwd MACs:                                                               204.86 MMACs\n",
      "fwd FLOPs:                                                              413.53 MFLOPS\n",
      "fwd+bwd MACs:                                                           614.58 MMACs\n",
      "fwd+bwd FLOPs:                                                          1.24 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  87.55 K = 100% Params, 204.86 MMACs = 100% MACs, 413.53 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "    (net): Sequential(\n",
      "      7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "      (0): Conv2d(7.06 K = 8.06% Params, 100.59 MMACs = 49.1% MACs, 201.18 MFLOPS = 48.65% FLOPs, 3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.11% Params, 0 MACs = 0% MACs, 1.37 MFLOPS = 0.33% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 684.29 KFLOPS = 0.17% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    80.4 K = 91.83% Params, 104.27 MMACs = 50.9% MACs, 210.28 MFLOPS = 50.85% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      4.46 K = 5.1% Params, 62.95 MMACs = 30.73% MACs, 126.82 MFLOPS = 30.67% FLOPs\n",
      "      (conv1): Conv2d(3.46 K = 3.95% Params, 49.27 MMACs = 24.05% MACs, 98.54 MFLOPS = 23.83% FLOPs, 48, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(576 = 0.66% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        400 = 0.46% Params, 5.47 MMACs = 2.67% MACs, 11.18 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(384 = 0.44% Params, 5.47 MMACs = 2.67% MACs, 10.95 MFLOPS = 2.65% FLOPs, 48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      3.68 K = 4.2% Params, 12.77 MMACs = 6.24% MACs, 26 MFLOPS = 6.29% FLOPs\n",
      "      (conv1): Conv2d(1.15 K = 1.32% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 2.63% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        160 = 0.18% Params, 456.19 KMACs = 0.22% MACs, 1.03 MFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(128 = 0.15% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      14.53 K = 16.59% Params, 12.77 MMACs = 6.24% MACs, 25.77 MFLOPS = 6.23% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 5.26% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 10.53% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.66% Params, 456.19 KMACs = 0.22% MACs, 969.41 KFLOPS = 0.23% FLOPs\n",
      "        (0): Conv2d(512 = 0.58% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 65.94% Params, 15.77 MMACs = 7.7% MACs, 31.68 MFLOPS = 7.66% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 21.05% Params, 5.07 MMACs = 2.47% MACs, 10.14 MFLOPS = 2.45% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 42.11% Params, 10.14 MMACs = 4.95% MACs, 20.28 MFLOPS = 4.9% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 2.49% Params, 563.2 KMACs = 0.27% MACs, 1.16 MFLOPS = 0.28% FLOPs\n",
      "        (0): Conv2d(2.05 K = 2.34% Params, 563.2 KMACs = 0.27% MACs, 1.13 MFLOPS = 0.27% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 17.6 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  73.98 K \n",
      "fwd MACs:                                                               812.42 KMACs\n",
      "fwd FLOPs:                                                              1.63 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          4.88 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  73.98 K = 100% Params, 812.42 KMACs = 100% MACs, 1.63 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 99.83% Params, 811.01 KMACs = 99.83% MACs, 1.62 MFLOPS = 99.74% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.020961948668823643, inplace=False)\n",
      "  (3): Linear(129 = 0.17% Params, 1.41 KMACs = 0.17% MACs, 2.82 KFLOPS = 0.17% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_22/best_model.pth\n",
      "   val_rmse=19.758587\n",
      "   params: total=461,569, trainable=461,569\n",
      "→ Single-pass full run (Trial 37, ValObjective: 19.6958)\n",
      "\n",
      "Best Trial: 37\n",
      "  Best Score: 19.6958\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512,256]\n",
      "    fusion_dropout: 0.1125423187454259\n",
      "    max_lr: 0.0006643909865591377\n",
      "    div_factor: 895\n",
      "    final_div_factor: 795\n",
      "    weight_decay: 0.006743930010561362\n",
      "    pct_start: 0.23038063722882474\n",
      "  Params: total=814,593  trainable=814,593\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  87.55 K \n",
      "fwd MACs:                                                               204.86 MMACs\n",
      "fwd FLOPs:                                                              413.53 MFLOPS\n",
      "fwd+bwd MACs:                                                           614.58 MMACs\n",
      "fwd+bwd FLOPs:                                                          1.24 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  87.55 K = 100% Params, 204.86 MMACs = 100% MACs, 413.53 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "    (net): Sequential(\n",
      "      7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "      (0): Conv2d(7.06 K = 8.06% Params, 100.59 MMACs = 49.1% MACs, 201.18 MFLOPS = 48.65% FLOPs, 3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.11% Params, 0 MACs = 0% MACs, 1.37 MFLOPS = 0.33% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 684.29 KFLOPS = 0.17% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    80.4 K = 91.83% Params, 104.27 MMACs = 50.9% MACs, 210.28 MFLOPS = 50.85% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      4.46 K = 5.1% Params, 62.95 MMACs = 30.73% MACs, 126.82 MFLOPS = 30.67% FLOPs\n",
      "      (conv1): Conv2d(3.46 K = 3.95% Params, 49.27 MMACs = 24.05% MACs, 98.54 MFLOPS = 23.83% FLOPs, 48, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(576 = 0.66% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        400 = 0.46% Params, 5.47 MMACs = 2.67% MACs, 11.18 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(384 = 0.44% Params, 5.47 MMACs = 2.67% MACs, 10.95 MFLOPS = 2.65% FLOPs, 48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      3.68 K = 4.2% Params, 12.77 MMACs = 6.24% MACs, 26 MFLOPS = 6.29% FLOPs\n",
      "      (conv1): Conv2d(1.15 K = 1.32% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 2.63% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        160 = 0.18% Params, 456.19 KMACs = 0.22% MACs, 1.03 MFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(128 = 0.15% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      14.53 K = 16.59% Params, 12.77 MMACs = 6.24% MACs, 25.77 MFLOPS = 6.23% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 5.26% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 10.53% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.66% Params, 456.19 KMACs = 0.22% MACs, 969.41 KFLOPS = 0.23% FLOPs\n",
      "        (0): Conv2d(512 = 0.58% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 65.94% Params, 15.77 MMACs = 7.7% MACs, 31.68 MFLOPS = 7.66% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 21.05% Params, 5.07 MMACs = 2.47% MACs, 10.14 MFLOPS = 2.45% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 42.11% Params, 10.14 MMACs = 4.95% MACs, 20.28 MFLOPS = 4.9% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 2.49% Params, 563.2 KMACs = 0.27% MACs, 1.16 MFLOPS = 0.28% FLOPs\n",
      "        (0): Conv2d(2.05 K = 2.34% Params, 563.2 KMACs = 0.27% MACs, 1.13 MFLOPS = 0.27% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 17.6 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  427.01 K\n",
      "fwd MACs:                                                               4.69 MMACs\n",
      "fwd FLOPs:                                                              9.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           14.07 MMACs\n",
      "fwd+bwd FLOPs:                                                          28.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  427.01 K = 100% Params, 4.69 MMACs = 100% MACs, 9.39 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(295.42 K = 69.18% Params, 3.24 MMACs = 69.19% MACs, 6.49 MFLOPS = 69.13% FLOPs, in_features=576, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (3): Linear(131.33 K = 30.76% Params, 1.44 MMACs = 30.75% MACs, 2.88 MFLOPS = 30.72% FLOPs, in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (6): Linear(257 = 0.06% Params, 2.82 KMACs = 0.06% MACs, 5.63 KFLOPS = 0.06% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_37/best_model.pth\n",
      "   val_rmse=19.178929\n",
      "   params: total=814,593, trainable=814,593\n",
      "\n",
      "Winner after single-pass: Trial 37 (trial_37) by val_rmse=19.178929\n",
      "\n",
      "Re-running winner with seeds [0, 1, 2, 3, 4] at 100 epochs...\n",
      "\n",
      "\n",
      "Best Trial: 37\n",
      "  Best Score: 19.6958\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512,256]\n",
      "    fusion_dropout: 0.1125423187454259\n",
      "    max_lr: 0.0006643909865591377\n",
      "    div_factor: 895\n",
      "    final_div_factor: 795\n",
      "    weight_decay: 0.006743930010561362\n",
      "    pct_start: 0.23038063722882474\n",
      "  Params: total=814,593  trainable=814,593\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  87.55 K \n",
      "fwd MACs:                                                               204.86 MMACs\n",
      "fwd FLOPs:                                                              413.53 MFLOPS\n",
      "fwd+bwd MACs:                                                           614.58 MMACs\n",
      "fwd+bwd FLOPs:                                                          1.24 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  87.55 K = 100% Params, 204.86 MMACs = 100% MACs, 413.53 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "    (net): Sequential(\n",
      "      7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "      (0): Conv2d(7.06 K = 8.06% Params, 100.59 MMACs = 49.1% MACs, 201.18 MFLOPS = 48.65% FLOPs, 3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.11% Params, 0 MACs = 0% MACs, 1.37 MFLOPS = 0.33% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 684.29 KFLOPS = 0.17% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    80.4 K = 91.83% Params, 104.27 MMACs = 50.9% MACs, 210.28 MFLOPS = 50.85% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      4.46 K = 5.1% Params, 62.95 MMACs = 30.73% MACs, 126.82 MFLOPS = 30.67% FLOPs\n",
      "      (conv1): Conv2d(3.46 K = 3.95% Params, 49.27 MMACs = 24.05% MACs, 98.54 MFLOPS = 23.83% FLOPs, 48, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(576 = 0.66% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        400 = 0.46% Params, 5.47 MMACs = 2.67% MACs, 11.18 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(384 = 0.44% Params, 5.47 MMACs = 2.67% MACs, 10.95 MFLOPS = 2.65% FLOPs, 48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      3.68 K = 4.2% Params, 12.77 MMACs = 6.24% MACs, 26 MFLOPS = 6.29% FLOPs\n",
      "      (conv1): Conv2d(1.15 K = 1.32% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 2.63% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        160 = 0.18% Params, 456.19 KMACs = 0.22% MACs, 1.03 MFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(128 = 0.15% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      14.53 K = 16.59% Params, 12.77 MMACs = 6.24% MACs, 25.77 MFLOPS = 6.23% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 5.26% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 10.53% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.66% Params, 456.19 KMACs = 0.22% MACs, 969.41 KFLOPS = 0.23% FLOPs\n",
      "        (0): Conv2d(512 = 0.58% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 65.94% Params, 15.77 MMACs = 7.7% MACs, 31.68 MFLOPS = 7.66% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 21.05% Params, 5.07 MMACs = 2.47% MACs, 10.14 MFLOPS = 2.45% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 42.11% Params, 10.14 MMACs = 4.95% MACs, 20.28 MFLOPS = 4.9% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 2.49% Params, 563.2 KMACs = 0.27% MACs, 1.16 MFLOPS = 0.28% FLOPs\n",
      "        (0): Conv2d(2.05 K = 2.34% Params, 563.2 KMACs = 0.27% MACs, 1.13 MFLOPS = 0.27% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 17.6 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  427.01 K\n",
      "fwd MACs:                                                               4.69 MMACs\n",
      "fwd FLOPs:                                                              9.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           14.07 MMACs\n",
      "fwd+bwd FLOPs:                                                          28.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  427.01 K = 100% Params, 4.69 MMACs = 100% MACs, 9.39 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(295.42 K = 69.18% Params, 3.24 MMACs = 69.19% MACs, 6.49 MFLOPS = 69.13% FLOPs, in_features=576, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (3): Linear(131.33 K = 30.76% Params, 1.44 MMACs = 30.75% MACs, 2.88 MFLOPS = 30.72% FLOPs, in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (6): Linear(257 = 0.06% Params, 2.82 KMACs = 0.06% MACs, 5.63 KFLOPS = 0.06% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_37_seed0/best_model.pth\n",
      "   Seed 0: val_rmse=19.178929, test_loss=525.123606, test_rmse=22.941115, val_loss=374.791560\n",
      "\n",
      "Best Trial: 37\n",
      "  Best Score: 19.6958\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512,256]\n",
      "    fusion_dropout: 0.1125423187454259\n",
      "    max_lr: 0.0006643909865591377\n",
      "    div_factor: 895\n",
      "    final_div_factor: 795\n",
      "    weight_decay: 0.006743930010561362\n",
      "    pct_start: 0.23038063722882474\n",
      "  Params: total=814,593  trainable=814,593\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  87.55 K \n",
      "fwd MACs:                                                               204.86 MMACs\n",
      "fwd FLOPs:                                                              413.53 MFLOPS\n",
      "fwd+bwd MACs:                                                           614.58 MMACs\n",
      "fwd+bwd FLOPs:                                                          1.24 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  87.55 K = 100% Params, 204.86 MMACs = 100% MACs, 413.53 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "    (net): Sequential(\n",
      "      7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "      (0): Conv2d(7.06 K = 8.06% Params, 100.59 MMACs = 49.1% MACs, 201.18 MFLOPS = 48.65% FLOPs, 3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.11% Params, 0 MACs = 0% MACs, 1.37 MFLOPS = 0.33% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 684.29 KFLOPS = 0.17% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    80.4 K = 91.83% Params, 104.27 MMACs = 50.9% MACs, 210.28 MFLOPS = 50.85% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      4.46 K = 5.1% Params, 62.95 MMACs = 30.73% MACs, 126.82 MFLOPS = 30.67% FLOPs\n",
      "      (conv1): Conv2d(3.46 K = 3.95% Params, 49.27 MMACs = 24.05% MACs, 98.54 MFLOPS = 23.83% FLOPs, 48, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(576 = 0.66% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        400 = 0.46% Params, 5.47 MMACs = 2.67% MACs, 11.18 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(384 = 0.44% Params, 5.47 MMACs = 2.67% MACs, 10.95 MFLOPS = 2.65% FLOPs, 48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      3.68 K = 4.2% Params, 12.77 MMACs = 6.24% MACs, 26 MFLOPS = 6.29% FLOPs\n",
      "      (conv1): Conv2d(1.15 K = 1.32% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 2.63% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        160 = 0.18% Params, 456.19 KMACs = 0.22% MACs, 1.03 MFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(128 = 0.15% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      14.53 K = 16.59% Params, 12.77 MMACs = 6.24% MACs, 25.77 MFLOPS = 6.23% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 5.26% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 10.53% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.66% Params, 456.19 KMACs = 0.22% MACs, 969.41 KFLOPS = 0.23% FLOPs\n",
      "        (0): Conv2d(512 = 0.58% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 65.94% Params, 15.77 MMACs = 7.7% MACs, 31.68 MFLOPS = 7.66% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 21.05% Params, 5.07 MMACs = 2.47% MACs, 10.14 MFLOPS = 2.45% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 42.11% Params, 10.14 MMACs = 4.95% MACs, 20.28 MFLOPS = 4.9% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 2.49% Params, 563.2 KMACs = 0.27% MACs, 1.16 MFLOPS = 0.28% FLOPs\n",
      "        (0): Conv2d(2.05 K = 2.34% Params, 563.2 KMACs = 0.27% MACs, 1.13 MFLOPS = 0.27% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 17.6 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  427.01 K\n",
      "fwd MACs:                                                               4.69 MMACs\n",
      "fwd FLOPs:                                                              9.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           14.07 MMACs\n",
      "fwd+bwd FLOPs:                                                          28.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  427.01 K = 100% Params, 4.69 MMACs = 100% MACs, 9.39 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(295.42 K = 69.18% Params, 3.24 MMACs = 69.19% MACs, 6.49 MFLOPS = 69.13% FLOPs, in_features=576, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (3): Linear(131.33 K = 30.76% Params, 1.44 MMACs = 30.75% MACs, 2.88 MFLOPS = 30.72% FLOPs, in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (6): Linear(257 = 0.06% Params, 2.82 KMACs = 0.06% MACs, 5.63 KFLOPS = 0.06% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_37_seed1/best_model.pth\n",
      "   Seed 1: val_rmse=20.006372, test_loss=544.217367, test_rmse=23.343410, val_loss=405.767820\n",
      "\n",
      "Best Trial: 37\n",
      "  Best Score: 19.6958\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512,256]\n",
      "    fusion_dropout: 0.1125423187454259\n",
      "    max_lr: 0.0006643909865591377\n",
      "    div_factor: 895\n",
      "    final_div_factor: 795\n",
      "    weight_decay: 0.006743930010561362\n",
      "    pct_start: 0.23038063722882474\n",
      "  Params: total=814,593  trainable=814,593\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  87.55 K \n",
      "fwd MACs:                                                               204.86 MMACs\n",
      "fwd FLOPs:                                                              413.53 MFLOPS\n",
      "fwd+bwd MACs:                                                           614.58 MMACs\n",
      "fwd+bwd FLOPs:                                                          1.24 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  87.55 K = 100% Params, 204.86 MMACs = 100% MACs, 413.53 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "    (net): Sequential(\n",
      "      7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "      (0): Conv2d(7.06 K = 8.06% Params, 100.59 MMACs = 49.1% MACs, 201.18 MFLOPS = 48.65% FLOPs, 3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.11% Params, 0 MACs = 0% MACs, 1.37 MFLOPS = 0.33% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 684.29 KFLOPS = 0.17% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    80.4 K = 91.83% Params, 104.27 MMACs = 50.9% MACs, 210.28 MFLOPS = 50.85% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      4.46 K = 5.1% Params, 62.95 MMACs = 30.73% MACs, 126.82 MFLOPS = 30.67% FLOPs\n",
      "      (conv1): Conv2d(3.46 K = 3.95% Params, 49.27 MMACs = 24.05% MACs, 98.54 MFLOPS = 23.83% FLOPs, 48, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(576 = 0.66% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        400 = 0.46% Params, 5.47 MMACs = 2.67% MACs, 11.18 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(384 = 0.44% Params, 5.47 MMACs = 2.67% MACs, 10.95 MFLOPS = 2.65% FLOPs, 48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      3.68 K = 4.2% Params, 12.77 MMACs = 6.24% MACs, 26 MFLOPS = 6.29% FLOPs\n",
      "      (conv1): Conv2d(1.15 K = 1.32% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 2.63% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        160 = 0.18% Params, 456.19 KMACs = 0.22% MACs, 1.03 MFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(128 = 0.15% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      14.53 K = 16.59% Params, 12.77 MMACs = 6.24% MACs, 25.77 MFLOPS = 6.23% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 5.26% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 10.53% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.66% Params, 456.19 KMACs = 0.22% MACs, 969.41 KFLOPS = 0.23% FLOPs\n",
      "        (0): Conv2d(512 = 0.58% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 65.94% Params, 15.77 MMACs = 7.7% MACs, 31.68 MFLOPS = 7.66% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 21.05% Params, 5.07 MMACs = 2.47% MACs, 10.14 MFLOPS = 2.45% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 42.11% Params, 10.14 MMACs = 4.95% MACs, 20.28 MFLOPS = 4.9% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 2.49% Params, 563.2 KMACs = 0.27% MACs, 1.16 MFLOPS = 0.28% FLOPs\n",
      "        (0): Conv2d(2.05 K = 2.34% Params, 563.2 KMACs = 0.27% MACs, 1.13 MFLOPS = 0.27% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 17.6 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  427.01 K\n",
      "fwd MACs:                                                               4.69 MMACs\n",
      "fwd FLOPs:                                                              9.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           14.07 MMACs\n",
      "fwd+bwd FLOPs:                                                          28.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  427.01 K = 100% Params, 4.69 MMACs = 100% MACs, 9.39 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(295.42 K = 69.18% Params, 3.24 MMACs = 69.19% MACs, 6.49 MFLOPS = 69.13% FLOPs, in_features=576, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (3): Linear(131.33 K = 30.76% Params, 1.44 MMACs = 30.75% MACs, 2.88 MFLOPS = 30.72% FLOPs, in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (6): Linear(257 = 0.06% Params, 2.82 KMACs = 0.06% MACs, 5.63 KFLOPS = 0.06% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_37_seed2/best_model.pth\n",
      "   Seed 2: val_rmse=19.940159, test_loss=541.094269, test_rmse=23.276124, val_loss=401.135218\n",
      "\n",
      "Best Trial: 37\n",
      "  Best Score: 19.6958\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512,256]\n",
      "    fusion_dropout: 0.1125423187454259\n",
      "    max_lr: 0.0006643909865591377\n",
      "    div_factor: 895\n",
      "    final_div_factor: 795\n",
      "    weight_decay: 0.006743930010561362\n",
      "    pct_start: 0.23038063722882474\n",
      "  Params: total=814,593  trainable=814,593\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  87.55 K \n",
      "fwd MACs:                                                               204.86 MMACs\n",
      "fwd FLOPs:                                                              413.53 MFLOPS\n",
      "fwd+bwd MACs:                                                           614.58 MMACs\n",
      "fwd+bwd FLOPs:                                                          1.24 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  87.55 K = 100% Params, 204.86 MMACs = 100% MACs, 413.53 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "    (net): Sequential(\n",
      "      7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "      (0): Conv2d(7.06 K = 8.06% Params, 100.59 MMACs = 49.1% MACs, 201.18 MFLOPS = 48.65% FLOPs, 3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.11% Params, 0 MACs = 0% MACs, 1.37 MFLOPS = 0.33% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 684.29 KFLOPS = 0.17% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    80.4 K = 91.83% Params, 104.27 MMACs = 50.9% MACs, 210.28 MFLOPS = 50.85% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      4.46 K = 5.1% Params, 62.95 MMACs = 30.73% MACs, 126.82 MFLOPS = 30.67% FLOPs\n",
      "      (conv1): Conv2d(3.46 K = 3.95% Params, 49.27 MMACs = 24.05% MACs, 98.54 MFLOPS = 23.83% FLOPs, 48, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(576 = 0.66% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        400 = 0.46% Params, 5.47 MMACs = 2.67% MACs, 11.18 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(384 = 0.44% Params, 5.47 MMACs = 2.67% MACs, 10.95 MFLOPS = 2.65% FLOPs, 48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      3.68 K = 4.2% Params, 12.77 MMACs = 6.24% MACs, 26 MFLOPS = 6.29% FLOPs\n",
      "      (conv1): Conv2d(1.15 K = 1.32% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 2.63% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        160 = 0.18% Params, 456.19 KMACs = 0.22% MACs, 1.03 MFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(128 = 0.15% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      14.53 K = 16.59% Params, 12.77 MMACs = 6.24% MACs, 25.77 MFLOPS = 6.23% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 5.26% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 10.53% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.66% Params, 456.19 KMACs = 0.22% MACs, 969.41 KFLOPS = 0.23% FLOPs\n",
      "        (0): Conv2d(512 = 0.58% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 65.94% Params, 15.77 MMACs = 7.7% MACs, 31.68 MFLOPS = 7.66% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 21.05% Params, 5.07 MMACs = 2.47% MACs, 10.14 MFLOPS = 2.45% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 42.11% Params, 10.14 MMACs = 4.95% MACs, 20.28 MFLOPS = 4.9% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 2.49% Params, 563.2 KMACs = 0.27% MACs, 1.16 MFLOPS = 0.28% FLOPs\n",
      "        (0): Conv2d(2.05 K = 2.34% Params, 563.2 KMACs = 0.27% MACs, 1.13 MFLOPS = 0.27% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 17.6 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  427.01 K\n",
      "fwd MACs:                                                               4.69 MMACs\n",
      "fwd FLOPs:                                                              9.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           14.07 MMACs\n",
      "fwd+bwd FLOPs:                                                          28.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  427.01 K = 100% Params, 4.69 MMACs = 100% MACs, 9.39 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(295.42 K = 69.18% Params, 3.24 MMACs = 69.19% MACs, 6.49 MFLOPS = 69.13% FLOPs, in_features=576, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (3): Linear(131.33 K = 30.76% Params, 1.44 MMACs = 30.75% MACs, 2.88 MFLOPS = 30.72% FLOPs, in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (6): Linear(257 = 0.06% Params, 2.82 KMACs = 0.06% MACs, 5.63 KFLOPS = 0.06% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_37_seed3/best_model.pth\n",
      "   Seed 3: val_rmse=20.061843, test_loss=537.941238, test_rmse=23.251876, val_loss=406.346161\n",
      "\n",
      "Best Trial: 37\n",
      "  Best Score: 19.6958\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512,256]\n",
      "    fusion_dropout: 0.1125423187454259\n",
      "    max_lr: 0.0006643909865591377\n",
      "    div_factor: 895\n",
      "    final_div_factor: 795\n",
      "    weight_decay: 0.006743930010561362\n",
      "    pct_start: 0.23038063722882474\n",
      "  Params: total=814,593  trainable=814,593\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  87.55 K \n",
      "fwd MACs:                                                               204.86 MMACs\n",
      "fwd FLOPs:                                                              413.53 MFLOPS\n",
      "fwd+bwd MACs:                                                           614.58 MMACs\n",
      "fwd+bwd FLOPs:                                                          1.24 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  87.55 K = 100% Params, 204.86 MMACs = 100% MACs, 413.53 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "    (net): Sequential(\n",
      "      7.15 K = 8.17% Params, 100.59 MMACs = 49.1% MACs, 203.23 MFLOPS = 49.15% FLOPs\n",
      "      (0): Conv2d(7.06 K = 8.06% Params, 100.59 MMACs = 49.1% MACs, 201.18 MFLOPS = 48.65% FLOPs, 3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(96 = 0.11% Params, 0 MACs = 0% MACs, 1.37 MFLOPS = 0.33% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 684.29 KFLOPS = 0.17% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    80.4 K = 91.83% Params, 104.27 MMACs = 50.9% MACs, 210.28 MFLOPS = 50.85% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      4.46 K = 5.1% Params, 62.95 MMACs = 30.73% MACs, 126.82 MFLOPS = 30.67% FLOPs\n",
      "      (conv1): Conv2d(3.46 K = 3.95% Params, 49.27 MMACs = 24.05% MACs, 98.54 MFLOPS = 23.83% FLOPs, 48, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(576 = 0.66% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        400 = 0.46% Params, 5.47 MMACs = 2.67% MACs, 11.18 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(384 = 0.44% Params, 5.47 MMACs = 2.67% MACs, 10.95 MFLOPS = 2.65% FLOPs, 48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16 = 0.02% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.06% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      3.68 K = 4.2% Params, 12.77 MMACs = 6.24% MACs, 26 MFLOPS = 6.29% FLOPs\n",
      "      (conv1): Conv2d(1.15 K = 1.32% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 2.63% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        160 = 0.18% Params, 456.19 KMACs = 0.22% MACs, 1.03 MFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(128 = 0.15% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.04% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.03% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      14.53 K = 16.59% Params, 12.77 MMACs = 6.24% MACs, 25.77 MFLOPS = 6.23% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 5.26% Params, 4.11 MMACs = 2% MACs, 8.21 MFLOPS = 1.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 10.53% Params, 8.21 MMACs = 4.01% MACs, 16.42 MFLOPS = 3.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.66% Params, 456.19 KMACs = 0.22% MACs, 969.41 KFLOPS = 0.23% FLOPs\n",
      "        (0): Conv2d(512 = 0.58% Params, 456.19 KMACs = 0.22% MACs, 912.38 KFLOPS = 0.22% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 65.94% Params, 15.77 MMACs = 7.7% MACs, 31.68 MFLOPS = 7.66% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 21.05% Params, 5.07 MMACs = 2.47% MACs, 10.14 MFLOPS = 2.45% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 42.11% Params, 10.14 MMACs = 4.95% MACs, 20.28 MFLOPS = 4.9% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 2.49% Params, 563.2 KMACs = 0.27% MACs, 1.16 MFLOPS = 0.28% FLOPs\n",
      "        (0): Conv2d(2.05 K = 2.34% Params, 563.2 KMACs = 0.27% MACs, 1.13 MFLOPS = 0.27% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.15% Params, 0 MACs = 0% MACs, 35.2 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 17.6 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  427.01 K\n",
      "fwd MACs:                                                               4.69 MMACs\n",
      "fwd FLOPs:                                                              9.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           14.07 MMACs\n",
      "fwd+bwd FLOPs:                                                          28.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  427.01 K = 100% Params, 4.69 MMACs = 100% MACs, 9.39 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(295.42 K = 69.18% Params, 3.24 MMACs = 69.19% MACs, 6.49 MFLOPS = 69.13% FLOPs, in_features=576, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (3): Linear(131.33 K = 30.76% Params, 1.44 MMACs = 30.75% MACs, 2.88 MFLOPS = 30.72% FLOPs, in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.82 KFLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1125423187454259, inplace=False)\n",
      "  (6): Linear(257 = 0.06% Params, 2.82 KMACs = 0.06% MACs, 5.63 KFLOPS = 0.06% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_37_seed4/best_model.pth\n",
      "   Seed 4: val_rmse=19.957369, test_loss=528.852971, test_rmse=23.007282, val_loss=401.764181\n",
      "\n",
      "Winner aggregated val_rmse: 19.828934 ± 0.366437\n",
      "Model params: total=814,593, trainable=814,593\n",
      "Saved multi-seed summary to: logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_37/winner_multi_seed_summary.txt\n",
      "{'winner_trial_number': 37, 'winner_trial_name': 'trial_37', 'primary_metric': 'val_rmse', 'aggregates': {'train_loss': {'mean': 289.3765965214482, 'std': 20.7588262780303}, 'val_loss': {'mean': 397.9609878540039, 'std': 13.158838832611897}, 'test_loss': {'mean': 535.4458902994792, 'std': 8.14072881467192}, 'min_lr': {'mean': 7.423362978314388e-07, 'std': 0.0}, 'max_lr': {'mean': 0.0006643909865591377, 'std': 0.0}, 'total_time': {'mean': 27.850912857055665, 'std': 0.050013558118791296}, 'average_epoch_time': {'mean': 0.2773601636886597, 'std': 0.0004324069832660929}, 'train_mse': {'mean': 289.3713806152344, 'std': 20.796448594821445}, 'train_mae': {'mean': 13.101933670043945, 'std': 0.5536993016549903}, 'train_rmse': {'mean': 17.002268187632637, 'std': 0.6064827811338649}, 'train_r2': {'mean': 0.9663483738899231, 'std': 0.0024184577509860808}, 'val_mse': {'mean': 393.2940612792969, 'std': 14.35960020253798}, 'val_mae': {'mean': 15.733129501342773, 'std': 0.4147892228411439}, 'val_rmse': {'mean': 19.828934429255575, 'std': 0.36643669231491327}, 'val_r2': {'mean': 0.9440341591835022, 'std': 0.0020433823447529795}, 'test_mse': {'mean': 536.5944458007813, 'std': 8.235979141850354}, 'test_mae': {'mean': 17.900419998168946, 'std': 0.24893712054638695}, 'test_rmse': {'mean': 23.163961270175548, 'std': 0.17798902560448948}, 'test_r2': {'mean': 0.9375478744506835, 'std': 0.0009585594172069672}, 'total_params': {'mean': 814593.0, 'std': 0.0}, 'trainable_params': {'mean': 814593.0, 'std': 0.0}, 'flops': {'mean': 429505472.0, 'std': 0.0}, 'macs': {'mean': 212838912.0, 'std': 0.0}}, 'total_params': 814593, 'trainable_params': 814593, 'flops': 429505472.0, 'macs': 212838912.0, 'summary_path': 'logs/Regression/Moneyball_cleaned/CNN_hybrid/DistanceMatrix/best_model/trial_37/winner_multi_seed_summary.txt'}\n"
     ]
    }
   ],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     attributes=attributes,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "     path_vision=path_vision, path_mlp=path_mlp,\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT: Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"Combination\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes — Train: (862, 72), Val: (185, 72), Test: (185, 72)\n",
      "Numerical features: 8 — ['Year', 'RA', 'W', 'OBP', 'SLG', 'BA', 'OOBP', 'OSLG']\n",
      "Categorical features: 6 — ['Team', 'League', 'Playoffs', 'RankSeason', 'RankPlayoffs', 'G']\n",
      "Total features: 72\n",
      "Images shape (C,H,W): (3, 72, 72)\n",
      "Attributes: 72\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=False, target_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6, 8, 9, 12, 18, 24, 36, 72]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = [2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vision=f\"./logs/{task_type}/{dataset_name}/{vision_name}/{name}/best_model/trial_91\"\n",
    "path_mlp=f\"./logs/{task_type}/{dataset_name}/mlp/best_model/trial_38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-16 05:01:58,987] A new study created in memory with name: no-name-a85b3d55-c6a6-419b-94db-b6a2eafa404a\n",
      "[I 2025-12-16 05:02:59,359] Trial 0 finished with value: 25.13047009794584 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.2540054955085003, 'max_lr': 8.822101135082419e-05, 'div_factor': 871, 'final_div_factor': 902, 'weight_decay': 9.399855179367282e-06, 'pct_start': 0.16240713635424858}. Best is trial 0 with value: 25.13047009794584.\n",
      "[I 2025-12-16 05:04:00,335] Trial 1 finished with value: 526.9720106419315 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.10882525812723393, 'max_lr': 1.9993133623816988e-05, 'div_factor': 519, 'final_div_factor': 349, 'weight_decay': 3.6964184832200727e-05, 'pct_start': 0.29601790972849185}. Best is trial 0 with value: 25.13047009794584.\n",
      "[I 2025-12-16 05:05:02,278] Trial 2 finished with value: 21.24306742959593 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.0654860134696681, 'max_lr': 0.003565452557286168, 'div_factor': 173, 'final_div_factor': 867, 'weight_decay': 0.0014220420161863362, 'pct_start': 0.34501547732618865}. Best is trial 2 with value: 21.24306742959593.\n",
      "[I 2025-12-16 05:06:04,403] Trial 3 finished with value: 20.832738435516735 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.05280206709419313, 'max_lr': 0.00033134298454374486, 'div_factor': 823, 'final_div_factor': 262, 'weight_decay': 1.1374837526024173e-06, 'pct_start': 0.19027969550505935}. Best is trial 3 with value: 20.832738435516735.\n",
      "[I 2025-12-16 05:07:06,384] Trial 4 finished with value: 20.78614031393011 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.06040990529130462, 'max_lr': 0.0010755876102320635, 'div_factor': 328, 'final_div_factor': 874, 'weight_decay': 8.276723938951219e-05, 'pct_start': 0.31854466019501676}. Best is trial 4 with value: 20.78614031393011.\n",
      "[I 2025-12-16 05:08:09,733] Trial 5 finished with value: 21.09555692492275 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.14384558085585658, 'max_lr': 0.0002312505488338953, 'div_factor': 82, 'final_div_factor': 409, 'weight_decay': 2.103024537720236e-05, 'pct_start': 0.24475769689411156}. Best is trial 4 with value: 20.78614031393011.\n",
      "[I 2025-12-16 05:09:12,150] Trial 6 finished with value: 22.382061233029063 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.2321179031686783, 'max_lr': 0.0025369824934606227, 'div_factor': 930, 'final_div_factor': 216, 'weight_decay': 0.0032033791333044895, 'pct_start': 0.23585691710152729}. Best is trial 4 with value: 20.78614031393011.\n",
      "[I 2025-12-16 05:10:12,564] Trial 7 finished with value: 44.76509325600417 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.1773625512351335, 'max_lr': 4.6121475624243835e-05, 'div_factor': 892, 'final_div_factor': 874, 'weight_decay': 2.165306657177362e-05, 'pct_start': 0.32952821512698965}. Best is trial 4 with value: 20.78614031393011.\n",
      "[I 2025-12-16 05:11:14,344] Trial 8 finished with value: 20.19481533478952 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.1974267699060838, 'max_lr': 0.002174231053727424, 'div_factor': 877, 'final_div_factor': 777, 'weight_decay': 5.2883954546039164e-05, 'pct_start': 0.252753549894411}. Best is trial 8 with value: 20.19481533478952.\n",
      "[I 2025-12-16 05:12:29,439] Trial 9 finished with value: 30.534592319613505 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.006113549640448701, 'max_lr': 4.923515416745692e-05, 'div_factor': 692, 'final_div_factor': 618, 'weight_decay': 6.536353548082907e-06, 'pct_start': 0.3300483282180561}. Best is trial 8 with value: 20.19481533478952.\n",
      "[I 2025-12-16 05:13:53,094] Trial 10 finished with value: 20.291917155741316 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.2037996675939764, 'max_lr': 0.008965769930771889, 'div_factor': 632, 'final_div_factor': 660, 'weight_decay': 0.00045355378605379116, 'pct_start': 0.10643810261699768}. Best is trial 8 with value: 20.19481533478952.\n",
      "[I 2025-12-16 05:14:55,098] Trial 11 finished with value: 20.117091929384635 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.19946804703309762, 'max_lr': 0.00938792844092786, 'div_factor': 634, 'final_div_factor': 665, 'weight_decay': 0.0007921063569036387, 'pct_start': 0.11218501649098817}. Best is trial 11 with value: 20.117091929384635.\n",
      "[I 2025-12-16 05:15:56,830] Trial 12 finished with value: 20.136325090114273 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.29445953381389756, 'max_lr': 0.009953158392958877, 'div_factor': 697, 'final_div_factor': 711, 'weight_decay': 0.00036650368206847864, 'pct_start': 0.11006784681668295}. Best is trial 11 with value: 20.117091929384635.\n",
      "[I 2025-12-16 05:16:58,671] Trial 13 finished with value: 23.423125800490954 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.2982246675055261, 'max_lr': 0.008639347595397737, 'div_factor': 462, 'final_div_factor': 497, 'weight_decay': 0.00040267853690294106, 'pct_start': 0.11701025193216155}. Best is trial 11 with value: 20.117091929384635.\n",
      "[I 2025-12-16 05:18:17,742] Trial 14 finished with value: 25.913633833506086 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.299796585614241, 'max_lr': 0.0007303854630488591, 'div_factor': 669, 'final_div_factor': 723, 'weight_decay': 0.007918092992976083, 'pct_start': 0.15631547560196468}. Best is trial 11 with value: 20.117091929384635.\n",
      "[I 2025-12-16 05:19:20,529] Trial 15 finished with value: 20.783846906584163 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.25340608555341987, 'max_lr': 0.005258873509102955, 'div_factor': 475, 'final_div_factor': 503, 'weight_decay': 0.00034782614007287233, 'pct_start': 0.14266367983728676}. Best is trial 11 with value: 20.117091929384635.\n",
      "[I 2025-12-16 05:20:20,685] Trial 16 finished with value: 22.60111759089408 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.14107325198259218, 'max_lr': 0.001061981449213432, 'div_factor': 759, 'final_div_factor': 997, 'weight_decay': 0.0011794084009296736, 'pct_start': 0.38309986321317047}. Best is trial 11 with value: 20.117091929384635.\n",
      "[I 2025-12-16 05:21:22,073] Trial 17 finished with value: 36.23499150510457 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.26604757136064283, 'max_lr': 0.008862917914756388, 'div_factor': 574, 'final_div_factor': 627, 'weight_decay': 0.00013841808418718805, 'pct_start': 0.20292944914042246}. Best is trial 11 with value: 20.117091929384635.\n",
      "[I 2025-12-16 05:22:29,771] Trial 18 finished with value: 21.53000126776224 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.2177819721316912, 'max_lr': 0.0017362509702516379, 'div_factor': 999, 'final_div_factor': 743, 'weight_decay': 0.00016895711765595257, 'pct_start': 0.1011542915636196}. Best is trial 11 with value: 20.117091929384635.\n",
      "[I 2025-12-16 05:23:31,308] Trial 19 finished with value: 20.20142859515525 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.1777712969823099, 'max_lr': 0.004791682675869648, 'div_factor': 255, 'final_div_factor': 117, 'weight_decay': 0.0012309710184052764, 'pct_start': 0.19285328523991466}. Best is trial 11 with value: 20.117091929384635.\n",
      "[I 2025-12-16 05:24:32,194] Trial 20 finished with value: 23.888439290549755 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.09374819286333214, 'max_lr': 0.0004104007471819279, 'div_factor': 344, 'final_div_factor': 534, 'weight_decay': 0.009670339052312833, 'pct_start': 0.1345032251534457}. Best is trial 11 with value: 20.117091929384635.\n",
      "[I 2025-12-16 05:25:33,354] Trial 21 finished with value: 19.909748913701563 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.1887536070722715, 'max_lr': 0.0027790801724628016, 'div_factor': 761, 'final_div_factor': 765, 'weight_decay': 7.371695170491163e-05, 'pct_start': 0.2705643026405789}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:26:34,448] Trial 22 finished with value: 20.428974204803993 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.1729500789905444, 'max_lr': 0.005141924880138659, 'div_factor': 750, 'final_div_factor': 758, 'weight_decay': 0.0003060499618235743, 'pct_start': 0.27806728375067796}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:27:35,563] Trial 23 finished with value: 21.050427154773782 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.2365712136277354, 'max_lr': 0.0031360624798463074, 'div_factor': 621, 'final_div_factor': 653, 'weight_decay': 0.0028818362226411074, 'pct_start': 0.22620591879830954}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:28:37,172] Trial 24 finished with value: 20.70313015919747 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.2735888629585125, 'max_lr': 0.009946061619920074, 'div_factor': 776, 'final_div_factor': 806, 'weight_decay': 0.0007260830813646781, 'pct_start': 0.2717884856978347}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:29:39,372] Trial 25 finished with value: 20.559308114488786 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.13995883128729844, 'max_lr': 0.001805435045059451, 'div_factor': 568, 'final_div_factor': 583, 'weight_decay': 0.00014947100704558358, 'pct_start': 0.16782077506219953}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:30:42,110] Trial 26 finished with value: 20.199960931419135 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.19201003857541735, 'max_lr': 0.005023515995202214, 'div_factor': 716, 'final_div_factor': 682, 'weight_decay': 0.0031047871817186387, 'pct_start': 0.12535524464453263}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:31:44,262] Trial 27 finished with value: 21.002627253810413 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.1216066486613016, 'max_lr': 0.0007217541811102406, 'div_factor': 403, 'final_div_factor': 977, 'weight_decay': 7.122522428166177e-05, 'pct_start': 0.2183923446454465}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:32:47,054] Trial 28 finished with value: 20.356765919496443 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.16283411411290244, 'max_lr': 0.006040073558354142, 'div_factor': 812, 'final_div_factor': 437, 'weight_decay': 0.00020531160676723944, 'pct_start': 0.1746802884928649}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:33:50,618] Trial 29 finished with value: 22.62053286463602 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.22459302431155256, 'max_lr': 0.00019242642682097574, 'div_factor': 600, 'final_div_factor': 809, 'weight_decay': 3.285870006468298e-06, 'pct_start': 0.1487155924460784}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:34:54,426] Trial 30 finished with value: 21.656103444670048 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.24855440893127534, 'max_lr': 0.0034215500966917442, 'div_factor': 539, 'final_div_factor': 928, 'weight_decay': 0.0006560860580688704, 'pct_start': 0.36828918287606227}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:36:00,847] Trial 31 finished with value: 20.686241641770472 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.20329043776453723, 'max_lr': 0.0022314053286314806, 'div_factor': 859, 'final_div_factor': 798, 'weight_decay': 5.092986240848289e-05, 'pct_start': 0.26058276104766837}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:37:03,514] Trial 32 finished with value: 20.11960240359899 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.1968635675589176, 'max_lr': 0.0016861760885560995, 'div_factor': 938, 'final_div_factor': 707, 'weight_decay': 2.975010599517731e-05, 'pct_start': 0.291352527643617}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:38:12,052] Trial 33 finished with value: 20.8761855833799 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.2710148957579801, 'max_lr': 0.0013112203067307435, 'div_factor': 664, 'final_div_factor': 702, 'weight_decay': 2.8679251475053554e-05, 'pct_start': 0.3052285909841248}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:39:15,700] Trial 34 finished with value: 710.2769706248401 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.15780598120433043, 'max_lr': 1.4789674846124483e-05, 'div_factor': 975, 'final_div_factor': 569, 'weight_decay': 9.604539168733121e-06, 'pct_start': 0.29206177753641144}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:40:18,454] Trial 35 finished with value: 20.0337872259649 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.11493428926182693, 'max_lr': 0.006803788150520997, 'div_factor': 815, 'final_div_factor': 712, 'weight_decay': 1.158909756332798e-05, 'pct_start': 0.2948166521907682}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:41:21,752] Trial 36 finished with value: 20.760920213359253 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.09268847842006821, 'max_lr': 0.0006001044108317878, 'div_factor': 930, 'final_div_factor': 840, 'weight_decay': 1.2750839779395095e-05, 'pct_start': 0.29246076077451016}. Best is trial 21 with value: 19.909748913701563.\n",
      "[I 2025-12-16 05:42:25,015] Trial 37 finished with value: 19.58582998823703 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.12127302372628164, 'max_lr': 0.0036376390246940513, 'div_factor': 817, 'final_div_factor': 595, 'weight_decay': 3.245030382463053e-06, 'pct_start': 0.35992753648560194}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:43:28,430] Trial 38 finished with value: 20.70579581829083 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.1288453604061798, 'max_lr': 0.0038596077591899515, 'div_factor': 781, 'final_div_factor': 382, 'weight_decay': 1.287998945643555e-06, 'pct_start': 0.35415566773048107}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:44:31,927] Trial 39 finished with value: 23.692212282764192 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.09937677525279483, 'max_lr': 0.0001138849006711119, 'div_factor': 838, 'final_div_factor': 592, 'weight_decay': 3.747165405695564e-06, 'pct_start': 0.39575699827177635}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:45:35,822] Trial 40 finished with value: 21.565861802016443 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.06627004547515211, 'max_lr': 0.006775489333167295, 'div_factor': 819, 'final_div_factor': 456, 'weight_decay': 2.1113901072158143e-06, 'pct_start': 0.3147621223948867}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:46:40,749] Trial 41 finished with value: 21.83171358967772 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.07786119822184825, 'max_lr': 0.003388445831111337, 'div_factor': 914, 'final_div_factor': 648, 'weight_decay': 6.9719392208158745e-06, 'pct_start': 0.34140688996360646}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:47:44,743] Trial 42 finished with value: 22.035414887796957 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.11573119655896415, 'max_lr': 0.0013423447801973372, 'div_factor': 730, 'final_div_factor': 754, 'weight_decay': 1.7474442708359232e-05, 'pct_start': 0.27892927818612395}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:48:48,868] Trial 43 finished with value: 19.90829577031936 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.027338176153879887, 'max_lr': 0.0025304268474329646, 'div_factor': 962, 'final_div_factor': 908, 'weight_decay': 3.886459329068295e-05, 'pct_start': 0.24082257634034807}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:49:53,197] Trial 44 finished with value: 20.19517347631065 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.03599903806272753, 'max_lr': 0.002663020368284111, 'div_factor': 879, 'final_div_factor': 903, 'weight_decay': 9.342111931461504e-05, 'pct_start': 0.24014722417602555}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:50:57,507] Trial 45 finished with value: 20.524476860339828 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.003965117501567531, 'max_lr': 0.006980247057791015, 'div_factor': 26, 'final_div_factor': 846, 'weight_decay': 5.259436767397025e-06, 'pct_start': 0.2646470832634893}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:52:01,987] Trial 46 finished with value: 20.06147777769624 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.023014935092422393, 'max_lr': 0.004481483567328792, 'div_factor': 803, 'final_div_factor': 929, 'weight_decay': 5.0029583194322314e-05, 'pct_start': 0.3303288002338135}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:53:10,431] Trial 47 finished with value: 20.02311591382414 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.03014648852030733, 'max_lr': 0.00377793187480828, 'div_factor': 976, 'final_div_factor': 924, 'weight_decay': 5.08431825325739e-05, 'pct_start': 0.32680212061303593}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:54:15,577] Trial 48 finished with value: 19.900448802698598 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.026757115999312193, 'max_lr': 0.002497223630438842, 'div_factor': 959, 'final_div_factor': 879, 'weight_decay': 1.781827375027565e-05, 'pct_start': 0.3143261426940812}. Best is trial 37 with value: 19.58582998823703.\n",
      "[I 2025-12-16 05:55:19,870] Trial 49 finished with value: 20.327964365718135 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.045714098289397126, 'max_lr': 0.0010811573800621488, 'div_factor': 959, 'final_div_factor': 879, 'weight_decay': 1.646224879802117e-05, 'pct_start': 0.35833344616325663}. Best is trial 37 with value: 19.58582998823703.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    attributes=attributes,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    epochs=epochs,\n",
    "    path_vision=path_vision,\n",
    "    path_mlp=path_mlp\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating top-5 trials once at 100 epochs (seed=0)...\n",
      "\n",
      "→ Single-pass full run (Trial 37, ValObjective: 19.5858)\n",
      "\n",
      "Best Trial: 37\n",
      "  Best Score: 19.5858\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512]\n",
      "    fusion_dropout: 0.12127302372628164\n",
      "    max_lr: 0.0036376390246940513\n",
      "    div_factor: 817\n",
      "    final_div_factor: 595\n",
      "    weight_decay: 3.245030382463053e-06\n",
      "    pct_start: 0.35992753648560194\n",
      "  Params: total=782,945  trainable=782,945\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  186.98 K\n",
      "fwd MACs:                                                               546.52 MMACs\n",
      "fwd FLOPs:                                                              1.1 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.64 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.3 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  186.98 K = 100% Params, 546.52 MMACs = 100% MACs, 1.1 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "    (net): Sequential(\n",
      "      9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "      (0): Conv2d(9.41 K = 5.03% Params, 134.12 MMACs = 24.54% MACs, 268.24 MFLOPS = 24.38% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    177.44 K = 94.9% Params, 412.4 MMACs = 75.46% MACs, 829.01 MFLOPS = 75.36% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      12.64 K = 6.76% Params, 178.83 MMACs = 32.72% MACs, 359.48 MFLOPS = 32.68% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 131.38 MMACs = 24.04% MACs, 262.77 MFLOPS = 23.89% FLOPs, 64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.23% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.06 K = 0.56% Params, 14.6 MMACs = 2.67% MACs, 29.65 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(1.02 K = 0.55% Params, 14.6 MMACs = 2.67% MACs, 29.2 MFLOPS = 2.65% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 7.77% Params, 51.09 MMACs = 9.35% MACs, 103.1 MFLOPS = 9.37% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.46% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.31% Params, 1.82 MMACs = 0.33% MACs, 3.88 MFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(512 = 0.27% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 9.93% Params, 65.69 MMACs = 12.02% MACs, 132.07 MFLOPS = 12.01% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 30.87% Params, 51.09 MMACs = 9.35% MACs, 102.64 MFLOPS = 9.33% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 9.86% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.16% Params, 1.82 MMACs = 0.33% MACs, 3.76 MFLOPS = 0.34% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.1% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 39.57% Params, 65.69 MMACs = 12.02% MACs, 131.73 MFLOPS = 11.97% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  295.94 K\n",
      "fwd MACs:                                                               3.25 MMACs\n",
      "fwd FLOPs:                                                              6.5 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.75 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.51 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  295.94 K = 100% Params, 3.25 MMACs = 100% MACs, 6.5 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(295.42 K = 99.83% Params, 3.24 MMACs = 99.83% MACs, 6.49 MFLOPS = 99.74% FLOPs, in_features=576, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.12127302372628164, inplace=False)\n",
      "  (3): Linear(513 = 0.17% Params, 5.63 KMACs = 0.17% MACs, 11.26 KFLOPS = 0.17% FLOPs, in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_37/best_model.pth\n",
      "   val_rmse=19.821780\n",
      "   params: total=782,945, trainable=782,945\n",
      "→ Single-pass full run (Trial 48, ValObjective: 19.9004)\n",
      "\n",
      "Best Trial: 48\n",
      "  Best Score: 19.9004\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512]\n",
      "    fusion_dropout: 0.026757115999312193\n",
      "    max_lr: 0.002497223630438842\n",
      "    div_factor: 959\n",
      "    final_div_factor: 879\n",
      "    weight_decay: 1.781827375027565e-05\n",
      "    pct_start: 0.3143261426940812\n",
      "  Params: total=782,945  trainable=782,945\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  186.98 K\n",
      "fwd MACs:                                                               546.52 MMACs\n",
      "fwd FLOPs:                                                              1.1 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.64 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.3 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  186.98 K = 100% Params, 546.52 MMACs = 100% MACs, 1.1 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "    (net): Sequential(\n",
      "      9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "      (0): Conv2d(9.41 K = 5.03% Params, 134.12 MMACs = 24.54% MACs, 268.24 MFLOPS = 24.38% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    177.44 K = 94.9% Params, 412.4 MMACs = 75.46% MACs, 829.01 MFLOPS = 75.36% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      12.64 K = 6.76% Params, 178.83 MMACs = 32.72% MACs, 359.48 MFLOPS = 32.68% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 131.38 MMACs = 24.04% MACs, 262.77 MFLOPS = 23.89% FLOPs, 64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.23% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.06 K = 0.56% Params, 14.6 MMACs = 2.67% MACs, 29.65 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(1.02 K = 0.55% Params, 14.6 MMACs = 2.67% MACs, 29.2 MFLOPS = 2.65% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 7.77% Params, 51.09 MMACs = 9.35% MACs, 103.1 MFLOPS = 9.37% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.46% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.31% Params, 1.82 MMACs = 0.33% MACs, 3.88 MFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(512 = 0.27% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 9.93% Params, 65.69 MMACs = 12.02% MACs, 132.07 MFLOPS = 12.01% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 30.87% Params, 51.09 MMACs = 9.35% MACs, 102.64 MFLOPS = 9.33% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 9.86% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.16% Params, 1.82 MMACs = 0.33% MACs, 3.76 MFLOPS = 0.34% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.1% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 39.57% Params, 65.69 MMACs = 12.02% MACs, 131.73 MFLOPS = 11.97% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  295.94 K\n",
      "fwd MACs:                                                               3.25 MMACs\n",
      "fwd FLOPs:                                                              6.5 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.75 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.51 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  295.94 K = 100% Params, 3.25 MMACs = 100% MACs, 6.5 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(295.42 K = 99.83% Params, 3.24 MMACs = 99.83% MACs, 6.49 MFLOPS = 99.74% FLOPs, in_features=576, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.026757115999312193, inplace=False)\n",
      "  (3): Linear(513 = 0.17% Params, 5.63 KMACs = 0.17% MACs, 11.26 KFLOPS = 0.17% FLOPs, in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_48/best_model.pth\n",
      "   val_rmse=19.926726\n",
      "   params: total=782,945, trainable=782,945\n",
      "→ Single-pass full run (Trial 43, ValObjective: 19.9083)\n",
      "\n",
      "Best Trial: 43\n",
      "  Best Score: 19.9083\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.027338176153879887\n",
      "    max_lr: 0.0025304268474329646\n",
      "    div_factor: 962\n",
      "    final_div_factor: 908\n",
      "    weight_decay: 3.886459329068295e-05\n",
      "    pct_start: 0.24082257634034807\n",
      "  Params: total=569,185  trainable=569,185\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  186.98 K\n",
      "fwd MACs:                                                               546.52 MMACs\n",
      "fwd FLOPs:                                                              1.1 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.64 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.3 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  186.98 K = 100% Params, 546.52 MMACs = 100% MACs, 1.1 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "    (net): Sequential(\n",
      "      9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "      (0): Conv2d(9.41 K = 5.03% Params, 134.12 MMACs = 24.54% MACs, 268.24 MFLOPS = 24.38% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    177.44 K = 94.9% Params, 412.4 MMACs = 75.46% MACs, 829.01 MFLOPS = 75.36% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      12.64 K = 6.76% Params, 178.83 MMACs = 32.72% MACs, 359.48 MFLOPS = 32.68% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 131.38 MMACs = 24.04% MACs, 262.77 MFLOPS = 23.89% FLOPs, 64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.23% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.06 K = 0.56% Params, 14.6 MMACs = 2.67% MACs, 29.65 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(1.02 K = 0.55% Params, 14.6 MMACs = 2.67% MACs, 29.2 MFLOPS = 2.65% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 7.77% Params, 51.09 MMACs = 9.35% MACs, 103.1 MFLOPS = 9.37% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.46% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.31% Params, 1.82 MMACs = 0.33% MACs, 3.88 MFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(512 = 0.27% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 9.93% Params, 65.69 MMACs = 12.02% MACs, 132.07 MFLOPS = 12.01% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 30.87% Params, 51.09 MMACs = 9.35% MACs, 102.64 MFLOPS = 9.33% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 9.86% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.16% Params, 1.82 MMACs = 0.33% MACs, 3.76 MFLOPS = 0.34% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.1% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 39.57% Params, 65.69 MMACs = 12.02% MACs, 131.73 MFLOPS = 11.97% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  82.18 K \n",
      "fwd MACs:                                                               901.82 KMACs\n",
      "fwd FLOPs:                                                              1.81 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.71 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.42 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  82.18 K = 100% Params, 901.82 KMACs = 100% MACs, 1.81 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 89.87% Params, 811.01 KMACs = 89.93% MACs, 1.62 MFLOPS = 89.82% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (3): Linear(8.26 K = 10.05% Params, 90.11 KMACs = 9.99% MACs, 180.22 KFLOPS = 9.98% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.04% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (6): Linear(65 = 0.08% Params, 704 MACs = 0.08% MACs, 1.41 KFLOPS = 0.08% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_43/best_model.pth\n",
      "   val_rmse=19.583659\n",
      "   params: total=569,185, trainable=569,185\n",
      "→ Single-pass full run (Trial 21, ValObjective: 19.9097)\n",
      "\n",
      "Best Trial: 21\n",
      "  Best Score: 19.9097\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128]\n",
      "    fusion_dropout: 0.1887536070722715\n",
      "    max_lr: 0.0027790801724628016\n",
      "    div_factor: 761\n",
      "    final_div_factor: 765\n",
      "    weight_decay: 7.371695170491163e-05\n",
      "    pct_start: 0.2705643026405789\n",
      "  Params: total=560,993  trainable=560,993\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  186.98 K\n",
      "fwd MACs:                                                               546.52 MMACs\n",
      "fwd FLOPs:                                                              1.1 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.64 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.3 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  186.98 K = 100% Params, 546.52 MMACs = 100% MACs, 1.1 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "    (net): Sequential(\n",
      "      9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "      (0): Conv2d(9.41 K = 5.03% Params, 134.12 MMACs = 24.54% MACs, 268.24 MFLOPS = 24.38% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    177.44 K = 94.9% Params, 412.4 MMACs = 75.46% MACs, 829.01 MFLOPS = 75.36% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      12.64 K = 6.76% Params, 178.83 MMACs = 32.72% MACs, 359.48 MFLOPS = 32.68% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 131.38 MMACs = 24.04% MACs, 262.77 MFLOPS = 23.89% FLOPs, 64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.23% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.06 K = 0.56% Params, 14.6 MMACs = 2.67% MACs, 29.65 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(1.02 K = 0.55% Params, 14.6 MMACs = 2.67% MACs, 29.2 MFLOPS = 2.65% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 7.77% Params, 51.09 MMACs = 9.35% MACs, 103.1 MFLOPS = 9.37% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.46% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.31% Params, 1.82 MMACs = 0.33% MACs, 3.88 MFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(512 = 0.27% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 9.93% Params, 65.69 MMACs = 12.02% MACs, 132.07 MFLOPS = 12.01% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 30.87% Params, 51.09 MMACs = 9.35% MACs, 102.64 MFLOPS = 9.33% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 9.86% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.16% Params, 1.82 MMACs = 0.33% MACs, 3.76 MFLOPS = 0.34% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.1% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 39.57% Params, 65.69 MMACs = 12.02% MACs, 131.73 MFLOPS = 11.97% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  73.98 K \n",
      "fwd MACs:                                                               812.42 KMACs\n",
      "fwd FLOPs:                                                              1.63 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.44 MMACs\n",
      "fwd+bwd FLOPs:                                                          4.88 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  73.98 K = 100% Params, 812.42 KMACs = 100% MACs, 1.63 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 99.83% Params, 811.01 KMACs = 99.83% MACs, 1.62 MFLOPS = 99.74% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1887536070722715, inplace=False)\n",
      "  (3): Linear(129 = 0.17% Params, 1.41 KMACs = 0.17% MACs, 2.82 KFLOPS = 0.17% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_21/best_model.pth\n",
      "   val_rmse=19.821110\n",
      "   params: total=560,993, trainable=560,993\n",
      "→ Single-pass full run (Trial 47, ValObjective: 20.0231)\n",
      "\n",
      "Best Trial: 47\n",
      "  Best Score: 20.0231\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [512]\n",
      "    fusion_dropout: 0.03014648852030733\n",
      "    max_lr: 0.00377793187480828\n",
      "    div_factor: 976\n",
      "    final_div_factor: 924\n",
      "    weight_decay: 5.08431825325739e-05\n",
      "    pct_start: 0.32680212061303593\n",
      "  Params: total=782,945  trainable=782,945\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  186.98 K\n",
      "fwd MACs:                                                               546.52 MMACs\n",
      "fwd FLOPs:                                                              1.1 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.64 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.3 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  186.98 K = 100% Params, 546.52 MMACs = 100% MACs, 1.1 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "    (net): Sequential(\n",
      "      9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "      (0): Conv2d(9.41 K = 5.03% Params, 134.12 MMACs = 24.54% MACs, 268.24 MFLOPS = 24.38% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    177.44 K = 94.9% Params, 412.4 MMACs = 75.46% MACs, 829.01 MFLOPS = 75.36% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      12.64 K = 6.76% Params, 178.83 MMACs = 32.72% MACs, 359.48 MFLOPS = 32.68% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 131.38 MMACs = 24.04% MACs, 262.77 MFLOPS = 23.89% FLOPs, 64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.23% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.06 K = 0.56% Params, 14.6 MMACs = 2.67% MACs, 29.65 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(1.02 K = 0.55% Params, 14.6 MMACs = 2.67% MACs, 29.2 MFLOPS = 2.65% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 7.77% Params, 51.09 MMACs = 9.35% MACs, 103.1 MFLOPS = 9.37% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.46% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.31% Params, 1.82 MMACs = 0.33% MACs, 3.88 MFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(512 = 0.27% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 9.93% Params, 65.69 MMACs = 12.02% MACs, 132.07 MFLOPS = 12.01% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 30.87% Params, 51.09 MMACs = 9.35% MACs, 102.64 MFLOPS = 9.33% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 9.86% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.16% Params, 1.82 MMACs = 0.33% MACs, 3.76 MFLOPS = 0.34% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.1% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 39.57% Params, 65.69 MMACs = 12.02% MACs, 131.73 MFLOPS = 11.97% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  295.94 K\n",
      "fwd MACs:                                                               3.25 MMACs\n",
      "fwd FLOPs:                                                              6.5 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.75 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.51 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  295.94 K = 100% Params, 3.25 MMACs = 100% MACs, 6.5 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(295.42 K = 99.83% Params, 3.24 MMACs = 99.83% MACs, 6.49 MFLOPS = 99.74% FLOPs, in_features=576, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.03014648852030733, inplace=False)\n",
      "  (3): Linear(513 = 0.17% Params, 5.63 KMACs = 0.17% MACs, 11.26 KFLOPS = 0.17% FLOPs, in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_47/best_model.pth\n",
      "   val_rmse=20.789424\n",
      "   params: total=782,945, trainable=782,945\n",
      "\n",
      "Winner after single-pass: Trial 43 (trial_43) by val_rmse=19.583659\n",
      "\n",
      "Re-running winner with seeds [0, 1, 2, 3, 4] at 100 epochs...\n",
      "\n",
      "\n",
      "Best Trial: 43\n",
      "  Best Score: 19.9083\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.027338176153879887\n",
      "    max_lr: 0.0025304268474329646\n",
      "    div_factor: 962\n",
      "    final_div_factor: 908\n",
      "    weight_decay: 3.886459329068295e-05\n",
      "    pct_start: 0.24082257634034807\n",
      "  Params: total=569,185  trainable=569,185\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  186.98 K\n",
      "fwd MACs:                                                               546.52 MMACs\n",
      "fwd FLOPs:                                                              1.1 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.64 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.3 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  186.98 K = 100% Params, 546.52 MMACs = 100% MACs, 1.1 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "    (net): Sequential(\n",
      "      9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "      (0): Conv2d(9.41 K = 5.03% Params, 134.12 MMACs = 24.54% MACs, 268.24 MFLOPS = 24.38% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    177.44 K = 94.9% Params, 412.4 MMACs = 75.46% MACs, 829.01 MFLOPS = 75.36% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      12.64 K = 6.76% Params, 178.83 MMACs = 32.72% MACs, 359.48 MFLOPS = 32.68% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 131.38 MMACs = 24.04% MACs, 262.77 MFLOPS = 23.89% FLOPs, 64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.23% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.06 K = 0.56% Params, 14.6 MMACs = 2.67% MACs, 29.65 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(1.02 K = 0.55% Params, 14.6 MMACs = 2.67% MACs, 29.2 MFLOPS = 2.65% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 7.77% Params, 51.09 MMACs = 9.35% MACs, 103.1 MFLOPS = 9.37% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.46% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.31% Params, 1.82 MMACs = 0.33% MACs, 3.88 MFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(512 = 0.27% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 9.93% Params, 65.69 MMACs = 12.02% MACs, 132.07 MFLOPS = 12.01% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 30.87% Params, 51.09 MMACs = 9.35% MACs, 102.64 MFLOPS = 9.33% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 9.86% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.16% Params, 1.82 MMACs = 0.33% MACs, 3.76 MFLOPS = 0.34% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.1% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 39.57% Params, 65.69 MMACs = 12.02% MACs, 131.73 MFLOPS = 11.97% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  82.18 K \n",
      "fwd MACs:                                                               901.82 KMACs\n",
      "fwd FLOPs:                                                              1.81 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.71 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.42 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  82.18 K = 100% Params, 901.82 KMACs = 100% MACs, 1.81 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 89.87% Params, 811.01 KMACs = 89.93% MACs, 1.62 MFLOPS = 89.82% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (3): Linear(8.26 K = 10.05% Params, 90.11 KMACs = 9.99% MACs, 180.22 KFLOPS = 9.98% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.04% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (6): Linear(65 = 0.08% Params, 704 MACs = 0.08% MACs, 1.41 KFLOPS = 0.08% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_43_seed0/best_model.pth\n",
      "   Seed 0: val_rmse=19.583659, test_loss=561.622925, test_rmse=23.696512, val_loss=384.723429\n",
      "\n",
      "Best Trial: 43\n",
      "  Best Score: 19.9083\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.027338176153879887\n",
      "    max_lr: 0.0025304268474329646\n",
      "    div_factor: 962\n",
      "    final_div_factor: 908\n",
      "    weight_decay: 3.886459329068295e-05\n",
      "    pct_start: 0.24082257634034807\n",
      "  Params: total=569,185  trainable=569,185\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  186.98 K\n",
      "fwd MACs:                                                               546.52 MMACs\n",
      "fwd FLOPs:                                                              1.1 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.64 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.3 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  186.98 K = 100% Params, 546.52 MMACs = 100% MACs, 1.1 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "    (net): Sequential(\n",
      "      9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "      (0): Conv2d(9.41 K = 5.03% Params, 134.12 MMACs = 24.54% MACs, 268.24 MFLOPS = 24.38% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    177.44 K = 94.9% Params, 412.4 MMACs = 75.46% MACs, 829.01 MFLOPS = 75.36% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      12.64 K = 6.76% Params, 178.83 MMACs = 32.72% MACs, 359.48 MFLOPS = 32.68% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 131.38 MMACs = 24.04% MACs, 262.77 MFLOPS = 23.89% FLOPs, 64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.23% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.06 K = 0.56% Params, 14.6 MMACs = 2.67% MACs, 29.65 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(1.02 K = 0.55% Params, 14.6 MMACs = 2.67% MACs, 29.2 MFLOPS = 2.65% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 7.77% Params, 51.09 MMACs = 9.35% MACs, 103.1 MFLOPS = 9.37% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.46% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.31% Params, 1.82 MMACs = 0.33% MACs, 3.88 MFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(512 = 0.27% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 9.93% Params, 65.69 MMACs = 12.02% MACs, 132.07 MFLOPS = 12.01% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 30.87% Params, 51.09 MMACs = 9.35% MACs, 102.64 MFLOPS = 9.33% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 9.86% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.16% Params, 1.82 MMACs = 0.33% MACs, 3.76 MFLOPS = 0.34% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.1% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 39.57% Params, 65.69 MMACs = 12.02% MACs, 131.73 MFLOPS = 11.97% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  82.18 K \n",
      "fwd MACs:                                                               901.82 KMACs\n",
      "fwd FLOPs:                                                              1.81 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.71 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.42 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  82.18 K = 100% Params, 901.82 KMACs = 100% MACs, 1.81 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 89.87% Params, 811.01 KMACs = 89.93% MACs, 1.62 MFLOPS = 89.82% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (3): Linear(8.26 K = 10.05% Params, 90.11 KMACs = 9.99% MACs, 180.22 KFLOPS = 9.98% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.04% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (6): Linear(65 = 0.08% Params, 704 MACs = 0.08% MACs, 1.41 KFLOPS = 0.08% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_43_seed1/best_model.pth\n",
      "   Seed 1: val_rmse=20.273249, test_loss=557.589717, test_rmse=23.644695, val_loss=413.366997\n",
      "\n",
      "Best Trial: 43\n",
      "  Best Score: 19.9083\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.027338176153879887\n",
      "    max_lr: 0.0025304268474329646\n",
      "    div_factor: 962\n",
      "    final_div_factor: 908\n",
      "    weight_decay: 3.886459329068295e-05\n",
      "    pct_start: 0.24082257634034807\n",
      "  Params: total=569,185  trainable=569,185\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  186.98 K\n",
      "fwd MACs:                                                               546.52 MMACs\n",
      "fwd FLOPs:                                                              1.1 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.64 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.3 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  186.98 K = 100% Params, 546.52 MMACs = 100% MACs, 1.1 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "    (net): Sequential(\n",
      "      9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "      (0): Conv2d(9.41 K = 5.03% Params, 134.12 MMACs = 24.54% MACs, 268.24 MFLOPS = 24.38% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    177.44 K = 94.9% Params, 412.4 MMACs = 75.46% MACs, 829.01 MFLOPS = 75.36% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      12.64 K = 6.76% Params, 178.83 MMACs = 32.72% MACs, 359.48 MFLOPS = 32.68% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 131.38 MMACs = 24.04% MACs, 262.77 MFLOPS = 23.89% FLOPs, 64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.23% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.06 K = 0.56% Params, 14.6 MMACs = 2.67% MACs, 29.65 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(1.02 K = 0.55% Params, 14.6 MMACs = 2.67% MACs, 29.2 MFLOPS = 2.65% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 7.77% Params, 51.09 MMACs = 9.35% MACs, 103.1 MFLOPS = 9.37% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.46% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.31% Params, 1.82 MMACs = 0.33% MACs, 3.88 MFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(512 = 0.27% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 9.93% Params, 65.69 MMACs = 12.02% MACs, 132.07 MFLOPS = 12.01% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 30.87% Params, 51.09 MMACs = 9.35% MACs, 102.64 MFLOPS = 9.33% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 9.86% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.16% Params, 1.82 MMACs = 0.33% MACs, 3.76 MFLOPS = 0.34% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.1% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 39.57% Params, 65.69 MMACs = 12.02% MACs, 131.73 MFLOPS = 11.97% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  82.18 K \n",
      "fwd MACs:                                                               901.82 KMACs\n",
      "fwd FLOPs:                                                              1.81 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.71 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.42 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  82.18 K = 100% Params, 901.82 KMACs = 100% MACs, 1.81 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 89.87% Params, 811.01 KMACs = 89.93% MACs, 1.62 MFLOPS = 89.82% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (3): Linear(8.26 K = 10.05% Params, 90.11 KMACs = 9.99% MACs, 180.22 KFLOPS = 9.98% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.04% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (6): Linear(65 = 0.08% Params, 704 MACs = 0.08% MACs, 1.41 KFLOPS = 0.08% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_43_seed2/best_model.pth\n",
      "   Seed 2: val_rmse=20.395461, test_loss=512.776210, test_rmse=22.720531, val_loss=418.683746\n",
      "\n",
      "Best Trial: 43\n",
      "  Best Score: 19.9083\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.027338176153879887\n",
      "    max_lr: 0.0025304268474329646\n",
      "    div_factor: 962\n",
      "    final_div_factor: 908\n",
      "    weight_decay: 3.886459329068295e-05\n",
      "    pct_start: 0.24082257634034807\n",
      "  Params: total=569,185  trainable=569,185\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  186.98 K\n",
      "fwd MACs:                                                               546.52 MMACs\n",
      "fwd FLOPs:                                                              1.1 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.64 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.3 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  186.98 K = 100% Params, 546.52 MMACs = 100% MACs, 1.1 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "    (net): Sequential(\n",
      "      9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "      (0): Conv2d(9.41 K = 5.03% Params, 134.12 MMACs = 24.54% MACs, 268.24 MFLOPS = 24.38% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    177.44 K = 94.9% Params, 412.4 MMACs = 75.46% MACs, 829.01 MFLOPS = 75.36% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      12.64 K = 6.76% Params, 178.83 MMACs = 32.72% MACs, 359.48 MFLOPS = 32.68% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 131.38 MMACs = 24.04% MACs, 262.77 MFLOPS = 23.89% FLOPs, 64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.23% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.06 K = 0.56% Params, 14.6 MMACs = 2.67% MACs, 29.65 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(1.02 K = 0.55% Params, 14.6 MMACs = 2.67% MACs, 29.2 MFLOPS = 2.65% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 7.77% Params, 51.09 MMACs = 9.35% MACs, 103.1 MFLOPS = 9.37% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.46% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.31% Params, 1.82 MMACs = 0.33% MACs, 3.88 MFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(512 = 0.27% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 9.93% Params, 65.69 MMACs = 12.02% MACs, 132.07 MFLOPS = 12.01% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 30.87% Params, 51.09 MMACs = 9.35% MACs, 102.64 MFLOPS = 9.33% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 9.86% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.16% Params, 1.82 MMACs = 0.33% MACs, 3.76 MFLOPS = 0.34% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.1% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 39.57% Params, 65.69 MMACs = 12.02% MACs, 131.73 MFLOPS = 11.97% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  82.18 K \n",
      "fwd MACs:                                                               901.82 KMACs\n",
      "fwd FLOPs:                                                              1.81 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.71 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.42 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  82.18 K = 100% Params, 901.82 KMACs = 100% MACs, 1.81 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 89.87% Params, 811.01 KMACs = 89.93% MACs, 1.62 MFLOPS = 89.82% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (3): Linear(8.26 K = 10.05% Params, 90.11 KMACs = 9.99% MACs, 180.22 KFLOPS = 9.98% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.04% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (6): Linear(65 = 0.08% Params, 704 MACs = 0.08% MACs, 1.41 KFLOPS = 0.08% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_43_seed3/best_model.pth\n",
      "   Seed 3: val_rmse=20.751223, test_loss=594.222132, test_rmse=24.437439, val_loss=434.523951\n",
      "\n",
      "Best Trial: 43\n",
      "  Best Score: 19.9083\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.027338176153879887\n",
      "    max_lr: 0.0025304268474329646\n",
      "    div_factor: 962\n",
      "    final_div_factor: 908\n",
      "    weight_decay: 3.886459329068295e-05\n",
      "    pct_start: 0.24082257634034807\n",
      "  Params: total=569,185  trainable=569,185\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  186.98 K\n",
      "fwd MACs:                                                               546.52 MMACs\n",
      "fwd FLOPs:                                                              1.1 GFLOPS\n",
      "fwd+bwd MACs:                                                           1.64 GMACs\n",
      "fwd+bwd FLOPs:                                                          3.3 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  186.98 K = 100% Params, 546.52 MMACs = 100% MACs, 1.1 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "    (net): Sequential(\n",
      "      9.54 K = 5.1% Params, 134.12 MMACs = 24.54% MACs, 270.98 MFLOPS = 24.63% FLOPs\n",
      "      (0): Conv2d(9.41 K = 5.03% Params, 134.12 MMACs = 24.54% MACs, 268.24 MFLOPS = 24.38% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 1.82 MFLOPS = 0.17% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 912.38 KFLOPS = 0.08% FLOPs, inplace=True)\n",
      "      (3): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    177.44 K = 94.9% Params, 412.4 MMACs = 75.46% MACs, 829.01 MFLOPS = 75.36% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      12.64 K = 6.76% Params, 178.83 MMACs = 32.72% MACs, 359.48 MFLOPS = 32.68% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 131.38 MMACs = 24.04% MACs, 262.77 MFLOPS = 23.89% FLOPs, 64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.3 K = 1.23% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.06 K = 0.56% Params, 14.6 MMACs = 2.67% MACs, 29.65 MFLOPS = 2.7% FLOPs\n",
      "        (0): Conv2d(1.02 K = 0.55% Params, 14.6 MMACs = 2.67% MACs, 29.2 MFLOPS = 2.65% FLOPs, 64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32 = 0.02% Params, 0 MACs = 0% MACs, 456.19 KFLOPS = 0.04% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      14.53 K = 7.77% Params, 51.09 MMACs = 9.35% MACs, 103.1 MFLOPS = 9.37% FLOPs\n",
      "      (conv1): Conv2d(4.61 K = 2.46% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        576 = 0.31% Params, 1.82 MMACs = 0.33% MACs, 3.88 MFLOPS = 0.35% FLOPs\n",
      "        (0): Conv2d(512 = 0.27% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      18.56 K = 9.93% Params, 65.69 MMACs = 12.02% MACs, 132.07 MFLOPS = 12.01% FLOPs\n",
      "      (conv1): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(9.22 K = 4.93% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64 = 0.03% Params, 0 MACs = 0% MACs, 228.1 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      57.73 K = 30.87% Params, 51.09 MMACs = 9.35% MACs, 102.64 MFLOPS = 9.33% FLOPs\n",
      "      (conv1): Conv2d(18.43 K = 9.86% Params, 16.42 MMACs = 3.01% MACs, 32.85 MFLOPS = 2.99% FLOPs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.18 K = 1.16% Params, 1.82 MMACs = 0.33% MACs, 3.76 MFLOPS = 0.34% FLOPs\n",
      "        (0): Conv2d(2.05 K = 1.1% Params, 1.82 MMACs = 0.33% MACs, 3.65 MFLOPS = 0.33% FLOPs, 32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      73.98 K = 39.57% Params, 65.69 MMACs = 12.02% MACs, 131.73 MFLOPS = 11.97% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 19.72% Params, 32.85 MMACs = 6.01% MACs, 65.69 MFLOPS = 5.97% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.07% Params, 0 MACs = 0% MACs, 114.05 KFLOPS = 0.01% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 57.02 KFLOPS = 0.01% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               3.29 MMACs\n",
      "fwd FLOPs:                                                              6.59 MFLOPS\n",
      "fwd+bwd MACs:                                                           9.87 MMACs\n",
      "fwd+bwd FLOPs:                                                          19.77 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 3.29 MMACs = 100% MACs, 6.59 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 405.5 KMACs = 12.33% MACs, 811.01 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 2.88 MMACs = 87.67% MACs, 5.77 MFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.63 KFLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  82.18 K \n",
      "fwd MACs:                                                               901.82 KMACs\n",
      "fwd FLOPs:                                                              1.81 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.71 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.42 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  82.18 K = 100% Params, 901.82 KMACs = 100% MACs, 1.81 MFLOPS = 100% FLOPs\n",
      "  (0): Linear(73.86 K = 89.87% Params, 811.01 KMACs = 89.93% MACs, 1.62 MFLOPS = 89.82% FLOPs, in_features=576, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.41 KFLOPS = 0.08% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (3): Linear(8.26 K = 10.05% Params, 90.11 KMACs = 9.99% MACs, 180.22 KFLOPS = 9.98% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 704 FLOPS = 0.04% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.027338176153879887, inplace=False)\n",
      "  (6): Linear(65 = 0.08% Params, 704 MACs = 0.08% MACs, 1.41 KFLOPS = 0.08% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_43_seed4/best_model.pth\n",
      "   Seed 4: val_rmse=20.085722, test_loss=525.226771, test_rmse=22.935157, val_loss=407.856397\n",
      "\n",
      "Winner aggregated val_rmse: 20.217863 ± 0.429755\n",
      "Model params: total=569,185, trainable=569,185\n",
      "Saved multi-seed summary to: logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_43/winner_multi_seed_summary.txt\n",
      "{'winner_trial_number': 43, 'winner_trial_name': 'trial_43', 'primary_metric': 'val_rmse', 'aggregates': {'train_loss': {'mean': 320.45086760344327, 'std': 26.481488390054295}, 'val_loss': {'mean': 411.83090413411463, 'std': 18.130825326515232}, 'test_loss': {'mean': 550.28755086263, 'std': 32.199391274714586}, 'min_lr': {'mean': 2.630381338287905e-06, 'std': 0.0}, 'max_lr': {'mean': 0.0025304268474329646, 'std': 0.0}, 'total_time': {'mean': 29.200751495361327, 'std': 1.1175304279244436}, 'average_epoch_time': {'mean': 0.29106840229034425, 'std': 0.01109381295650245}, 'train_mse': {'mean': 320.28302001953125, 'std': 26.690065978582968}, 'train_mae': {'mean': 13.928546714782716, 'std': 0.7348437519319235}, 'train_rmse': {'mean': 17.884159577467365, 'std': 0.7414986743191855}, 'train_r2': {'mean': 0.9627535820007325, 'std': 0.0031038594840682907}, 'val_mse': {'mean': 408.90972290039065, 'std': 17.3202249441232}, 'val_mae': {'mean': 15.929244041442871, 'std': 0.3638061096502682}, 'val_rmse': {'mean': 20.217862679157626, 'std': 0.42975514411680393}, 'val_r2': {'mean': 0.9418120503425598, 'std': 0.002464666835514033}, 'test_mse': {'mean': 552.0057250976563, 'std': 32.16221695855503}, 'test_mae': {'mean': 18.287691116333008, 'std': 0.5166849985131367}, 'test_rmse': {'mean': 23.486866643218367, 'std': 0.6826605867236631}, 'test_r2': {'mean': 0.9357542037963867, 'std': 0.003743234363708163}, 'total_params': {'mean': 569185.0, 'std': 0.0}, 'trainable_params': {'mean': 569185.0, 'std': 0.0}, 'flops': {'mean': 1108445184.0, 'std': 0.0}, 'macs': {'mean': 550708928.0, 'std': 0.0}}, 'total_params': 569185, 'trainable_params': 569185, 'flops': 1108445184.0, 'macs': 550708928.0, 'summary_path': 'logs/Regression/Moneyball_cleaned/CNN_hybrid/Combination/best_model/trial_43/winner_multi_seed_summary.txt'}\n"
     ]
    }
   ],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     attributes=attributes,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "     path_vision=path_vision, path_mlp=path_mlp,\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: SuperTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"SuperTML\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes — Train: (862, 72), Val: (185, 72), Test: (185, 72)\n",
      "Numerical features: 8 — ['Year', 'RA', 'W', 'OBP', 'SLG', 'BA', 'OOBP', 'OSLG']\n",
      "Categorical features: 6 — ['Team', 'League', 'Playoffs', 'RankSeason', 'RankPlayoffs', 'G']\n",
      "Total features: 72\n",
      "Images shape (C,H,W): (3, 224, 224)\n",
      "Attributes: 72\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 7, 8, 14, 16, 28, 32, 56, 112, 224]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = [16, 28, 32, 56, 112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vision=f\"./logs/{task_type}/{dataset_name}/{vision_name}/{name}/best_model/trial_71\"\n",
    "path_mlp=f\"./logs/{task_type}/{dataset_name}/mlp/best_model/trial_38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-16 16:35:31,635] A new study created in memory with name: no-name-a5187c94-9164-4927-83f3-e1d364eddffe\n",
      "[I 2025-12-16 16:36:09,314] Trial 0 finished with value: 24.54771790007296 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.27557766707284714, 'max_lr': 0.0029811140756228166, 'div_factor': 65, 'final_div_factor': 159, 'weight_decay': 4.908968978839107e-06, 'pct_start': 0.308077718620802}. Best is trial 0 with value: 24.54771790007296.\n",
      "[I 2025-12-16 16:36:46,189] Trial 1 finished with value: 23.90646190674056 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.1981266007290174, 'max_lr': 0.00013264703039176558, 'div_factor': 163, 'final_div_factor': 699, 'weight_decay': 1.569961149841879e-05, 'pct_start': 0.12219358858880182}. Best is trial 1 with value: 23.90646190674056.\n",
      "[I 2025-12-16 16:37:22,522] Trial 2 finished with value: 25.526604630175797 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.11131598071207838, 'max_lr': 0.0007336440375824384, 'div_factor': 947, 'final_div_factor': 441, 'weight_decay': 6.25130180985406e-05, 'pct_start': 0.3348077422593799}. Best is trial 1 with value: 23.90646190674056.\n",
      "[I 2025-12-16 16:37:58,993] Trial 3 finished with value: 23.68258962338222 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.164038704287643, 'max_lr': 0.006913816888167701, 'div_factor': 218, 'final_div_factor': 909, 'weight_decay': 1.4503279278436909e-05, 'pct_start': 0.252557120355862}. Best is trial 3 with value: 23.68258962338222.\n",
      "[I 2025-12-16 16:38:35,987] Trial 4 finished with value: 23.630188387371273 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.07343373377288258, 'max_lr': 0.0010107613272094822, 'div_factor': 690, 'final_div_factor': 425, 'weight_decay': 1.2893158427637567e-05, 'pct_start': 0.3474672560204828}. Best is trial 4 with value: 23.630188387371273.\n",
      "[I 2025-12-16 16:39:12,615] Trial 5 finished with value: 656.0547804871176 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.04046838107442624, 'max_lr': 1.5921743326011514e-05, 'div_factor': 549, 'final_div_factor': 842, 'weight_decay': 4.507331504961008e-06, 'pct_start': 0.35591054952947776}. Best is trial 4 with value: 23.630188387371273.\n",
      "[I 2025-12-16 16:39:49,087] Trial 6 finished with value: 27.223384633407314 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.22422366134953103, 'max_lr': 9.355312970967169e-05, 'div_factor': 909, 'final_div_factor': 749, 'weight_decay': 0.007952810025249883, 'pct_start': 0.24365012072717854}. Best is trial 4 with value: 23.630188387371273.\n",
      "[I 2025-12-16 16:40:25,565] Trial 7 finished with value: 21.294214542876354 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.09880952189112159, 'max_lr': 0.0005783587996557906, 'div_factor': 142, 'final_div_factor': 305, 'weight_decay': 0.00320777798838355, 'pct_start': 0.11915171800342549}. Best is trial 7 with value: 21.294214542876354.\n",
      "[I 2025-12-16 16:41:01,745] Trial 8 finished with value: 27.398492078288953 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.2705490790695814, 'max_lr': 0.00013696584931889398, 'div_factor': 641, 'final_div_factor': 902, 'weight_decay': 3.7803708509155784e-06, 'pct_start': 0.29666656414456616}. Best is trial 7 with value: 21.294214542876354.\n",
      "[I 2025-12-16 16:41:38,229] Trial 9 finished with value: 661.5385948680546 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.25957912592678145, 'max_lr': 1.3715558583778099e-05, 'div_factor': 779, 'final_div_factor': 639, 'weight_decay': 9.19476031951352e-06, 'pct_start': 0.2766302939145508}. Best is trial 7 with value: 21.294214542876354.\n",
      "[I 2025-12-16 16:42:14,460] Trial 10 finished with value: 23.528601589840342 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.008773490311018484, 'max_lr': 0.0004507067528038877, 'div_factor': 393, 'final_div_factor': 118, 'weight_decay': 0.003678293287027182, 'pct_start': 0.11285173706530428}. Best is trial 7 with value: 21.294214542876354.\n",
      "[I 2025-12-16 16:42:50,863] Trial 11 finished with value: 24.650211653661568 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.005797308821896172, 'max_lr': 0.0005349697709076695, 'div_factor': 347, 'final_div_factor': 102, 'weight_decay': 0.003652212570229992, 'pct_start': 0.10670120866814126}. Best is trial 7 with value: 21.294214542876354.\n",
      "[I 2025-12-16 16:43:27,510] Trial 12 finished with value: 22.722442368879083 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.10388136707470606, 'max_lr': 0.001977717570670693, 'div_factor': 379, 'final_div_factor': 279, 'weight_decay': 0.0012890577049722903, 'pct_start': 0.17309480381770936}. Best is trial 7 with value: 21.294214542876354.\n",
      "[I 2025-12-16 16:44:04,790] Trial 13 finished with value: 23.871171414638017 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.11497806107062361, 'max_lr': 0.002231987607827577, 'div_factor': 358, 'final_div_factor': 302, 'weight_decay': 0.0005500988730668588, 'pct_start': 0.17999183250756823}. Best is trial 7 with value: 21.294214542876354.\n",
      "[I 2025-12-16 16:44:41,412] Trial 14 finished with value: 20.82430924349593 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.09327607833575707, 'max_lr': 0.0020543039529979123, 'div_factor': 218, 'final_div_factor': 247, 'weight_decay': 0.0006411136304186442, 'pct_start': 0.17882855769471326}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:45:18,062] Trial 15 finished with value: 22.237209801498466 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.16122536646397284, 'max_lr': 0.006150879887281976, 'div_factor': 19, 'final_div_factor': 282, 'weight_decay': 0.00031750358732192104, 'pct_start': 0.17094217416067425}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:45:54,783] Trial 16 finished with value: 25.82142584589092 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.06782902837440893, 'max_lr': 0.00020959912099010174, 'div_factor': 205, 'final_div_factor': 501, 'weight_decay': 0.00015284357059010771, 'pct_start': 0.21058321203424796}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:46:31,640] Trial 17 finished with value: 298.30861745849717 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.13375840125993588, 'max_lr': 4.936850236148973e-05, 'div_factor': 133, 'final_div_factor': 369, 'weight_decay': 0.0013908936601208259, 'pct_start': 0.14477181207697626}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:47:08,679] Trial 18 finished with value: 21.5196620724604 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.06491720128458492, 'max_lr': 0.001111823267640637, 'div_factor': 269, 'final_div_factor': 568, 'weight_decay': 8.414130724270634e-05, 'pct_start': 0.21112151678055385}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:47:45,695] Trial 19 finished with value: 27.30352621815198 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.1897221170054525, 'max_lr': 0.004319494235832741, 'div_factor': 445, 'final_div_factor': 228, 'weight_decay': 0.0007802365269104825, 'pct_start': 0.3886646137236267}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:48:22,354] Trial 20 finished with value: 27.58470508433693 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.09046674598552593, 'max_lr': 0.0002884482455604395, 'div_factor': 534, 'final_div_factor': 367, 'weight_decay': 0.003143189344994712, 'pct_start': 0.1443275078042731}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:48:59,150] Trial 21 finished with value: 21.390943860616503 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.04805621255413349, 'max_lr': 0.0011475721823692967, 'div_factor': 260, 'final_div_factor': 533, 'weight_decay': 5.9459689578291835e-05, 'pct_start': 0.21107945075557824}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:49:36,104] Trial 22 finished with value: 21.87408759257886 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.03635938861818165, 'max_lr': 0.0013811689810360147, 'div_factor': 299, 'final_div_factor': 540, 'weight_decay': 3.9241186283787975e-05, 'pct_start': 0.21214098345766183}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:50:12,844] Trial 23 finished with value: 21.62993799180425 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.04512940557501877, 'max_lr': 0.003378802971246279, 'div_factor': 110, 'final_div_factor': 214, 'weight_decay': 1.2596996528113189e-06, 'pct_start': 0.1418083025937009}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:50:49,841] Trial 24 finished with value: 22.953163557234465 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.13476889076545973, 'max_lr': 0.00940638717887667, 'div_factor': 242, 'final_div_factor': 378, 'weight_decay': 0.00021321450338055738, 'pct_start': 0.19144227447016388}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:51:26,408] Trial 25 finished with value: 22.510155992412276 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.08704538755097188, 'max_lr': 0.0005966002944832415, 'div_factor': 10, 'final_div_factor': 612, 'weight_decay': 3.668106222516577e-05, 'pct_start': 0.23882732774430873}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:52:03,330] Trial 26 finished with value: 21.53249724254017 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.023862682739929655, 'max_lr': 0.0016152482370798567, 'div_factor': 117, 'final_div_factor': 446, 'weight_decay': 0.00038018763349859964, 'pct_start': 0.1412467482067205}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:52:40,463] Trial 27 finished with value: 25.419245888173737 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.05775397955258363, 'max_lr': 0.0003436952083491164, 'div_factor': 469, 'final_div_factor': 182, 'weight_decay': 0.009011502493878287, 'pct_start': 0.159899526948795}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:53:17,641] Trial 28 finished with value: 22.666694790691803 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.13179679070494077, 'max_lr': 0.0007526646404944589, 'div_factor': 300, 'final_div_factor': 320, 'weight_decay': 0.0016138903551141314, 'pct_start': 0.20152849248858598}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:53:53,950] Trial 29 finished with value: 21.10077501654449 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.09211430401686244, 'max_lr': 0.003139874632467112, 'div_factor': 90, 'final_div_factor': 512, 'weight_decay': 0.00014405111247079954, 'pct_start': 0.2243822419567121}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:54:30,457] Trial 30 finished with value: 22.05085702212402 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.08708228660945172, 'max_lr': 0.0027248163747576833, 'div_factor': 63, 'final_div_factor': 229, 'weight_decay': 0.000758949670807519, 'pct_start': 0.2321032231923118}. Best is trial 14 with value: 20.82430924349593.\n",
      "[I 2025-12-16 16:55:07,093] Trial 31 finished with value: 20.598531489203758 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.10542356406752211, 'max_lr': 0.0038593839426685596, 'div_factor': 165, 'final_div_factor': 509, 'weight_decay': 0.00012504161601556435, 'pct_start': 0.26771610126421264}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 16:55:43,382] Trial 32 finished with value: 21.345840999560444 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.11284735271614128, 'max_lr': 0.0033491003944407785, 'div_factor': 78, 'final_div_factor': 689, 'weight_decay': 0.0001516780668884433, 'pct_start': 0.2620095657615628}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 16:56:19,597] Trial 33 finished with value: 21.143269420713416 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.14516937579337652, 'max_lr': 0.009854772084550387, 'div_factor': 166, 'final_div_factor': 495, 'weight_decay': 0.000228043375028, 'pct_start': 0.22613166879176455}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 16:56:55,872] Trial 34 finished with value: 21.2223186834694 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.18757332318578634, 'max_lr': 0.005824879868623341, 'div_factor': 183, 'final_div_factor': 448, 'weight_decay': 0.00025667942542765675, 'pct_start': 0.28907960812173394}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 16:57:32,300] Trial 35 finished with value: 23.53473061077766 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.16352282385245392, 'max_lr': 0.009205375930456762, 'div_factor': 175, 'final_div_factor': 760, 'weight_decay': 0.00010743380419404548, 'pct_start': 0.3186350906268765}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 16:58:09,037] Trial 36 finished with value: 23.93329505740251 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.12242341154260106, 'max_lr': 0.004561380906550471, 'div_factor': 56, 'final_div_factor': 505, 'weight_decay': 2.691477075504016e-05, 'pct_start': 0.2622600500283}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 16:58:45,306] Trial 37 finished with value: 21.196886892089026 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.15793063017813874, 'max_lr': 0.004700202479662143, 'div_factor': 170, 'final_div_factor': 601, 'weight_decay': 0.0004963763553382345, 'pct_start': 0.27165364767265077}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 16:59:21,638] Trial 38 finished with value: 23.933750267257924 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.1440577827743263, 'max_lr': 0.007743797581330369, 'div_factor': 227, 'final_div_factor': 687, 'weight_decay': 0.0001447031043566335, 'pct_start': 0.23343085464434277}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 16:59:58,620] Trial 39 finished with value: 22.58851877436681 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.2170614484771261, 'max_lr': 0.0016892084877248572, 'div_factor': 84, 'final_div_factor': 478, 'weight_decay': 6.83549708646503e-05, 'pct_start': 0.22294293203371462}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 17:00:34,947] Trial 40 finished with value: 22.742319005116183 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.07610584426138123, 'max_lr': 0.0025150812990883246, 'div_factor': 613, 'final_div_factor': 800, 'weight_decay': 0.00019062858368660096, 'pct_start': 0.3130846997990477}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 17:01:11,369] Trial 41 finished with value: 20.910694419681075 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.14852453154598794, 'max_lr': 0.004288147078886923, 'div_factor': 169, 'final_div_factor': 615, 'weight_decay': 0.0003109144573388821, 'pct_start': 0.27305725856828306}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 17:01:48,106] Trial 42 finished with value: 22.599997515382885 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.1765023414261566, 'max_lr': 0.003620296995123153, 'div_factor': 141, 'final_div_factor': 663, 'weight_decay': 0.00035185827280146575, 'pct_start': 0.2511890444547048}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 17:02:25,850] Trial 43 finished with value: 20.73561424800788 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.147594752160056, 'max_lr': 0.006105107205998571, 'div_factor': 300, 'final_div_factor': 984, 'weight_decay': 0.0007885282975659109, 'pct_start': 0.28094322682495443}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 17:03:03,227] Trial 44 finished with value: 20.94908582199576 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.09813137942793375, 'max_lr': 0.005470513405643858, 'div_factor': 295, 'final_div_factor': 974, 'weight_decay': 0.000719578464240059, 'pct_start': 0.29706518144942856}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 17:03:40,295] Trial 45 finished with value: 21.235626745206243 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.11841294114109664, 'max_lr': 0.004780956073124522, 'div_factor': 313, 'final_div_factor': 995, 'weight_decay': 0.0007710519657861113, 'pct_start': 0.3330725067312124}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 17:04:16,420] Trial 46 finished with value: 23.775410440233465 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.29338035997290646, 'max_lr': 0.00669036050272655, 'div_factor': 417, 'final_div_factor': 960, 'weight_decay': 0.001923512514298137, 'pct_start': 0.28723133596586475}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 17:04:52,756] Trial 47 finished with value: 21.590746614127745 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.10607141233134318, 'max_lr': 0.002404612759257192, 'div_factor': 347, 'final_div_factor': 885, 'weight_decay': 0.0005526100058641155, 'pct_start': 0.29956167006281775}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 17:05:29,560] Trial 48 finished with value: 24.064647073446377 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.2027958334552937, 'max_lr': 0.006238318476397035, 'div_factor': 857, 'final_div_factor': 818, 'weight_decay': 0.0010330728709340834, 'pct_start': 0.35809245584413346}. Best is trial 31 with value: 20.598531489203758.\n",
      "[I 2025-12-16 17:06:06,263] Trial 49 finished with value: 28.95421168865971 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.15011277186094035, 'max_lr': 4.0368361650212067e-05, 'div_factor': 222, 'final_div_factor': 735, 'weight_decay': 0.005572173445090961, 'pct_start': 0.27464273822957763}. Best is trial 31 with value: 20.598531489203758.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    attributes=attributes,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    epochs=epochs,\n",
    "    path_vision=path_vision,\n",
    "    path_mlp=path_mlp\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating top-5 trials once at 100 epochs (seed=0)...\n",
      "\n",
      "→ Single-pass full run (Trial 31, ValObjective: 20.5985)\n",
      "\n",
      "Best Trial: 31\n",
      "  Best Score: 20.5985\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.10542356406752211\n",
      "    max_lr: 0.0038593839426685596\n",
      "    div_factor: 165\n",
      "    final_div_factor: 509\n",
      "    weight_decay: 0.00012504161601556435\n",
      "    pct_start: 0.26771610126421264\n",
      "  Params: total=12,489,777  trainable=12,489,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  11.96 M \n",
      "fwd MACs:                                                               1.98 GMACs\n",
      "fwd FLOPs:                                                              3.97 GFLOPS\n",
      "fwd+bwd MACs:                                                           5.95 GMACs\n",
      "fwd+bwd FLOPs:                                                          11.92 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  11.96 M = 100% Params, 1.98 GMACs = 100% MACs, 3.97 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "    (net): Sequential(\n",
      "      2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "      (0): Conv2d(2.35 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.01 MFLOPS = 1.49% FLOPs, 3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.01% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (3): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    11.96 M = 99.98% Params, 1.95 GMACs = 98.51% MACs, 3.91 GFLOPS = 98.49% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      28.7 K = 0.24% Params, 89.11 MMACs = 4.49% MACs, 179.43 MFLOPS = 4.52% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.06% Params, 21.68 MMACs = 1.09% MACs, 43.35 MFLOPS = 1.09% FLOPs, 16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        864 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 5.12 MFLOPS = 0.13% FLOPs\n",
      "        (0): Conv2d(768 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 4.82 MFLOPS = 0.12% FLOPs, 16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      129.6 K = 1.08% Params, 101.15 MMACs = 5.1% MACs, 202.91 MFLOPS = 5.11% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 0.35% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.38 MFLOPS = 0.19% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      517.25 K = 4.32% Params, 101.15 MMACs = 5.1% MACs, 202.61 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 1.39% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.16% Params, 3.61 MMACs = 0.18% MACs, 7.3 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.15% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      2.07 M = 17.28% Params, 101.15 MMACs = 5.1% MACs, 202.46 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(663.55 K = 5.55% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        74.5 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.26 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(73.73 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 192, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 18.82 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  229.89 K\n",
      "fwd MACs:                                                               229.63 KMACs\n",
      "fwd FLOPs:                                                              459.52 KFLOPS\n",
      "fwd+bwd MACs:                                                           688.9 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.38 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  229.89 K = 100% Params, 229.63 KMACs = 100% MACs, 459.52 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(229.63 K = 99.89% Params, 229.38 KMACs = 99.89% MACs, 458.75 KFLOPS = 99.83% FLOPs, in_features=896, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.10542356406752211, inplace=False)\n",
      "  (3): Linear(257 = 0.11% Params, 256 MACs = 0.11% MACs, 512 FLOPS = 0.11% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_31/best_model.pth\n",
      "   val_rmse=21.577657\n",
      "   params: total=12,489,777, trainable=12,489,777\n",
      "→ Single-pass full run (Trial 43, ValObjective: 20.7356)\n",
      "\n",
      "Best Trial: 43\n",
      "  Best Score: 20.7356\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.147594752160056\n",
      "    max_lr: 0.006105107205998571\n",
      "    div_factor: 300\n",
      "    final_div_factor: 984\n",
      "    weight_decay: 0.0007885282975659109\n",
      "    pct_start: 0.28094322682495443\n",
      "  Params: total=12,489,777  trainable=12,489,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  11.96 M \n",
      "fwd MACs:                                                               1.98 GMACs\n",
      "fwd FLOPs:                                                              3.97 GFLOPS\n",
      "fwd+bwd MACs:                                                           5.95 GMACs\n",
      "fwd+bwd FLOPs:                                                          11.92 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  11.96 M = 100% Params, 1.98 GMACs = 100% MACs, 3.97 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "    (net): Sequential(\n",
      "      2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "      (0): Conv2d(2.35 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.01 MFLOPS = 1.49% FLOPs, 3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.01% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (3): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    11.96 M = 99.98% Params, 1.95 GMACs = 98.51% MACs, 3.91 GFLOPS = 98.49% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      28.7 K = 0.24% Params, 89.11 MMACs = 4.49% MACs, 179.43 MFLOPS = 4.52% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.06% Params, 21.68 MMACs = 1.09% MACs, 43.35 MFLOPS = 1.09% FLOPs, 16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        864 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 5.12 MFLOPS = 0.13% FLOPs\n",
      "        (0): Conv2d(768 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 4.82 MFLOPS = 0.12% FLOPs, 16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      129.6 K = 1.08% Params, 101.15 MMACs = 5.1% MACs, 202.91 MFLOPS = 5.11% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 0.35% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.38 MFLOPS = 0.19% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      517.25 K = 4.32% Params, 101.15 MMACs = 5.1% MACs, 202.61 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 1.39% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.16% Params, 3.61 MMACs = 0.18% MACs, 7.3 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.15% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      2.07 M = 17.28% Params, 101.15 MMACs = 5.1% MACs, 202.46 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(663.55 K = 5.55% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        74.5 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.26 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(73.73 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 192, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 18.82 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  229.89 K\n",
      "fwd MACs:                                                               229.63 KMACs\n",
      "fwd FLOPs:                                                              459.52 KFLOPS\n",
      "fwd+bwd MACs:                                                           688.9 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.38 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  229.89 K = 100% Params, 229.63 KMACs = 100% MACs, 459.52 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(229.63 K = 99.89% Params, 229.38 KMACs = 99.89% MACs, 458.75 KFLOPS = 99.83% FLOPs, in_features=896, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.147594752160056, inplace=False)\n",
      "  (3): Linear(257 = 0.11% Params, 256 MACs = 0.11% MACs, 512 FLOPS = 0.11% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_43/best_model.pth\n",
      "   val_rmse=20.537410\n",
      "   params: total=12,489,777, trainable=12,489,777\n",
      "→ Single-pass full run (Trial 14, ValObjective: 20.8243)\n",
      "\n",
      "Best Trial: 14\n",
      "  Best Score: 20.8243\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.09327607833575707\n",
      "    max_lr: 0.0020543039529979123\n",
      "    div_factor: 218\n",
      "    final_div_factor: 247\n",
      "    weight_decay: 0.0006411136304186442\n",
      "    pct_start: 0.17882855769471326\n",
      "  Params: total=12,489,777  trainable=12,489,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  11.96 M \n",
      "fwd MACs:                                                               1.98 GMACs\n",
      "fwd FLOPs:                                                              3.97 GFLOPS\n",
      "fwd+bwd MACs:                                                           5.95 GMACs\n",
      "fwd+bwd FLOPs:                                                          11.92 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  11.96 M = 100% Params, 1.98 GMACs = 100% MACs, 3.97 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "    (net): Sequential(\n",
      "      2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "      (0): Conv2d(2.35 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.01 MFLOPS = 1.49% FLOPs, 3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.01% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (3): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    11.96 M = 99.98% Params, 1.95 GMACs = 98.51% MACs, 3.91 GFLOPS = 98.49% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      28.7 K = 0.24% Params, 89.11 MMACs = 4.49% MACs, 179.43 MFLOPS = 4.52% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.06% Params, 21.68 MMACs = 1.09% MACs, 43.35 MFLOPS = 1.09% FLOPs, 16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        864 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 5.12 MFLOPS = 0.13% FLOPs\n",
      "        (0): Conv2d(768 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 4.82 MFLOPS = 0.12% FLOPs, 16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      129.6 K = 1.08% Params, 101.15 MMACs = 5.1% MACs, 202.91 MFLOPS = 5.11% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 0.35% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.38 MFLOPS = 0.19% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      517.25 K = 4.32% Params, 101.15 MMACs = 5.1% MACs, 202.61 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 1.39% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.16% Params, 3.61 MMACs = 0.18% MACs, 7.3 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.15% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      2.07 M = 17.28% Params, 101.15 MMACs = 5.1% MACs, 202.46 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(663.55 K = 5.55% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        74.5 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.26 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(73.73 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 192, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 18.82 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  229.89 K\n",
      "fwd MACs:                                                               229.63 KMACs\n",
      "fwd FLOPs:                                                              459.52 KFLOPS\n",
      "fwd+bwd MACs:                                                           688.9 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.38 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  229.89 K = 100% Params, 229.63 KMACs = 100% MACs, 459.52 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(229.63 K = 99.89% Params, 229.38 KMACs = 99.89% MACs, 458.75 KFLOPS = 99.83% FLOPs, in_features=896, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.09327607833575707, inplace=False)\n",
      "  (3): Linear(257 = 0.11% Params, 256 MACs = 0.11% MACs, 512 FLOPS = 0.11% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_14/best_model.pth\n",
      "   val_rmse=21.891271\n",
      "   params: total=12,489,777, trainable=12,489,777\n",
      "→ Single-pass full run (Trial 41, ValObjective: 20.9107)\n",
      "\n",
      "Best Trial: 41\n",
      "  Best Score: 20.9107\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.14852453154598794\n",
      "    max_lr: 0.004288147078886923\n",
      "    div_factor: 169\n",
      "    final_div_factor: 615\n",
      "    weight_decay: 0.0003109144573388821\n",
      "    pct_start: 0.27305725856828306\n",
      "  Params: total=12,489,777  trainable=12,489,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  11.96 M \n",
      "fwd MACs:                                                               1.98 GMACs\n",
      "fwd FLOPs:                                                              3.97 GFLOPS\n",
      "fwd+bwd MACs:                                                           5.95 GMACs\n",
      "fwd+bwd FLOPs:                                                          11.92 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  11.96 M = 100% Params, 1.98 GMACs = 100% MACs, 3.97 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "    (net): Sequential(\n",
      "      2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "      (0): Conv2d(2.35 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.01 MFLOPS = 1.49% FLOPs, 3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.01% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (3): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    11.96 M = 99.98% Params, 1.95 GMACs = 98.51% MACs, 3.91 GFLOPS = 98.49% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      28.7 K = 0.24% Params, 89.11 MMACs = 4.49% MACs, 179.43 MFLOPS = 4.52% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.06% Params, 21.68 MMACs = 1.09% MACs, 43.35 MFLOPS = 1.09% FLOPs, 16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        864 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 5.12 MFLOPS = 0.13% FLOPs\n",
      "        (0): Conv2d(768 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 4.82 MFLOPS = 0.12% FLOPs, 16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      129.6 K = 1.08% Params, 101.15 MMACs = 5.1% MACs, 202.91 MFLOPS = 5.11% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 0.35% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.38 MFLOPS = 0.19% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      517.25 K = 4.32% Params, 101.15 MMACs = 5.1% MACs, 202.61 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 1.39% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.16% Params, 3.61 MMACs = 0.18% MACs, 7.3 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.15% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      2.07 M = 17.28% Params, 101.15 MMACs = 5.1% MACs, 202.46 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(663.55 K = 5.55% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        74.5 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.26 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(73.73 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 192, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 18.82 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  229.89 K\n",
      "fwd MACs:                                                               229.63 KMACs\n",
      "fwd FLOPs:                                                              459.52 KFLOPS\n",
      "fwd+bwd MACs:                                                           688.9 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.38 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  229.89 K = 100% Params, 229.63 KMACs = 100% MACs, 459.52 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(229.63 K = 99.89% Params, 229.38 KMACs = 99.89% MACs, 458.75 KFLOPS = 99.83% FLOPs, in_features=896, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.14852453154598794, inplace=False)\n",
      "  (3): Linear(257 = 0.11% Params, 256 MACs = 0.11% MACs, 512 FLOPS = 0.11% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_41/best_model.pth\n",
      "   val_rmse=20.434926\n",
      "   params: total=12,489,777, trainable=12,489,777\n",
      "→ Single-pass full run (Trial 44, ValObjective: 20.9491)\n",
      "\n",
      "Best Trial: 44\n",
      "  Best Score: 20.9491\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128]\n",
      "    fusion_dropout: 0.09813137942793375\n",
      "    max_lr: 0.005470513405643858\n",
      "    div_factor: 295\n",
      "    final_div_factor: 974\n",
      "    weight_decay: 0.000719578464240059\n",
      "    pct_start: 0.29706518144942856\n",
      "  Params: total=12,374,833  trainable=12,374,833\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  11.96 M \n",
      "fwd MACs:                                                               1.98 GMACs\n",
      "fwd FLOPs:                                                              3.97 GFLOPS\n",
      "fwd+bwd MACs:                                                           5.95 GMACs\n",
      "fwd+bwd FLOPs:                                                          11.92 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  11.96 M = 100% Params, 1.98 GMACs = 100% MACs, 3.97 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "    (net): Sequential(\n",
      "      2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "      (0): Conv2d(2.35 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.01 MFLOPS = 1.49% FLOPs, 3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.01% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (3): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    11.96 M = 99.98% Params, 1.95 GMACs = 98.51% MACs, 3.91 GFLOPS = 98.49% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      28.7 K = 0.24% Params, 89.11 MMACs = 4.49% MACs, 179.43 MFLOPS = 4.52% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.06% Params, 21.68 MMACs = 1.09% MACs, 43.35 MFLOPS = 1.09% FLOPs, 16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        864 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 5.12 MFLOPS = 0.13% FLOPs\n",
      "        (0): Conv2d(768 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 4.82 MFLOPS = 0.12% FLOPs, 16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      129.6 K = 1.08% Params, 101.15 MMACs = 5.1% MACs, 202.91 MFLOPS = 5.11% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 0.35% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.38 MFLOPS = 0.19% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      517.25 K = 4.32% Params, 101.15 MMACs = 5.1% MACs, 202.61 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 1.39% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.16% Params, 3.61 MMACs = 0.18% MACs, 7.3 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.15% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      2.07 M = 17.28% Params, 101.15 MMACs = 5.1% MACs, 202.46 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(663.55 K = 5.55% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        74.5 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.26 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(73.73 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 192, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 18.82 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  114.94 K\n",
      "fwd MACs:                                                               114.82 KMACs\n",
      "fwd FLOPs:                                                              229.76 KFLOPS\n",
      "fwd+bwd MACs:                                                           344.45 KMACs\n",
      "fwd+bwd FLOPs:                                                          689.28 KFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  114.94 K = 100% Params, 114.82 KMACs = 100% MACs, 229.76 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(114.82 K = 99.89% Params, 114.69 KMACs = 99.89% MACs, 229.38 KFLOPS = 99.83% FLOPs, in_features=896, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 128 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.09813137942793375, inplace=False)\n",
      "  (3): Linear(129 = 0.11% Params, 128 MACs = 0.11% MACs, 256 FLOPS = 0.11% FLOPs, in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_44/best_model.pth\n",
      "   val_rmse=20.774605\n",
      "   params: total=12,374,833, trainable=12,374,833\n",
      "\n",
      "Winner after single-pass: Trial 41 (trial_41) by val_rmse=20.434926\n",
      "\n",
      "Re-running winner with seeds [0, 1, 2, 3, 4] at 100 epochs...\n",
      "\n",
      "\n",
      "Best Trial: 41\n",
      "  Best Score: 20.9107\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.14852453154598794\n",
      "    max_lr: 0.004288147078886923\n",
      "    div_factor: 169\n",
      "    final_div_factor: 615\n",
      "    weight_decay: 0.0003109144573388821\n",
      "    pct_start: 0.27305725856828306\n",
      "  Params: total=12,489,777  trainable=12,489,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  11.96 M \n",
      "fwd MACs:                                                               1.98 GMACs\n",
      "fwd FLOPs:                                                              3.97 GFLOPS\n",
      "fwd+bwd MACs:                                                           5.95 GMACs\n",
      "fwd+bwd FLOPs:                                                          11.92 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  11.96 M = 100% Params, 1.98 GMACs = 100% MACs, 3.97 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "    (net): Sequential(\n",
      "      2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "      (0): Conv2d(2.35 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.01 MFLOPS = 1.49% FLOPs, 3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.01% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (3): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    11.96 M = 99.98% Params, 1.95 GMACs = 98.51% MACs, 3.91 GFLOPS = 98.49% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      28.7 K = 0.24% Params, 89.11 MMACs = 4.49% MACs, 179.43 MFLOPS = 4.52% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.06% Params, 21.68 MMACs = 1.09% MACs, 43.35 MFLOPS = 1.09% FLOPs, 16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        864 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 5.12 MFLOPS = 0.13% FLOPs\n",
      "        (0): Conv2d(768 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 4.82 MFLOPS = 0.12% FLOPs, 16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      129.6 K = 1.08% Params, 101.15 MMACs = 5.1% MACs, 202.91 MFLOPS = 5.11% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 0.35% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.38 MFLOPS = 0.19% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      517.25 K = 4.32% Params, 101.15 MMACs = 5.1% MACs, 202.61 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 1.39% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.16% Params, 3.61 MMACs = 0.18% MACs, 7.3 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.15% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      2.07 M = 17.28% Params, 101.15 MMACs = 5.1% MACs, 202.46 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(663.55 K = 5.55% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        74.5 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.26 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(73.73 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 192, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 18.82 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  229.89 K\n",
      "fwd MACs:                                                               229.63 KMACs\n",
      "fwd FLOPs:                                                              459.52 KFLOPS\n",
      "fwd+bwd MACs:                                                           688.9 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.38 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  229.89 K = 100% Params, 229.63 KMACs = 100% MACs, 459.52 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(229.63 K = 99.89% Params, 229.38 KMACs = 99.89% MACs, 458.75 KFLOPS = 99.83% FLOPs, in_features=896, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.14852453154598794, inplace=False)\n",
      "  (3): Linear(257 = 0.11% Params, 256 MACs = 0.11% MACs, 512 FLOPS = 0.11% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_41_seed0/best_model.pth\n",
      "   Seed 0: val_rmse=20.434926, test_loss=613.996536, test_rmse=24.853058, val_loss=419.760015\n",
      "\n",
      "Best Trial: 41\n",
      "  Best Score: 20.9107\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.14852453154598794\n",
      "    max_lr: 0.004288147078886923\n",
      "    div_factor: 169\n",
      "    final_div_factor: 615\n",
      "    weight_decay: 0.0003109144573388821\n",
      "    pct_start: 0.27305725856828306\n",
      "  Params: total=12,489,777  trainable=12,489,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  11.96 M \n",
      "fwd MACs:                                                               1.98 GMACs\n",
      "fwd FLOPs:                                                              3.97 GFLOPS\n",
      "fwd+bwd MACs:                                                           5.95 GMACs\n",
      "fwd+bwd FLOPs:                                                          11.92 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  11.96 M = 100% Params, 1.98 GMACs = 100% MACs, 3.97 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "    (net): Sequential(\n",
      "      2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "      (0): Conv2d(2.35 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.01 MFLOPS = 1.49% FLOPs, 3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.01% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (3): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    11.96 M = 99.98% Params, 1.95 GMACs = 98.51% MACs, 3.91 GFLOPS = 98.49% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      28.7 K = 0.24% Params, 89.11 MMACs = 4.49% MACs, 179.43 MFLOPS = 4.52% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.06% Params, 21.68 MMACs = 1.09% MACs, 43.35 MFLOPS = 1.09% FLOPs, 16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        864 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 5.12 MFLOPS = 0.13% FLOPs\n",
      "        (0): Conv2d(768 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 4.82 MFLOPS = 0.12% FLOPs, 16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      129.6 K = 1.08% Params, 101.15 MMACs = 5.1% MACs, 202.91 MFLOPS = 5.11% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 0.35% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.38 MFLOPS = 0.19% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      517.25 K = 4.32% Params, 101.15 MMACs = 5.1% MACs, 202.61 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 1.39% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.16% Params, 3.61 MMACs = 0.18% MACs, 7.3 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.15% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      2.07 M = 17.28% Params, 101.15 MMACs = 5.1% MACs, 202.46 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(663.55 K = 5.55% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        74.5 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.26 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(73.73 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 192, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 18.82 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  229.89 K\n",
      "fwd MACs:                                                               229.63 KMACs\n",
      "fwd FLOPs:                                                              459.52 KFLOPS\n",
      "fwd+bwd MACs:                                                           688.9 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.38 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  229.89 K = 100% Params, 229.63 KMACs = 100% MACs, 459.52 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(229.63 K = 99.89% Params, 229.38 KMACs = 99.89% MACs, 458.75 KFLOPS = 99.83% FLOPs, in_features=896, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.14852453154598794, inplace=False)\n",
      "  (3): Linear(257 = 0.11% Params, 256 MACs = 0.11% MACs, 512 FLOPS = 0.11% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_41_seed1/best_model.pth\n",
      "   Seed 1: val_rmse=21.793530, test_loss=615.969610, test_rmse=24.911684, val_loss=475.455444\n",
      "\n",
      "Best Trial: 41\n",
      "  Best Score: 20.9107\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.14852453154598794\n",
      "    max_lr: 0.004288147078886923\n",
      "    div_factor: 169\n",
      "    final_div_factor: 615\n",
      "    weight_decay: 0.0003109144573388821\n",
      "    pct_start: 0.27305725856828306\n",
      "  Params: total=12,489,777  trainable=12,489,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  11.96 M \n",
      "fwd MACs:                                                               1.98 GMACs\n",
      "fwd FLOPs:                                                              3.97 GFLOPS\n",
      "fwd+bwd MACs:                                                           5.95 GMACs\n",
      "fwd+bwd FLOPs:                                                          11.92 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  11.96 M = 100% Params, 1.98 GMACs = 100% MACs, 3.97 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "    (net): Sequential(\n",
      "      2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "      (0): Conv2d(2.35 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.01 MFLOPS = 1.49% FLOPs, 3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.01% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (3): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    11.96 M = 99.98% Params, 1.95 GMACs = 98.51% MACs, 3.91 GFLOPS = 98.49% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      28.7 K = 0.24% Params, 89.11 MMACs = 4.49% MACs, 179.43 MFLOPS = 4.52% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.06% Params, 21.68 MMACs = 1.09% MACs, 43.35 MFLOPS = 1.09% FLOPs, 16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        864 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 5.12 MFLOPS = 0.13% FLOPs\n",
      "        (0): Conv2d(768 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 4.82 MFLOPS = 0.12% FLOPs, 16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      129.6 K = 1.08% Params, 101.15 MMACs = 5.1% MACs, 202.91 MFLOPS = 5.11% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 0.35% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.38 MFLOPS = 0.19% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      517.25 K = 4.32% Params, 101.15 MMACs = 5.1% MACs, 202.61 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 1.39% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.16% Params, 3.61 MMACs = 0.18% MACs, 7.3 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.15% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      2.07 M = 17.28% Params, 101.15 MMACs = 5.1% MACs, 202.46 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(663.55 K = 5.55% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        74.5 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.26 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(73.73 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 192, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 18.82 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  229.89 K\n",
      "fwd MACs:                                                               229.63 KMACs\n",
      "fwd FLOPs:                                                              459.52 KFLOPS\n",
      "fwd+bwd MACs:                                                           688.9 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.38 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  229.89 K = 100% Params, 229.63 KMACs = 100% MACs, 459.52 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(229.63 K = 99.89% Params, 229.38 KMACs = 99.89% MACs, 458.75 KFLOPS = 99.83% FLOPs, in_features=896, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.14852453154598794, inplace=False)\n",
      "  (3): Linear(257 = 0.11% Params, 256 MACs = 0.11% MACs, 512 FLOPS = 0.11% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_41_seed2/best_model.pth\n",
      "   Seed 2: val_rmse=20.820316, test_loss=581.525889, test_rmse=24.126342, val_loss=433.906550\n",
      "\n",
      "Best Trial: 41\n",
      "  Best Score: 20.9107\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.14852453154598794\n",
      "    max_lr: 0.004288147078886923\n",
      "    div_factor: 169\n",
      "    final_div_factor: 615\n",
      "    weight_decay: 0.0003109144573388821\n",
      "    pct_start: 0.27305725856828306\n",
      "  Params: total=12,489,777  trainable=12,489,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  11.96 M \n",
      "fwd MACs:                                                               1.98 GMACs\n",
      "fwd FLOPs:                                                              3.97 GFLOPS\n",
      "fwd+bwd MACs:                                                           5.95 GMACs\n",
      "fwd+bwd FLOPs:                                                          11.92 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  11.96 M = 100% Params, 1.98 GMACs = 100% MACs, 3.97 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "    (net): Sequential(\n",
      "      2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "      (0): Conv2d(2.35 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.01 MFLOPS = 1.49% FLOPs, 3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.01% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (3): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    11.96 M = 99.98% Params, 1.95 GMACs = 98.51% MACs, 3.91 GFLOPS = 98.49% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      28.7 K = 0.24% Params, 89.11 MMACs = 4.49% MACs, 179.43 MFLOPS = 4.52% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.06% Params, 21.68 MMACs = 1.09% MACs, 43.35 MFLOPS = 1.09% FLOPs, 16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        864 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 5.12 MFLOPS = 0.13% FLOPs\n",
      "        (0): Conv2d(768 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 4.82 MFLOPS = 0.12% FLOPs, 16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      129.6 K = 1.08% Params, 101.15 MMACs = 5.1% MACs, 202.91 MFLOPS = 5.11% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 0.35% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.38 MFLOPS = 0.19% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      517.25 K = 4.32% Params, 101.15 MMACs = 5.1% MACs, 202.61 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 1.39% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.16% Params, 3.61 MMACs = 0.18% MACs, 7.3 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.15% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      2.07 M = 17.28% Params, 101.15 MMACs = 5.1% MACs, 202.46 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(663.55 K = 5.55% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        74.5 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.26 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(73.73 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 192, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 18.82 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  229.89 K\n",
      "fwd MACs:                                                               229.63 KMACs\n",
      "fwd FLOPs:                                                              459.52 KFLOPS\n",
      "fwd+bwd MACs:                                                           688.9 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.38 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  229.89 K = 100% Params, 229.63 KMACs = 100% MACs, 459.52 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(229.63 K = 99.89% Params, 229.38 KMACs = 99.89% MACs, 458.75 KFLOPS = 99.83% FLOPs, in_features=896, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.14852453154598794, inplace=False)\n",
      "  (3): Linear(257 = 0.11% Params, 256 MACs = 0.11% MACs, 512 FLOPS = 0.11% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_41_seed3/best_model.pth\n",
      "   Seed 3: val_rmse=20.936576, test_loss=604.519577, test_rmse=24.742682, val_loss=438.895457\n",
      "\n",
      "Best Trial: 41\n",
      "  Best Score: 20.9107\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.14852453154598794\n",
      "    max_lr: 0.004288147078886923\n",
      "    div_factor: 169\n",
      "    final_div_factor: 615\n",
      "    weight_decay: 0.0003109144573388821\n",
      "    pct_start: 0.27305725856828306\n",
      "  Params: total=12,489,777  trainable=12,489,777\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  11.96 M \n",
      "fwd MACs:                                                               1.98 GMACs\n",
      "fwd FLOPs:                                                              3.97 GFLOPS\n",
      "fwd+bwd MACs:                                                           5.95 GMACs\n",
      "fwd+bwd FLOPs:                                                          11.92 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  11.96 M = 100% Params, 1.98 GMACs = 100% MACs, 3.97 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "    (net): Sequential(\n",
      "      2.38 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.81 MFLOPS = 1.51% FLOPs\n",
      "      (0): Conv2d(2.35 K = 0.02% Params, 29.5 MMACs = 1.49% MACs, 59.01 MFLOPS = 1.49% FLOPs, 3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.01% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (3): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.01% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    11.96 M = 99.98% Params, 1.95 GMACs = 98.51% MACs, 3.91 GFLOPS = 98.49% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      28.7 K = 0.24% Params, 89.11 MMACs = 4.49% MACs, 179.43 MFLOPS = 4.52% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.06% Params, 21.68 MMACs = 1.09% MACs, 43.35 MFLOPS = 1.09% FLOPs, 16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        864 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 5.12 MFLOPS = 0.13% FLOPs\n",
      "        (0): Conv2d(768 = 0.01% Params, 2.41 MMACs = 0.12% MACs, 4.82 MFLOPS = 0.12% FLOPs, 16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      41.66 K = 0.35% Params, 130.06 MMACs = 6.56% MACs, 261.02 MFLOPS = 6.57% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.17% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 301.06 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      129.6 K = 1.08% Params, 101.15 MMACs = 5.1% MACs, 202.91 MFLOPS = 5.11% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 0.35% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.38 MFLOPS = 0.19% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.04% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      166.27 K = 1.39% Params, 130.06 MMACs = 6.56% MACs, 260.56 MFLOPS = 6.56% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 0.69% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      517.25 K = 4.32% Params, 101.15 MMACs = 5.1% MACs, 202.61 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 1.39% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.16% Params, 3.61 MMACs = 0.18% MACs, 7.3 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.15% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      664.32 K = 5.55% Params, 130.06 MMACs = 6.56% MACs, 260.34 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 2.77% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      2.07 M = 17.28% Params, 101.15 MMACs = 5.1% MACs, 202.46 MFLOPS = 5.1% FLOPs\n",
      "      (conv1): Conv2d(663.55 K = 5.55% Params, 32.51 MMACs = 1.64% MACs, 65.03 MFLOPS = 1.64% FLOPs, 192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        74.5 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.26 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(73.73 K = 0.62% Params, 3.61 MMACs = 0.18% MACs, 7.23 MFLOPS = 0.18% FLOPs, 192, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      2.66 M = 22.21% Params, 130.06 MMACs = 6.56% MACs, 260.23 MFLOPS = 6.55% FLOPs\n",
      "      (conv1): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(1.33 M = 11.1% Params, 65.03 MMACs = 3.28% MACs, 130.06 MFLOPS = 3.27% FLOPs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(768 = 0.01% Params, 0 MACs = 0% MACs, 37.63 KFLOPS = 0% FLOPs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 18.82 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  229.89 K\n",
      "fwd MACs:                                                               229.63 KMACs\n",
      "fwd FLOPs:                                                              459.52 KFLOPS\n",
      "fwd+bwd MACs:                                                           688.9 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.38 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  229.89 K = 100% Params, 229.63 KMACs = 100% MACs, 459.52 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(229.63 K = 99.89% Params, 229.38 KMACs = 99.89% MACs, 458.75 KFLOPS = 99.83% FLOPs, in_features=896, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.14852453154598794, inplace=False)\n",
      "  (3): Linear(257 = 0.11% Params, 256 MACs = 0.11% MACs, 512 FLOPS = 0.11% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_41_seed4/best_model.pth\n",
      "   Seed 4: val_rmse=20.510373, test_loss=540.932281, test_rmse=23.283620, val_loss=423.215200\n",
      "\n",
      "Winner aggregated val_rmse: 20.899144 ± 0.541799\n",
      "Model params: total=12,489,777, trainable=12,489,777\n",
      "Saved multi-seed summary to: logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_41/winner_multi_seed_summary.txt\n",
      "{'winner_trial_number': 41, 'winner_trial_name': 'trial_41', 'primary_metric': 'val_rmse', 'aggregates': {'train_loss': {'mean': 247.72666812472872, 'std': 32.31709989552053}, 'val_loss': {'mean': 438.246533203125, 'std': 22.200517832473672}, 'test_loss': {'mean': 591.3887786865234, 'std': 31.348367242124507}, 'min_lr': {'mean': 2.5373651354360496e-05, 'std': 3.788046498485219e-21}, 'max_lr': {'mean': 0.004288147078886923, 'std': 0.0}, 'total_time': {'mean': 73.04389071464539, 'std': 1.381629609742851}, 'average_epoch_time': {'mean': 0.7279641842842102, 'std': 0.013600545725156522}, 'train_mse': {'mean': 247.7310302734375, 'std': 32.30243801796717}, 'train_mae': {'mean': 12.279661178588867, 'std': 0.8212418914883155}, 'train_rmse': {'mean': 15.712429988542798, 'std': 1.031124464224932}, 'train_r2': {'mean': 0.9711908102035522, 'std': 0.0037565198059004807}, 'val_mse': {'mean': 437.0090637207031, 'std': 22.903826891147496}, 'val_mae': {'mean': 16.48829574584961, 'std': 0.41083784097760656}, 'val_rmse': {'mean': 20.899144174054314, 'std': 0.5417985243551544}, 'val_r2': {'mean': 0.9378135204315186, 'std': 0.0032592116196292573}, 'test_mse': {'mean': 594.9348266601562, 'std': 33.26930878894494}, 'test_mae': {'mean': 19.258049774169923, 'std': 0.5485017647962899}, 'test_rmse': {'mean': 24.383477160607402, 'std': 0.6899893280819467}, 'test_r2': {'mean': 0.9307578563690185, 'std': 0.0038720863913908306}, 'total_params': {'mean': 12489777.0, 'std': 0.0}, 'trainable_params': {'mean': 12489777.0, 'std': 0.0}, 'flops': {'mean': 3974163584.0, 'std': 0.0}, 'macs': {'mean': 1983283456.0, 'std': 0.0}}, 'total_params': 12489777, 'trainable_params': 12489777, 'flops': 3974163584.0, 'macs': 1983283456.0, 'summary_path': 'logs/Regression/Moneyball_cleaned/CNN_hybrid/SuperTML/best_model/trial_41/winner_multi_seed_summary.txt'}\n"
     ]
    }
   ],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     attributes=attributes,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "     path_vision=path_vision, path_mlp=path_mlp,\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: FeatureWrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"FeatureWrap\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes — Train: (862, 72), Val: (185, 72), Test: (185, 72)\n",
      "Numerical features: 8 — ['Year', 'RA', 'W', 'OBP', 'SLG', 'BA', 'OOBP', 'OSLG']\n",
      "Categorical features: 6 — ['Team', 'League', 'Playoffs', 'RankSeason', 'RankPlayoffs', 'G']\n",
      "Total features: 72\n",
      "Images shape (C,H,W): (3, 10, 10)\n",
      "Attributes: 72\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 5, 10]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vision=f\"./logs/{task_type}/{dataset_name}/{vision_name}/{name}/best_model/trial_58\"\n",
    "path_mlp=f\"./logs/{task_type}/{dataset_name}/mlp/best_model/trial_38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-16 17:18:36,074] A new study created in memory with name: no-name-9273b031-6f61-4f7a-9242-49da27ff044c\n",
      "[I 2025-12-16 17:18:50,016] Trial 0 finished with value: 23.983639544912954 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.11070268565962542, 'max_lr': 0.00034370255985647605, 'div_factor': 544, 'final_div_factor': 627, 'weight_decay': 2.3765414762184266e-06, 'pct_start': 0.27128468879789597}. Best is trial 0 with value: 23.983639544912954.\n",
      "[I 2025-12-16 17:19:04,136] Trial 1 finished with value: 675.2891278926382 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.11987182810733296, 'max_lr': 1.1127345453114277e-05, 'div_factor': 678, 'final_div_factor': 272, 'weight_decay': 0.003212128155429053, 'pct_start': 0.16071755081564296}. Best is trial 0 with value: 23.983639544912954.\n",
      "[I 2025-12-16 17:19:17,430] Trial 2 finished with value: 24.541278553232384 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.18049188215880088, 'max_lr': 0.0005273777436872174, 'div_factor': 108, 'final_div_factor': 430, 'weight_decay': 2.6664614441031645e-06, 'pct_start': 0.31273176244939727}. Best is trial 0 with value: 23.983639544912954.\n",
      "[I 2025-12-16 17:19:31,091] Trial 3 finished with value: 619.2955675604339 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.2496564725397449, 'max_lr': 1.8741156226280223e-05, 'div_factor': 866, 'final_div_factor': 107, 'weight_decay': 4.9573227044681234e-05, 'pct_start': 0.16300918106847556}. Best is trial 0 with value: 23.983639544912954.\n",
      "[I 2025-12-16 17:19:44,666] Trial 4 finished with value: 22.089919388133104 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.006567415485883288, 'max_lr': 0.001872213563975202, 'div_factor': 636, 'final_div_factor': 573, 'weight_decay': 0.00014798379214309265, 'pct_start': 0.2771382603280397}. Best is trial 4 with value: 22.089919388133104.\n",
      "[I 2025-12-16 17:19:58,035] Trial 5 finished with value: 23.07384544782252 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.1429871996564027, 'max_lr': 0.0013519631175914724, 'div_factor': 469, 'final_div_factor': 413, 'weight_decay': 0.00017338147845613705, 'pct_start': 0.3274073267575459}. Best is trial 4 with value: 22.089919388133104.\n",
      "[I 2025-12-16 17:20:11,553] Trial 6 finished with value: 27.3362634840523 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.20555026000446416, 'max_lr': 0.0007922594124713813, 'div_factor': 865, 'final_div_factor': 225, 'weight_decay': 0.00011330871875093843, 'pct_start': 0.12531729477592188}. Best is trial 4 with value: 22.089919388133104.\n",
      "[I 2025-12-16 17:20:25,165] Trial 7 finished with value: 23.2986158491933 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.02038668976564669, 'max_lr': 0.0005218877775889104, 'div_factor': 553, 'final_div_factor': 881, 'weight_decay': 2.0877758694398813e-05, 'pct_start': 0.2828352050365134}. Best is trial 4 with value: 22.089919388133104.\n",
      "[I 2025-12-16 17:20:38,776] Trial 8 finished with value: 22.12140173666492 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.05382350734827028, 'max_lr': 0.001879333219069215, 'div_factor': 94, 'final_div_factor': 751, 'weight_decay': 0.00403973123372512, 'pct_start': 0.2270001395410328}. Best is trial 4 with value: 22.089919388133104.\n",
      "[I 2025-12-16 17:20:52,161] Trial 9 finished with value: 20.914614795892057 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.24146975086778671, 'max_lr': 0.006823695741840761, 'div_factor': 292, 'final_div_factor': 175, 'weight_decay': 9.564518775984315e-06, 'pct_start': 0.14698722724143942}. Best is trial 9 with value: 20.914614795892057.\n",
      "[I 2025-12-16 17:21:05,992] Trial 10 finished with value: 26.57378207095647 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.2898372399073981, 'max_lr': 0.00010277264392593953, 'div_factor': 294, 'final_div_factor': 962, 'weight_decay': 1.1582018874028361e-05, 'pct_start': 0.38607248072913597}. Best is trial 9 with value: 20.914614795892057.\n",
      "[I 2025-12-16 17:21:19,830] Trial 11 finished with value: 20.748902808906244 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.001955269809990917, 'max_lr': 0.007447532173072955, 'div_factor': 304, 'final_div_factor': 585, 'weight_decay': 0.000521013207818956, 'pct_start': 0.2272947214860106}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:21:34,373] Trial 12 finished with value: 24.637535970769022 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.07271429782607218, 'max_lr': 0.00984592683330648, 'div_factor': 305, 'final_div_factor': 431, 'weight_decay': 0.000855361955323772, 'pct_start': 0.20933051526798072}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:21:48,108] Trial 13 finished with value: 26.425757864457502 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.20914108760334915, 'max_lr': 0.008583182304513187, 'div_factor': 318, 'final_div_factor': 709, 'weight_decay': 0.0007837308189025781, 'pct_start': 0.10134647274263903}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:22:01,407] Trial 14 finished with value: 25.34163422695878 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.29075919447196197, 'max_lr': 0.004390791217512557, 'div_factor': 205, 'final_div_factor': 328, 'weight_decay': 8.21316666503786e-06, 'pct_start': 0.18964385615896678}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:22:14,982] Trial 15 finished with value: 26.622876006462352 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.24845905997902962, 'max_lr': 7.652726353993626e-05, 'div_factor': 12, 'final_div_factor': 125, 'weight_decay': 0.0005640491324279407, 'pct_start': 0.1628541838216983}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:22:28,248] Trial 16 finished with value: 22.150414649406116 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.06432858072440956, 'max_lr': 0.0039716200702343855, 'div_factor': 440, 'final_div_factor': 521, 'weight_decay': 1.3900291678264802e-06, 'pct_start': 0.2239175800467768}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:22:41,923] Trial 17 finished with value: 27.55915822641246 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.18064133381306877, 'max_lr': 0.0041499189229714005, 'div_factor': 391, 'final_div_factor': 771, 'weight_decay': 4.676824613982487e-05, 'pct_start': 0.1288714651232601}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:22:55,531] Trial 18 finished with value: 23.602636985223707 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.2446757718307631, 'max_lr': 0.0001689821380081384, 'div_factor': 191, 'final_div_factor': 647, 'weight_decay': 0.0003330017767870863, 'pct_start': 0.24882667504260697}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:23:09,270] Trial 19 finished with value: 23.562790117577332 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.09645221782415, 'max_lr': 0.00511014272100918, 'div_factor': 970, 'final_div_factor': 527, 'weight_decay': 8.715094512594877e-06, 'pct_start': 0.18995972838447367}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:23:22,896] Trial 20 finished with value: 23.442154485746265 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.03417562905520726, 'max_lr': 0.001205608622543105, 'div_factor': 217, 'final_div_factor': 844, 'weight_decay': 0.001983806255645716, 'pct_start': 0.3645615313316969}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:23:36,468] Trial 21 finished with value: 22.13429056571853 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.013505978113061763, 'max_lr': 0.002378548436996013, 'div_factor': 652, 'final_div_factor': 594, 'weight_decay': 0.009676860894890317, 'pct_start': 0.29250683199083083}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:23:50,110] Trial 22 finished with value: 21.340571288279897 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.0022287076560533747, 'max_lr': 0.0075257346482438085, 'div_factor': 700, 'final_div_factor': 511, 'weight_decay': 0.00021933972051564328, 'pct_start': 0.2567624329068728}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:24:03,800] Trial 23 finished with value: 22.869681652073506 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.038655580043232864, 'max_lr': 0.00952241669596326, 'div_factor': 721, 'final_div_factor': 349, 'weight_decay': 4.580658043084927e-05, 'pct_start': 0.24651461344572764}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:24:17,765] Trial 24 finished with value: 22.15129294313843 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.08347713302975275, 'max_lr': 0.003125557629838416, 'div_factor': 783, 'final_div_factor': 214, 'weight_decay': 0.00039804986649037466, 'pct_start': 0.19025050701112298}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:24:31,389] Trial 25 finished with value: 21.195380171749694 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.14690786976880027, 'max_lr': 0.007385252129619658, 'div_factor': 378, 'final_div_factor': 506, 'weight_decay': 0.0014935189628965293, 'pct_start': 0.3267133409268835}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:24:45,039] Trial 26 finished with value: 21.436046495458736 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.14132725786364467, 'max_lr': 0.0060565786350595925, 'div_factor': 408, 'final_div_factor': 681, 'weight_decay': 0.0017287572131533119, 'pct_start': 0.3411099361727539}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:24:58,587] Trial 27 finished with value: 22.18567196474661 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.16416371259634432, 'max_lr': 0.002680732423475709, 'div_factor': 346, 'final_div_factor': 468, 'weight_decay': 0.007408075275044026, 'pct_start': 0.36103867256430705}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:25:12,067] Trial 28 finished with value: 27.831066046860073 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.2215317342873384, 'max_lr': 3.941366768802335e-05, 'div_factor': 510, 'final_div_factor': 357, 'weight_decay': 0.0010026073734033896, 'pct_start': 0.1436197385929382}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:25:25,409] Trial 29 finished with value: 24.272762351811135 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.12693683370568346, 'max_lr': 0.00023580030406625545, 'div_factor': 585, 'final_div_factor': 625, 'weight_decay': 0.0016015091256921755, 'pct_start': 0.30186600842948885}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:25:39,178] Trial 30 finished with value: 23.063076932146867 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.0980660722216607, 'max_lr': 0.0010814317707251405, 'div_factor': 247, 'final_div_factor': 214, 'weight_decay': 3.688038703891658e-06, 'pct_start': 0.39105224912720615}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:25:52,641] Trial 31 finished with value: 23.459401225508188 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.2708202204278315, 'max_lr': 0.006007137520523029, 'div_factor': 148, 'final_div_factor': 531, 'weight_decay': 0.00018630246231222564, 'pct_start': 0.2564686183681761}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:26:05,971] Trial 32 finished with value: 20.94244563208257 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.03861898902985868, 'max_lr': 0.006926376038192465, 'div_factor': 380, 'final_div_factor': 483, 'weight_decay': 0.00039058997934487884, 'pct_start': 0.26143022140544503}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:26:19,277] Trial 33 finished with value: 20.88234997032669 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.03937595740899432, 'max_lr': 0.006156923416562257, 'div_factor': 357, 'final_div_factor': 285, 'weight_decay': 0.003626638505966491, 'pct_start': 0.3211459879490864}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:26:32,368] Trial 34 finished with value: 22.654176898687606 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.04302451543573969, 'max_lr': 0.003185645457495815, 'div_factor': 277, 'final_div_factor': 290, 'weight_decay': 0.004130108929019722, 'pct_start': 0.2294944072894441}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:26:45,795] Trial 35 finished with value: 23.384704859944645 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.05670948362372881, 'max_lr': 0.0019325466188636095, 'div_factor': 489, 'final_div_factor': 173, 'weight_decay': 8.276993613434979e-05, 'pct_start': 0.3083883564899596}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:26:59,083] Trial 36 finished with value: 21.714593239807936 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.023886335344548884, 'max_lr': 0.005233308046066749, 'div_factor': 427, 'final_div_factor': 262, 'weight_decay': 2.4346201212905807e-05, 'pct_start': 0.27292532287346377}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:27:12,555] Trial 37 finished with value: 483.4370111503669 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.11520474105358547, 'max_lr': 1.4466236451927376e-05, 'div_factor': 338, 'final_div_factor': 389, 'weight_decay': 0.00035096558146236633, 'pct_start': 0.2077531478914493}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:27:25,742] Trial 38 finished with value: 23.631175046578434 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.0011078506581012745, 'max_lr': 0.0004968485281596576, 'div_factor': 92, 'final_div_factor': 171, 'weight_decay': 7.962497612648404e-05, 'pct_start': 0.1023732627175298}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:27:39,228] Trial 39 finished with value: 23.43628642691446 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.028731735099999493, 'max_lr': 0.00158891104219993, 'div_factor': 156, 'final_div_factor': 284, 'weight_decay': 0.0028413456722982122, 'pct_start': 0.17884089097093087}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:27:52,443] Trial 40 finished with value: 23.43329519573525 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.0889993236639552, 'max_lr': 0.000830058578463103, 'div_factor': 593, 'final_div_factor': 439, 'weight_decay': 0.005781145848450108, 'pct_start': 0.3238641890709353}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:28:05,791] Trial 41 finished with value: 21.84561572750175 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.18585053020109216, 'max_lr': 0.007196441574357353, 'div_factor': 369, 'final_div_factor': 574, 'weight_decay': 0.0012668924552649752, 'pct_start': 0.34227003437939735}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:28:19,411] Trial 42 finished with value: 22.63867179074821 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.07677234824594867, 'max_lr': 0.0034707435785263532, 'div_factor': 275, 'final_div_factor': 490, 'weight_decay': 0.0027099712243339826, 'pct_start': 0.29559183075169093}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:28:32,996] Trial 43 finished with value: 21.11611155578432 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.01644382390718173, 'max_lr': 0.006659682699055841, 'div_factor': 470, 'final_div_factor': 398, 'weight_decay': 0.000660771691360496, 'pct_start': 0.3227879315462188}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:28:46,314] Trial 44 finished with value: 20.940083358584157 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.01609831048035666, 'max_lr': 0.009806726286509299, 'div_factor': 457, 'final_div_factor': 381, 'weight_decay': 0.000456320235146774, 'pct_start': 0.2843720954621821}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:28:59,657] Trial 45 finished with value: 21.602261799259587 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.04953772335072225, 'max_lr': 0.00951674046459069, 'div_factor': 528, 'final_div_factor': 319, 'weight_decay': 0.00012270999898853058, 'pct_start': 0.2835340488469026}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:29:13,146] Trial 46 finished with value: 22.0749483516171 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.06865620118727574, 'max_lr': 0.004815045732202797, 'div_factor': 244, 'final_div_factor': 129, 'weight_decay': 0.0002541900374787309, 'pct_start': 0.26957605431736736}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:29:27,275] Trial 47 finished with value: 22.21534344490312 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.01562470176257369, 'max_lr': 0.0026746862552548497, 'div_factor': 322, 'final_div_factor': 256, 'weight_decay': 0.000443826932008393, 'pct_start': 0.14531846756910244}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:29:41,131] Trial 48 finished with value: 21.971727410923382 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.03175644616906701, 'max_lr': 0.004041116150959987, 'div_factor': 451, 'final_div_factor': 454, 'weight_decay': 4.33056556216116e-06, 'pct_start': 0.22937016971310778}. Best is trial 11 with value: 20.748902808906244.\n",
      "[I 2025-12-16 17:29:55,119] Trial 49 finished with value: 22.46621269574881 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.05773904442275725, 'max_lr': 0.002222933316041453, 'div_factor': 411, 'final_div_factor': 180, 'weight_decay': 2.3664187008748215e-05, 'pct_start': 0.2672088424302619}. Best is trial 11 with value: 20.748902808906244.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    attributes=attributes,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    epochs=epochs,\n",
    "    path_vision=path_vision,\n",
    "    path_mlp=path_mlp\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating top-5 trials once at 100 epochs (seed=0)...\n",
      "\n",
      "→ Single-pass full run (Trial 11, ValObjective: 20.7489)\n",
      "\n",
      "Best Trial: 11\n",
      "  Best Score: 20.7489\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.001955269809990917\n",
      "    max_lr: 0.007447532173072955\n",
      "    div_factor: 304\n",
      "    final_div_factor: 585\n",
      "    weight_decay: 0.000521013207818956\n",
      "    pct_start: 0.2272947214860106\n",
      "  Params: total=3,941,145  trainable=3,941,145\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3.53 M  \n",
      "fwd MACs:                                                               38.13 MMACs\n",
      "fwd FLOPs:                                                              76.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           114.38 MMACs\n",
      "fwd+bwd FLOPs:                                                          229.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3.53 M = 100% Params, 38.13 MMACs = 100% MACs, 76.39 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "    (net): Sequential(\n",
      "      464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "      (0): Conv2d(432 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 86.4 KFLOPS = 0.11% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 3.2 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3.53 M = 99.99% Params, 38.08 MMACs = 99.89% MACs, 76.29 MFLOPS = 99.88% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      17.64 K = 0.5% Params, 1.74 MMACs = 4.57% MACs, 3.51 MFLOPS = 4.6% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.15% Params, 518.4 KMACs = 1.36% MACs, 1.04 MFLOPS = 1.36% FLOPs, 16, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        648 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 122.4 KFLOPS = 0.16% FLOPs\n",
      "        (0): Conv2d(576 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 115.2 KFLOPS = 0.15% FLOPs, 16, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      23.47 K = 0.66% Params, 2.33 MMACs = 6.12% MACs, 4.69 MFLOPS = 6.14% FLOPs\n",
      "      (conv1): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      73.01 K = 2.07% Params, 1.81 MMACs = 4.76% MACs, 3.64 MFLOPS = 4.77% FLOPs\n",
      "      (conv1): Conv2d(23.33 K = 0.66% Params, 583.2 KMACs = 1.53% MACs, 1.17 MFLOPS = 1.53% FLOPs, 36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.74 K = 0.08% Params, 64.8 KMACs = 0.17% MACs, 133.2 KFLOPS = 0.17% FLOPs\n",
      "        (0): Conv2d(2.59 K = 0.07% Params, 64.8 KMACs = 0.17% MACs, 129.6 KFLOPS = 0.17% FLOPs, 36, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      93.6 K = 2.65% Params, 2.33 MMACs = 6.12% MACs, 4.68 MFLOPS = 6.12% FLOPs\n",
      "      (conv1): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      291.17 K = 8.25% Params, 2.61 MMACs = 6.85% MACs, 5.24 MFLOPS = 6.85% FLOPs\n",
      "      (conv1): Conv2d(93.31 K = 2.64% Params, 839.81 KMACs = 2.2% MACs, 1.68 MFLOPS = 2.2% FLOPs, 72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        10.66 K = 0.3% Params, 93.31 KMACs = 0.24% MACs, 189.22 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(10.37 K = 0.29% Params, 93.31 KMACs = 0.24% MACs, 186.62 KFLOPS = 0.24% FLOPs, 72, 144, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      373.82 K = 10.59% Params, 3.36 MMACs = 8.81% MACs, 6.73 MFLOPS = 8.81% FLOPs\n",
      "      (conv1): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      1.16 M = 32.94% Params, 10.45 MMACs = 27.41% MACs, 20.92 MFLOPS = 27.39% FLOPs\n",
      "      (conv1): Conv2d(373.25 K = 10.57% Params, 3.36 MMACs = 8.81% MACs, 6.72 MFLOPS = 8.8% FLOPs, 144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        42.05 K = 1.19% Params, 373.25 KMACs = 0.98% MACs, 751.68 KFLOPS = 0.98% FLOPs\n",
      "        (0): Conv2d(41.47 K = 1.17% Params, 373.25 KMACs = 0.98% MACs, 746.5 KFLOPS = 0.98% FLOPs, 144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      1.49 M = 42.32% Params, 13.44 MMACs = 35.24% MACs, 26.89 MFLOPS = 35.2% FLOPs\n",
      "      (conv1): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  110.85 K\n",
      "fwd MACs:                                                               110.66 KMACs\n",
      "fwd FLOPs:                                                              221.5 KFLOPS\n",
      "fwd+bwd MACs:                                                           331.97 KMACs\n",
      "fwd+bwd FLOPs:                                                          664.51 KFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  110.85 K = 100% Params, 110.66 KMACs = 100% MACs, 221.5 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(102.53 K = 92.49% Params, 102.4 KMACs = 92.54% MACs, 204.8 KFLOPS = 92.46% FLOPs, in_features=800, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 128 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (3): Linear(8.26 K = 7.45% Params, 8.19 KMACs = 7.4% MACs, 16.38 KFLOPS = 7.4% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 64 FLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (6): Linear(65 = 0.06% Params, 64 MACs = 0.06% MACs, 128 FLOPS = 0.06% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_11/best_model.pth\n",
      "   val_rmse=20.522283\n",
      "   params: total=3,941,145, trainable=3,941,145\n",
      "→ Single-pass full run (Trial 33, ValObjective: 20.8823)\n",
      "\n",
      "Best Trial: 33\n",
      "  Best Score: 20.8823\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.03937595740899432\n",
      "    max_lr: 0.006156923416562257\n",
      "    div_factor: 357\n",
      "    final_div_factor: 285\n",
      "    weight_decay: 0.003626638505966491\n",
      "    pct_start: 0.3211459879490864\n",
      "  Params: total=4,035,609  trainable=4,035,609\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3.53 M  \n",
      "fwd MACs:                                                               38.13 MMACs\n",
      "fwd FLOPs:                                                              76.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           114.38 MMACs\n",
      "fwd+bwd FLOPs:                                                          229.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3.53 M = 100% Params, 38.13 MMACs = 100% MACs, 76.39 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "    (net): Sequential(\n",
      "      464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "      (0): Conv2d(432 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 86.4 KFLOPS = 0.11% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 3.2 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3.53 M = 99.99% Params, 38.08 MMACs = 99.89% MACs, 76.29 MFLOPS = 99.88% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      17.64 K = 0.5% Params, 1.74 MMACs = 4.57% MACs, 3.51 MFLOPS = 4.6% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.15% Params, 518.4 KMACs = 1.36% MACs, 1.04 MFLOPS = 1.36% FLOPs, 16, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        648 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 122.4 KFLOPS = 0.16% FLOPs\n",
      "        (0): Conv2d(576 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 115.2 KFLOPS = 0.15% FLOPs, 16, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      23.47 K = 0.66% Params, 2.33 MMACs = 6.12% MACs, 4.69 MFLOPS = 6.14% FLOPs\n",
      "      (conv1): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      73.01 K = 2.07% Params, 1.81 MMACs = 4.76% MACs, 3.64 MFLOPS = 4.77% FLOPs\n",
      "      (conv1): Conv2d(23.33 K = 0.66% Params, 583.2 KMACs = 1.53% MACs, 1.17 MFLOPS = 1.53% FLOPs, 36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.74 K = 0.08% Params, 64.8 KMACs = 0.17% MACs, 133.2 KFLOPS = 0.17% FLOPs\n",
      "        (0): Conv2d(2.59 K = 0.07% Params, 64.8 KMACs = 0.17% MACs, 129.6 KFLOPS = 0.17% FLOPs, 36, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      93.6 K = 2.65% Params, 2.33 MMACs = 6.12% MACs, 4.68 MFLOPS = 6.12% FLOPs\n",
      "      (conv1): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      291.17 K = 8.25% Params, 2.61 MMACs = 6.85% MACs, 5.24 MFLOPS = 6.85% FLOPs\n",
      "      (conv1): Conv2d(93.31 K = 2.64% Params, 839.81 KMACs = 2.2% MACs, 1.68 MFLOPS = 2.2% FLOPs, 72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        10.66 K = 0.3% Params, 93.31 KMACs = 0.24% MACs, 189.22 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(10.37 K = 0.29% Params, 93.31 KMACs = 0.24% MACs, 186.62 KFLOPS = 0.24% FLOPs, 72, 144, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      373.82 K = 10.59% Params, 3.36 MMACs = 8.81% MACs, 6.73 MFLOPS = 8.81% FLOPs\n",
      "      (conv1): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      1.16 M = 32.94% Params, 10.45 MMACs = 27.41% MACs, 20.92 MFLOPS = 27.39% FLOPs\n",
      "      (conv1): Conv2d(373.25 K = 10.57% Params, 3.36 MMACs = 8.81% MACs, 6.72 MFLOPS = 8.8% FLOPs, 144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        42.05 K = 1.19% Params, 373.25 KMACs = 0.98% MACs, 751.68 KFLOPS = 0.98% FLOPs\n",
      "        (0): Conv2d(41.47 K = 1.17% Params, 373.25 KMACs = 0.98% MACs, 746.5 KFLOPS = 0.98% FLOPs, 144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      1.49 M = 42.32% Params, 13.44 MMACs = 35.24% MACs, 26.89 MFLOPS = 35.2% FLOPs\n",
      "      (conv1): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  205.31 K\n",
      "fwd MACs:                                                               205.06 KMACs\n",
      "fwd FLOPs:                                                              410.37 KFLOPS\n",
      "fwd+bwd MACs:                                                           615.17 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.23 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  205.31 K = 100% Params, 205.06 KMACs = 100% MACs, 410.37 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(205.06 K = 99.87% Params, 204.8 KMACs = 99.88% MACs, 409.6 KFLOPS = 99.81% FLOPs, in_features=800, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.03937595740899432, inplace=False)\n",
      "  (3): Linear(257 = 0.13% Params, 256 MACs = 0.12% MACs, 512 FLOPS = 0.12% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_33/best_model.pth\n",
      "   val_rmse=21.314491\n",
      "   params: total=4,035,609, trainable=4,035,609\n",
      "→ Single-pass full run (Trial 9, ValObjective: 20.9146)\n",
      "\n",
      "Best Trial: 9\n",
      "  Best Score: 20.9146\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.24146975086778671\n",
      "    max_lr: 0.006823695741840761\n",
      "    div_factor: 292\n",
      "    final_div_factor: 175\n",
      "    weight_decay: 9.564518775984315e-06\n",
      "    pct_start: 0.14698722724143942\n",
      "  Params: total=4,035,609  trainable=4,035,609\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3.53 M  \n",
      "fwd MACs:                                                               38.13 MMACs\n",
      "fwd FLOPs:                                                              76.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           114.38 MMACs\n",
      "fwd+bwd FLOPs:                                                          229.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3.53 M = 100% Params, 38.13 MMACs = 100% MACs, 76.39 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "    (net): Sequential(\n",
      "      464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "      (0): Conv2d(432 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 86.4 KFLOPS = 0.11% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 3.2 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3.53 M = 99.99% Params, 38.08 MMACs = 99.89% MACs, 76.29 MFLOPS = 99.88% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      17.64 K = 0.5% Params, 1.74 MMACs = 4.57% MACs, 3.51 MFLOPS = 4.6% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.15% Params, 518.4 KMACs = 1.36% MACs, 1.04 MFLOPS = 1.36% FLOPs, 16, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        648 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 122.4 KFLOPS = 0.16% FLOPs\n",
      "        (0): Conv2d(576 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 115.2 KFLOPS = 0.15% FLOPs, 16, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      23.47 K = 0.66% Params, 2.33 MMACs = 6.12% MACs, 4.69 MFLOPS = 6.14% FLOPs\n",
      "      (conv1): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      73.01 K = 2.07% Params, 1.81 MMACs = 4.76% MACs, 3.64 MFLOPS = 4.77% FLOPs\n",
      "      (conv1): Conv2d(23.33 K = 0.66% Params, 583.2 KMACs = 1.53% MACs, 1.17 MFLOPS = 1.53% FLOPs, 36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.74 K = 0.08% Params, 64.8 KMACs = 0.17% MACs, 133.2 KFLOPS = 0.17% FLOPs\n",
      "        (0): Conv2d(2.59 K = 0.07% Params, 64.8 KMACs = 0.17% MACs, 129.6 KFLOPS = 0.17% FLOPs, 36, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      93.6 K = 2.65% Params, 2.33 MMACs = 6.12% MACs, 4.68 MFLOPS = 6.12% FLOPs\n",
      "      (conv1): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      291.17 K = 8.25% Params, 2.61 MMACs = 6.85% MACs, 5.24 MFLOPS = 6.85% FLOPs\n",
      "      (conv1): Conv2d(93.31 K = 2.64% Params, 839.81 KMACs = 2.2% MACs, 1.68 MFLOPS = 2.2% FLOPs, 72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        10.66 K = 0.3% Params, 93.31 KMACs = 0.24% MACs, 189.22 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(10.37 K = 0.29% Params, 93.31 KMACs = 0.24% MACs, 186.62 KFLOPS = 0.24% FLOPs, 72, 144, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      373.82 K = 10.59% Params, 3.36 MMACs = 8.81% MACs, 6.73 MFLOPS = 8.81% FLOPs\n",
      "      (conv1): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      1.16 M = 32.94% Params, 10.45 MMACs = 27.41% MACs, 20.92 MFLOPS = 27.39% FLOPs\n",
      "      (conv1): Conv2d(373.25 K = 10.57% Params, 3.36 MMACs = 8.81% MACs, 6.72 MFLOPS = 8.8% FLOPs, 144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        42.05 K = 1.19% Params, 373.25 KMACs = 0.98% MACs, 751.68 KFLOPS = 0.98% FLOPs\n",
      "        (0): Conv2d(41.47 K = 1.17% Params, 373.25 KMACs = 0.98% MACs, 746.5 KFLOPS = 0.98% FLOPs, 144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      1.49 M = 42.32% Params, 13.44 MMACs = 35.24% MACs, 26.89 MFLOPS = 35.2% FLOPs\n",
      "      (conv1): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  205.31 K\n",
      "fwd MACs:                                                               205.06 KMACs\n",
      "fwd FLOPs:                                                              410.37 KFLOPS\n",
      "fwd+bwd MACs:                                                           615.17 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.23 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  205.31 K = 100% Params, 205.06 KMACs = 100% MACs, 410.37 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(205.06 K = 99.87% Params, 204.8 KMACs = 99.88% MACs, 409.6 KFLOPS = 99.81% FLOPs, in_features=800, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.24146975086778671, inplace=False)\n",
      "  (3): Linear(257 = 0.13% Params, 256 MACs = 0.12% MACs, 512 FLOPS = 0.12% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_9/best_model.pth\n",
      "   val_rmse=20.645548\n",
      "   params: total=4,035,609, trainable=4,035,609\n",
      "→ Single-pass full run (Trial 44, ValObjective: 20.9401)\n",
      "\n",
      "Best Trial: 44\n",
      "  Best Score: 20.9401\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [32]\n",
      "    fusion_dropout: 0.01609831048035666\n",
      "    max_lr: 0.009806726286509299\n",
      "    div_factor: 457\n",
      "    final_div_factor: 381\n",
      "    weight_decay: 0.000456320235146774\n",
      "    pct_start: 0.2843720954621821\n",
      "  Params: total=3,855,961  trainable=3,855,961\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3.53 M  \n",
      "fwd MACs:                                                               38.13 MMACs\n",
      "fwd FLOPs:                                                              76.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           114.38 MMACs\n",
      "fwd+bwd FLOPs:                                                          229.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3.53 M = 100% Params, 38.13 MMACs = 100% MACs, 76.39 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "    (net): Sequential(\n",
      "      464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "      (0): Conv2d(432 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 86.4 KFLOPS = 0.11% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 3.2 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3.53 M = 99.99% Params, 38.08 MMACs = 99.89% MACs, 76.29 MFLOPS = 99.88% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      17.64 K = 0.5% Params, 1.74 MMACs = 4.57% MACs, 3.51 MFLOPS = 4.6% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.15% Params, 518.4 KMACs = 1.36% MACs, 1.04 MFLOPS = 1.36% FLOPs, 16, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        648 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 122.4 KFLOPS = 0.16% FLOPs\n",
      "        (0): Conv2d(576 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 115.2 KFLOPS = 0.15% FLOPs, 16, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      23.47 K = 0.66% Params, 2.33 MMACs = 6.12% MACs, 4.69 MFLOPS = 6.14% FLOPs\n",
      "      (conv1): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      73.01 K = 2.07% Params, 1.81 MMACs = 4.76% MACs, 3.64 MFLOPS = 4.77% FLOPs\n",
      "      (conv1): Conv2d(23.33 K = 0.66% Params, 583.2 KMACs = 1.53% MACs, 1.17 MFLOPS = 1.53% FLOPs, 36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.74 K = 0.08% Params, 64.8 KMACs = 0.17% MACs, 133.2 KFLOPS = 0.17% FLOPs\n",
      "        (0): Conv2d(2.59 K = 0.07% Params, 64.8 KMACs = 0.17% MACs, 129.6 KFLOPS = 0.17% FLOPs, 36, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      93.6 K = 2.65% Params, 2.33 MMACs = 6.12% MACs, 4.68 MFLOPS = 6.12% FLOPs\n",
      "      (conv1): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      291.17 K = 8.25% Params, 2.61 MMACs = 6.85% MACs, 5.24 MFLOPS = 6.85% FLOPs\n",
      "      (conv1): Conv2d(93.31 K = 2.64% Params, 839.81 KMACs = 2.2% MACs, 1.68 MFLOPS = 2.2% FLOPs, 72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        10.66 K = 0.3% Params, 93.31 KMACs = 0.24% MACs, 189.22 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(10.37 K = 0.29% Params, 93.31 KMACs = 0.24% MACs, 186.62 KFLOPS = 0.24% FLOPs, 72, 144, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      373.82 K = 10.59% Params, 3.36 MMACs = 8.81% MACs, 6.73 MFLOPS = 8.81% FLOPs\n",
      "      (conv1): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      1.16 M = 32.94% Params, 10.45 MMACs = 27.41% MACs, 20.92 MFLOPS = 27.39% FLOPs\n",
      "      (conv1): Conv2d(373.25 K = 10.57% Params, 3.36 MMACs = 8.81% MACs, 6.72 MFLOPS = 8.8% FLOPs, 144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        42.05 K = 1.19% Params, 373.25 KMACs = 0.98% MACs, 751.68 KFLOPS = 0.98% FLOPs\n",
      "        (0): Conv2d(41.47 K = 1.17% Params, 373.25 KMACs = 0.98% MACs, 746.5 KFLOPS = 0.98% FLOPs, 144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      1.49 M = 42.32% Params, 13.44 MMACs = 35.24% MACs, 26.89 MFLOPS = 35.2% FLOPs\n",
      "      (conv1): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  25.66 K \n",
      "fwd MACs:                                                               25.63 KMACs\n",
      "fwd FLOPs:                                                              51.3 KFLOPS\n",
      "fwd+bwd MACs:                                                           76.9 KMACs\n",
      "fwd+bwd FLOPs:                                                          153.89 KFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  25.66 K = 100% Params, 25.63 KMACs = 100% MACs, 51.3 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(25.63 K = 99.87% Params, 25.6 KMACs = 99.88% MACs, 51.2 KFLOPS = 99.81% FLOPs, in_features=800, out_features=32, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 32 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.01609831048035666, inplace=False)\n",
      "  (3): Linear(33 = 0.13% Params, 32 MACs = 0.12% MACs, 64 FLOPS = 0.12% FLOPs, in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_44/best_model.pth\n",
      "   val_rmse=20.796439\n",
      "   params: total=3,855,961, trainable=3,855,961\n",
      "→ Single-pass full run (Trial 32, ValObjective: 20.9424)\n",
      "\n",
      "Best Trial: 32\n",
      "  Best Score: 20.9424\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.03861898902985868\n",
      "    max_lr: 0.006926376038192465\n",
      "    div_factor: 380\n",
      "    final_div_factor: 483\n",
      "    weight_decay: 0.00039058997934487884\n",
      "    pct_start: 0.26143022140544503\n",
      "  Params: total=4,035,609  trainable=4,035,609\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3.53 M  \n",
      "fwd MACs:                                                               38.13 MMACs\n",
      "fwd FLOPs:                                                              76.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           114.38 MMACs\n",
      "fwd+bwd FLOPs:                                                          229.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3.53 M = 100% Params, 38.13 MMACs = 100% MACs, 76.39 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "    (net): Sequential(\n",
      "      464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "      (0): Conv2d(432 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 86.4 KFLOPS = 0.11% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 3.2 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3.53 M = 99.99% Params, 38.08 MMACs = 99.89% MACs, 76.29 MFLOPS = 99.88% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      17.64 K = 0.5% Params, 1.74 MMACs = 4.57% MACs, 3.51 MFLOPS = 4.6% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.15% Params, 518.4 KMACs = 1.36% MACs, 1.04 MFLOPS = 1.36% FLOPs, 16, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        648 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 122.4 KFLOPS = 0.16% FLOPs\n",
      "        (0): Conv2d(576 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 115.2 KFLOPS = 0.15% FLOPs, 16, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      23.47 K = 0.66% Params, 2.33 MMACs = 6.12% MACs, 4.69 MFLOPS = 6.14% FLOPs\n",
      "      (conv1): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      73.01 K = 2.07% Params, 1.81 MMACs = 4.76% MACs, 3.64 MFLOPS = 4.77% FLOPs\n",
      "      (conv1): Conv2d(23.33 K = 0.66% Params, 583.2 KMACs = 1.53% MACs, 1.17 MFLOPS = 1.53% FLOPs, 36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.74 K = 0.08% Params, 64.8 KMACs = 0.17% MACs, 133.2 KFLOPS = 0.17% FLOPs\n",
      "        (0): Conv2d(2.59 K = 0.07% Params, 64.8 KMACs = 0.17% MACs, 129.6 KFLOPS = 0.17% FLOPs, 36, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      93.6 K = 2.65% Params, 2.33 MMACs = 6.12% MACs, 4.68 MFLOPS = 6.12% FLOPs\n",
      "      (conv1): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      291.17 K = 8.25% Params, 2.61 MMACs = 6.85% MACs, 5.24 MFLOPS = 6.85% FLOPs\n",
      "      (conv1): Conv2d(93.31 K = 2.64% Params, 839.81 KMACs = 2.2% MACs, 1.68 MFLOPS = 2.2% FLOPs, 72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        10.66 K = 0.3% Params, 93.31 KMACs = 0.24% MACs, 189.22 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(10.37 K = 0.29% Params, 93.31 KMACs = 0.24% MACs, 186.62 KFLOPS = 0.24% FLOPs, 72, 144, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      373.82 K = 10.59% Params, 3.36 MMACs = 8.81% MACs, 6.73 MFLOPS = 8.81% FLOPs\n",
      "      (conv1): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      1.16 M = 32.94% Params, 10.45 MMACs = 27.41% MACs, 20.92 MFLOPS = 27.39% FLOPs\n",
      "      (conv1): Conv2d(373.25 K = 10.57% Params, 3.36 MMACs = 8.81% MACs, 6.72 MFLOPS = 8.8% FLOPs, 144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        42.05 K = 1.19% Params, 373.25 KMACs = 0.98% MACs, 751.68 KFLOPS = 0.98% FLOPs\n",
      "        (0): Conv2d(41.47 K = 1.17% Params, 373.25 KMACs = 0.98% MACs, 746.5 KFLOPS = 0.98% FLOPs, 144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      1.49 M = 42.32% Params, 13.44 MMACs = 35.24% MACs, 26.89 MFLOPS = 35.2% FLOPs\n",
      "      (conv1): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  205.31 K\n",
      "fwd MACs:                                                               205.06 KMACs\n",
      "fwd FLOPs:                                                              410.37 KFLOPS\n",
      "fwd+bwd MACs:                                                           615.17 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.23 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  205.31 K = 100% Params, 205.06 KMACs = 100% MACs, 410.37 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(205.06 K = 99.87% Params, 204.8 KMACs = 99.88% MACs, 409.6 KFLOPS = 99.81% FLOPs, in_features=800, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.03861898902985868, inplace=False)\n",
      "  (3): Linear(257 = 0.13% Params, 256 MACs = 0.12% MACs, 512 FLOPS = 0.12% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_32/best_model.pth\n",
      "   val_rmse=20.859428\n",
      "   params: total=4,035,609, trainable=4,035,609\n",
      "\n",
      "Winner after single-pass: Trial 11 (trial_11) by val_rmse=20.522283\n",
      "\n",
      "Re-running winner with seeds [0, 1, 2, 3, 4] at 100 epochs...\n",
      "\n",
      "\n",
      "Best Trial: 11\n",
      "  Best Score: 20.7489\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.001955269809990917\n",
      "    max_lr: 0.007447532173072955\n",
      "    div_factor: 304\n",
      "    final_div_factor: 585\n",
      "    weight_decay: 0.000521013207818956\n",
      "    pct_start: 0.2272947214860106\n",
      "  Params: total=3,941,145  trainable=3,941,145\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3.53 M  \n",
      "fwd MACs:                                                               38.13 MMACs\n",
      "fwd FLOPs:                                                              76.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           114.38 MMACs\n",
      "fwd+bwd FLOPs:                                                          229.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3.53 M = 100% Params, 38.13 MMACs = 100% MACs, 76.39 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "    (net): Sequential(\n",
      "      464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "      (0): Conv2d(432 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 86.4 KFLOPS = 0.11% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 3.2 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3.53 M = 99.99% Params, 38.08 MMACs = 99.89% MACs, 76.29 MFLOPS = 99.88% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      17.64 K = 0.5% Params, 1.74 MMACs = 4.57% MACs, 3.51 MFLOPS = 4.6% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.15% Params, 518.4 KMACs = 1.36% MACs, 1.04 MFLOPS = 1.36% FLOPs, 16, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        648 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 122.4 KFLOPS = 0.16% FLOPs\n",
      "        (0): Conv2d(576 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 115.2 KFLOPS = 0.15% FLOPs, 16, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      23.47 K = 0.66% Params, 2.33 MMACs = 6.12% MACs, 4.69 MFLOPS = 6.14% FLOPs\n",
      "      (conv1): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      73.01 K = 2.07% Params, 1.81 MMACs = 4.76% MACs, 3.64 MFLOPS = 4.77% FLOPs\n",
      "      (conv1): Conv2d(23.33 K = 0.66% Params, 583.2 KMACs = 1.53% MACs, 1.17 MFLOPS = 1.53% FLOPs, 36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.74 K = 0.08% Params, 64.8 KMACs = 0.17% MACs, 133.2 KFLOPS = 0.17% FLOPs\n",
      "        (0): Conv2d(2.59 K = 0.07% Params, 64.8 KMACs = 0.17% MACs, 129.6 KFLOPS = 0.17% FLOPs, 36, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      93.6 K = 2.65% Params, 2.33 MMACs = 6.12% MACs, 4.68 MFLOPS = 6.12% FLOPs\n",
      "      (conv1): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      291.17 K = 8.25% Params, 2.61 MMACs = 6.85% MACs, 5.24 MFLOPS = 6.85% FLOPs\n",
      "      (conv1): Conv2d(93.31 K = 2.64% Params, 839.81 KMACs = 2.2% MACs, 1.68 MFLOPS = 2.2% FLOPs, 72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        10.66 K = 0.3% Params, 93.31 KMACs = 0.24% MACs, 189.22 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(10.37 K = 0.29% Params, 93.31 KMACs = 0.24% MACs, 186.62 KFLOPS = 0.24% FLOPs, 72, 144, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      373.82 K = 10.59% Params, 3.36 MMACs = 8.81% MACs, 6.73 MFLOPS = 8.81% FLOPs\n",
      "      (conv1): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      1.16 M = 32.94% Params, 10.45 MMACs = 27.41% MACs, 20.92 MFLOPS = 27.39% FLOPs\n",
      "      (conv1): Conv2d(373.25 K = 10.57% Params, 3.36 MMACs = 8.81% MACs, 6.72 MFLOPS = 8.8% FLOPs, 144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        42.05 K = 1.19% Params, 373.25 KMACs = 0.98% MACs, 751.68 KFLOPS = 0.98% FLOPs\n",
      "        (0): Conv2d(41.47 K = 1.17% Params, 373.25 KMACs = 0.98% MACs, 746.5 KFLOPS = 0.98% FLOPs, 144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      1.49 M = 42.32% Params, 13.44 MMACs = 35.24% MACs, 26.89 MFLOPS = 35.2% FLOPs\n",
      "      (conv1): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  110.85 K\n",
      "fwd MACs:                                                               110.66 KMACs\n",
      "fwd FLOPs:                                                              221.5 KFLOPS\n",
      "fwd+bwd MACs:                                                           331.97 KMACs\n",
      "fwd+bwd FLOPs:                                                          664.51 KFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  110.85 K = 100% Params, 110.66 KMACs = 100% MACs, 221.5 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(102.53 K = 92.49% Params, 102.4 KMACs = 92.54% MACs, 204.8 KFLOPS = 92.46% FLOPs, in_features=800, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 128 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (3): Linear(8.26 K = 7.45% Params, 8.19 KMACs = 7.4% MACs, 16.38 KFLOPS = 7.4% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 64 FLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (6): Linear(65 = 0.06% Params, 64 MACs = 0.06% MACs, 128 FLOPS = 0.06% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_11_seed0/best_model.pth\n",
      "   Seed 0: val_rmse=20.522283, test_loss=579.509206, test_rmse=24.018598, val_loss=420.585403\n",
      "\n",
      "Best Trial: 11\n",
      "  Best Score: 20.7489\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.001955269809990917\n",
      "    max_lr: 0.007447532173072955\n",
      "    div_factor: 304\n",
      "    final_div_factor: 585\n",
      "    weight_decay: 0.000521013207818956\n",
      "    pct_start: 0.2272947214860106\n",
      "  Params: total=3,941,145  trainable=3,941,145\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3.53 M  \n",
      "fwd MACs:                                                               38.13 MMACs\n",
      "fwd FLOPs:                                                              76.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           114.38 MMACs\n",
      "fwd+bwd FLOPs:                                                          229.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3.53 M = 100% Params, 38.13 MMACs = 100% MACs, 76.39 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "    (net): Sequential(\n",
      "      464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "      (0): Conv2d(432 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 86.4 KFLOPS = 0.11% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 3.2 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3.53 M = 99.99% Params, 38.08 MMACs = 99.89% MACs, 76.29 MFLOPS = 99.88% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      17.64 K = 0.5% Params, 1.74 MMACs = 4.57% MACs, 3.51 MFLOPS = 4.6% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.15% Params, 518.4 KMACs = 1.36% MACs, 1.04 MFLOPS = 1.36% FLOPs, 16, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        648 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 122.4 KFLOPS = 0.16% FLOPs\n",
      "        (0): Conv2d(576 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 115.2 KFLOPS = 0.15% FLOPs, 16, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      23.47 K = 0.66% Params, 2.33 MMACs = 6.12% MACs, 4.69 MFLOPS = 6.14% FLOPs\n",
      "      (conv1): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      73.01 K = 2.07% Params, 1.81 MMACs = 4.76% MACs, 3.64 MFLOPS = 4.77% FLOPs\n",
      "      (conv1): Conv2d(23.33 K = 0.66% Params, 583.2 KMACs = 1.53% MACs, 1.17 MFLOPS = 1.53% FLOPs, 36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.74 K = 0.08% Params, 64.8 KMACs = 0.17% MACs, 133.2 KFLOPS = 0.17% FLOPs\n",
      "        (0): Conv2d(2.59 K = 0.07% Params, 64.8 KMACs = 0.17% MACs, 129.6 KFLOPS = 0.17% FLOPs, 36, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      93.6 K = 2.65% Params, 2.33 MMACs = 6.12% MACs, 4.68 MFLOPS = 6.12% FLOPs\n",
      "      (conv1): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      291.17 K = 8.25% Params, 2.61 MMACs = 6.85% MACs, 5.24 MFLOPS = 6.85% FLOPs\n",
      "      (conv1): Conv2d(93.31 K = 2.64% Params, 839.81 KMACs = 2.2% MACs, 1.68 MFLOPS = 2.2% FLOPs, 72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        10.66 K = 0.3% Params, 93.31 KMACs = 0.24% MACs, 189.22 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(10.37 K = 0.29% Params, 93.31 KMACs = 0.24% MACs, 186.62 KFLOPS = 0.24% FLOPs, 72, 144, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      373.82 K = 10.59% Params, 3.36 MMACs = 8.81% MACs, 6.73 MFLOPS = 8.81% FLOPs\n",
      "      (conv1): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      1.16 M = 32.94% Params, 10.45 MMACs = 27.41% MACs, 20.92 MFLOPS = 27.39% FLOPs\n",
      "      (conv1): Conv2d(373.25 K = 10.57% Params, 3.36 MMACs = 8.81% MACs, 6.72 MFLOPS = 8.8% FLOPs, 144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        42.05 K = 1.19% Params, 373.25 KMACs = 0.98% MACs, 751.68 KFLOPS = 0.98% FLOPs\n",
      "        (0): Conv2d(41.47 K = 1.17% Params, 373.25 KMACs = 0.98% MACs, 746.5 KFLOPS = 0.98% FLOPs, 144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      1.49 M = 42.32% Params, 13.44 MMACs = 35.24% MACs, 26.89 MFLOPS = 35.2% FLOPs\n",
      "      (conv1): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  110.85 K\n",
      "fwd MACs:                                                               110.66 KMACs\n",
      "fwd FLOPs:                                                              221.5 KFLOPS\n",
      "fwd+bwd MACs:                                                           331.97 KMACs\n",
      "fwd+bwd FLOPs:                                                          664.51 KFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  110.85 K = 100% Params, 110.66 KMACs = 100% MACs, 221.5 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(102.53 K = 92.49% Params, 102.4 KMACs = 92.54% MACs, 204.8 KFLOPS = 92.46% FLOPs, in_features=800, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 128 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (3): Linear(8.26 K = 7.45% Params, 8.19 KMACs = 7.4% MACs, 16.38 KFLOPS = 7.4% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 64 FLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (6): Linear(65 = 0.06% Params, 64 MACs = 0.06% MACs, 128 FLOPS = 0.06% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_11_seed1/best_model.pth\n",
      "   Seed 1: val_rmse=20.933940, test_loss=586.312012, test_rmse=24.152931, val_loss=438.371073\n",
      "\n",
      "Best Trial: 11\n",
      "  Best Score: 20.7489\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.001955269809990917\n",
      "    max_lr: 0.007447532173072955\n",
      "    div_factor: 304\n",
      "    final_div_factor: 585\n",
      "    weight_decay: 0.000521013207818956\n",
      "    pct_start: 0.2272947214860106\n",
      "  Params: total=3,941,145  trainable=3,941,145\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3.53 M  \n",
      "fwd MACs:                                                               38.13 MMACs\n",
      "fwd FLOPs:                                                              76.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           114.38 MMACs\n",
      "fwd+bwd FLOPs:                                                          229.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3.53 M = 100% Params, 38.13 MMACs = 100% MACs, 76.39 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "    (net): Sequential(\n",
      "      464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "      (0): Conv2d(432 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 86.4 KFLOPS = 0.11% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 3.2 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3.53 M = 99.99% Params, 38.08 MMACs = 99.89% MACs, 76.29 MFLOPS = 99.88% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      17.64 K = 0.5% Params, 1.74 MMACs = 4.57% MACs, 3.51 MFLOPS = 4.6% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.15% Params, 518.4 KMACs = 1.36% MACs, 1.04 MFLOPS = 1.36% FLOPs, 16, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        648 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 122.4 KFLOPS = 0.16% FLOPs\n",
      "        (0): Conv2d(576 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 115.2 KFLOPS = 0.15% FLOPs, 16, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      23.47 K = 0.66% Params, 2.33 MMACs = 6.12% MACs, 4.69 MFLOPS = 6.14% FLOPs\n",
      "      (conv1): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      73.01 K = 2.07% Params, 1.81 MMACs = 4.76% MACs, 3.64 MFLOPS = 4.77% FLOPs\n",
      "      (conv1): Conv2d(23.33 K = 0.66% Params, 583.2 KMACs = 1.53% MACs, 1.17 MFLOPS = 1.53% FLOPs, 36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.74 K = 0.08% Params, 64.8 KMACs = 0.17% MACs, 133.2 KFLOPS = 0.17% FLOPs\n",
      "        (0): Conv2d(2.59 K = 0.07% Params, 64.8 KMACs = 0.17% MACs, 129.6 KFLOPS = 0.17% FLOPs, 36, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      93.6 K = 2.65% Params, 2.33 MMACs = 6.12% MACs, 4.68 MFLOPS = 6.12% FLOPs\n",
      "      (conv1): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      291.17 K = 8.25% Params, 2.61 MMACs = 6.85% MACs, 5.24 MFLOPS = 6.85% FLOPs\n",
      "      (conv1): Conv2d(93.31 K = 2.64% Params, 839.81 KMACs = 2.2% MACs, 1.68 MFLOPS = 2.2% FLOPs, 72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        10.66 K = 0.3% Params, 93.31 KMACs = 0.24% MACs, 189.22 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(10.37 K = 0.29% Params, 93.31 KMACs = 0.24% MACs, 186.62 KFLOPS = 0.24% FLOPs, 72, 144, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      373.82 K = 10.59% Params, 3.36 MMACs = 8.81% MACs, 6.73 MFLOPS = 8.81% FLOPs\n",
      "      (conv1): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      1.16 M = 32.94% Params, 10.45 MMACs = 27.41% MACs, 20.92 MFLOPS = 27.39% FLOPs\n",
      "      (conv1): Conv2d(373.25 K = 10.57% Params, 3.36 MMACs = 8.81% MACs, 6.72 MFLOPS = 8.8% FLOPs, 144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        42.05 K = 1.19% Params, 373.25 KMACs = 0.98% MACs, 751.68 KFLOPS = 0.98% FLOPs\n",
      "        (0): Conv2d(41.47 K = 1.17% Params, 373.25 KMACs = 0.98% MACs, 746.5 KFLOPS = 0.98% FLOPs, 144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      1.49 M = 42.32% Params, 13.44 MMACs = 35.24% MACs, 26.89 MFLOPS = 35.2% FLOPs\n",
      "      (conv1): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  110.85 K\n",
      "fwd MACs:                                                               110.66 KMACs\n",
      "fwd FLOPs:                                                              221.5 KFLOPS\n",
      "fwd+bwd MACs:                                                           331.97 KMACs\n",
      "fwd+bwd FLOPs:                                                          664.51 KFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  110.85 K = 100% Params, 110.66 KMACs = 100% MACs, 221.5 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(102.53 K = 92.49% Params, 102.4 KMACs = 92.54% MACs, 204.8 KFLOPS = 92.46% FLOPs, in_features=800, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 128 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (3): Linear(8.26 K = 7.45% Params, 8.19 KMACs = 7.4% MACs, 16.38 KFLOPS = 7.4% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 64 FLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (6): Linear(65 = 0.06% Params, 64 MACs = 0.06% MACs, 128 FLOPS = 0.06% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_11_seed2/best_model.pth\n",
      "   Seed 2: val_rmse=21.255297, test_loss=653.034576, test_rmse=25.562296, val_loss=448.540344\n",
      "\n",
      "Best Trial: 11\n",
      "  Best Score: 20.7489\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.001955269809990917\n",
      "    max_lr: 0.007447532173072955\n",
      "    div_factor: 304\n",
      "    final_div_factor: 585\n",
      "    weight_decay: 0.000521013207818956\n",
      "    pct_start: 0.2272947214860106\n",
      "  Params: total=3,941,145  trainable=3,941,145\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3.53 M  \n",
      "fwd MACs:                                                               38.13 MMACs\n",
      "fwd FLOPs:                                                              76.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           114.38 MMACs\n",
      "fwd+bwd FLOPs:                                                          229.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3.53 M = 100% Params, 38.13 MMACs = 100% MACs, 76.39 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "    (net): Sequential(\n",
      "      464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "      (0): Conv2d(432 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 86.4 KFLOPS = 0.11% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 3.2 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3.53 M = 99.99% Params, 38.08 MMACs = 99.89% MACs, 76.29 MFLOPS = 99.88% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      17.64 K = 0.5% Params, 1.74 MMACs = 4.57% MACs, 3.51 MFLOPS = 4.6% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.15% Params, 518.4 KMACs = 1.36% MACs, 1.04 MFLOPS = 1.36% FLOPs, 16, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        648 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 122.4 KFLOPS = 0.16% FLOPs\n",
      "        (0): Conv2d(576 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 115.2 KFLOPS = 0.15% FLOPs, 16, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      23.47 K = 0.66% Params, 2.33 MMACs = 6.12% MACs, 4.69 MFLOPS = 6.14% FLOPs\n",
      "      (conv1): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      73.01 K = 2.07% Params, 1.81 MMACs = 4.76% MACs, 3.64 MFLOPS = 4.77% FLOPs\n",
      "      (conv1): Conv2d(23.33 K = 0.66% Params, 583.2 KMACs = 1.53% MACs, 1.17 MFLOPS = 1.53% FLOPs, 36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.74 K = 0.08% Params, 64.8 KMACs = 0.17% MACs, 133.2 KFLOPS = 0.17% FLOPs\n",
      "        (0): Conv2d(2.59 K = 0.07% Params, 64.8 KMACs = 0.17% MACs, 129.6 KFLOPS = 0.17% FLOPs, 36, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      93.6 K = 2.65% Params, 2.33 MMACs = 6.12% MACs, 4.68 MFLOPS = 6.12% FLOPs\n",
      "      (conv1): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      291.17 K = 8.25% Params, 2.61 MMACs = 6.85% MACs, 5.24 MFLOPS = 6.85% FLOPs\n",
      "      (conv1): Conv2d(93.31 K = 2.64% Params, 839.81 KMACs = 2.2% MACs, 1.68 MFLOPS = 2.2% FLOPs, 72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        10.66 K = 0.3% Params, 93.31 KMACs = 0.24% MACs, 189.22 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(10.37 K = 0.29% Params, 93.31 KMACs = 0.24% MACs, 186.62 KFLOPS = 0.24% FLOPs, 72, 144, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      373.82 K = 10.59% Params, 3.36 MMACs = 8.81% MACs, 6.73 MFLOPS = 8.81% FLOPs\n",
      "      (conv1): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      1.16 M = 32.94% Params, 10.45 MMACs = 27.41% MACs, 20.92 MFLOPS = 27.39% FLOPs\n",
      "      (conv1): Conv2d(373.25 K = 10.57% Params, 3.36 MMACs = 8.81% MACs, 6.72 MFLOPS = 8.8% FLOPs, 144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        42.05 K = 1.19% Params, 373.25 KMACs = 0.98% MACs, 751.68 KFLOPS = 0.98% FLOPs\n",
      "        (0): Conv2d(41.47 K = 1.17% Params, 373.25 KMACs = 0.98% MACs, 746.5 KFLOPS = 0.98% FLOPs, 144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      1.49 M = 42.32% Params, 13.44 MMACs = 35.24% MACs, 26.89 MFLOPS = 35.2% FLOPs\n",
      "      (conv1): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  110.85 K\n",
      "fwd MACs:                                                               110.66 KMACs\n",
      "fwd FLOPs:                                                              221.5 KFLOPS\n",
      "fwd+bwd MACs:                                                           331.97 KMACs\n",
      "fwd+bwd FLOPs:                                                          664.51 KFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  110.85 K = 100% Params, 110.66 KMACs = 100% MACs, 221.5 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(102.53 K = 92.49% Params, 102.4 KMACs = 92.54% MACs, 204.8 KFLOPS = 92.46% FLOPs, in_features=800, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 128 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (3): Linear(8.26 K = 7.45% Params, 8.19 KMACs = 7.4% MACs, 16.38 KFLOPS = 7.4% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 64 FLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (6): Linear(65 = 0.06% Params, 64 MACs = 0.06% MACs, 128 FLOPS = 0.06% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_11_seed3/best_model.pth\n",
      "   Seed 3: val_rmse=21.077114, test_loss=610.424815, test_rmse=24.702389, val_loss=441.896973\n",
      "\n",
      "Best Trial: 11\n",
      "  Best Score: 20.7489\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [128,64]\n",
      "    fusion_dropout: 0.001955269809990917\n",
      "    max_lr: 0.007447532173072955\n",
      "    div_factor: 304\n",
      "    final_div_factor: 585\n",
      "    weight_decay: 0.000521013207818956\n",
      "    pct_start: 0.2272947214860106\n",
      "  Params: total=3,941,145  trainable=3,941,145\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3.53 M  \n",
      "fwd MACs:                                                               38.13 MMACs\n",
      "fwd FLOPs:                                                              76.39 MFLOPS\n",
      "fwd+bwd MACs:                                                           114.38 MMACs\n",
      "fwd+bwd FLOPs:                                                          229.16 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3.53 M = 100% Params, 38.13 MMACs = 100% MACs, 76.39 MFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "    (net): Sequential(\n",
      "      464 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 91.2 KFLOPS = 0.12% FLOPs\n",
      "      (0): Conv2d(432 = 0.01% Params, 43.2 KMACs = 0.11% MACs, 86.4 KFLOPS = 0.11% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 3.2 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3.53 M = 99.99% Params, 38.08 MMACs = 99.89% MACs, 76.29 MFLOPS = 99.88% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      17.64 K = 0.5% Params, 1.74 MMACs = 4.57% MACs, 3.51 MFLOPS = 4.6% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.15% Params, 518.4 KMACs = 1.36% MACs, 1.04 MFLOPS = 1.36% FLOPs, 16, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        648 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 122.4 KFLOPS = 0.16% FLOPs\n",
      "        (0): Conv2d(576 = 0.02% Params, 57.6 KMACs = 0.15% MACs, 115.2 KFLOPS = 0.15% FLOPs, 16, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      23.47 K = 0.66% Params, 2.33 MMACs = 6.12% MACs, 4.69 MFLOPS = 6.14% FLOPs\n",
      "      (conv1): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(11.66 K = 0.33% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(72 = 0% Params, 0 MACs = 0% MACs, 7.2 KFLOPS = 0.01% FLOPs, 36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      73.01 K = 2.07% Params, 1.81 MMACs = 4.76% MACs, 3.64 MFLOPS = 4.77% FLOPs\n",
      "      (conv1): Conv2d(23.33 K = 0.66% Params, 583.2 KMACs = 1.53% MACs, 1.17 MFLOPS = 1.53% FLOPs, 36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        2.74 K = 0.08% Params, 64.8 KMACs = 0.17% MACs, 133.2 KFLOPS = 0.17% FLOPs\n",
      "        (0): Conv2d(2.59 K = 0.07% Params, 64.8 KMACs = 0.17% MACs, 129.6 KFLOPS = 0.17% FLOPs, 36, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      93.6 K = 2.65% Params, 2.33 MMACs = 6.12% MACs, 4.68 MFLOPS = 6.12% FLOPs\n",
      "      (conv1): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(46.66 K = 1.32% Params, 1.17 MMACs = 3.06% MACs, 2.33 MFLOPS = 3.05% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(144 = 0% Params, 0 MACs = 0% MACs, 3.6 KFLOPS = 0% FLOPs, 72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      291.17 K = 8.25% Params, 2.61 MMACs = 6.85% MACs, 5.24 MFLOPS = 6.85% FLOPs\n",
      "      (conv1): Conv2d(93.31 K = 2.64% Params, 839.81 KMACs = 2.2% MACs, 1.68 MFLOPS = 2.2% FLOPs, 72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        10.66 K = 0.3% Params, 93.31 KMACs = 0.24% MACs, 189.22 KFLOPS = 0.25% FLOPs\n",
      "        (0): Conv2d(10.37 K = 0.29% Params, 93.31 KMACs = 0.24% MACs, 186.62 KFLOPS = 0.24% FLOPs, 72, 144, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      373.82 K = 10.59% Params, 3.36 MMACs = 8.81% MACs, 6.73 MFLOPS = 8.81% FLOPs\n",
      "      (conv1): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(186.62 K = 5.29% Params, 1.68 MMACs = 4.41% MACs, 3.36 MFLOPS = 4.4% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(288 = 0.01% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      1.16 M = 32.94% Params, 10.45 MMACs = 27.41% MACs, 20.92 MFLOPS = 27.39% FLOPs\n",
      "      (conv1): Conv2d(373.25 K = 10.57% Params, 3.36 MMACs = 8.81% MACs, 6.72 MFLOPS = 8.8% FLOPs, 144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        42.05 K = 1.19% Params, 373.25 KMACs = 0.98% MACs, 751.68 KFLOPS = 0.98% FLOPs\n",
      "        (0): Conv2d(41.47 K = 1.17% Params, 373.25 KMACs = 0.98% MACs, 746.5 KFLOPS = 0.98% FLOPs, 144, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      1.49 M = 42.32% Params, 13.44 MMACs = 35.24% MACs, 26.89 MFLOPS = 35.2% FLOPs\n",
      "      (conv1): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(746.5 K = 21.15% Params, 6.72 MMACs = 17.62% MACs, 13.44 MFLOPS = 17.59% FLOPs, 288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(576 = 0.02% Params, 0 MACs = 0% MACs, 5.18 KFLOPS = 0.01% FLOPs, 288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 2.59 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  110.85 K\n",
      "fwd MACs:                                                               110.66 KMACs\n",
      "fwd FLOPs:                                                              221.5 KFLOPS\n",
      "fwd+bwd MACs:                                                           331.97 KMACs\n",
      "fwd+bwd FLOPs:                                                          664.51 KFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  110.85 K = 100% Params, 110.66 KMACs = 100% MACs, 221.5 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(102.53 K = 92.49% Params, 102.4 KMACs = 92.54% MACs, 204.8 KFLOPS = 92.46% FLOPs, in_features=800, out_features=128, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 128 FLOPS = 0.06% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (3): Linear(8.26 K = 7.45% Params, 8.19 KMACs = 7.4% MACs, 16.38 KFLOPS = 7.4% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 64 FLOPS = 0.03% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.001955269809990917, inplace=False)\n",
      "  (6): Linear(65 = 0.06% Params, 64 MACs = 0.06% MACs, 128 FLOPS = 0.06% FLOPs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_11_seed4/best_model.pth\n",
      "   Seed 4: val_rmse=20.450650, test_loss=658.703659, test_rmse=25.637068, val_loss=417.077489\n",
      "\n",
      "Winner aggregated val_rmse: 20.847857 ± 0.349910\n",
      "Model params: total=3,941,145, trainable=3,941,145\n",
      "Saved multi-seed summary to: logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_11/winner_multi_seed_summary.txt\n",
      "{'winner_trial_number': 11, 'winner_trial_name': 'trial_11', 'primary_metric': 'val_rmse', 'aggregates': {'train_loss': {'mean': 292.1260329635055, 'std': 48.21320067233721}, 'val_loss': {'mean': 433.29425659179685, 'std': 13.754294544085635}, 'test_loss': {'mean': 617.5968536376953, 'std': 36.8322209955985}, 'min_lr': {'mean': 2.4498461095634722e-05, 'std': 0.0}, 'max_lr': {'mean': 0.007447532173072954, 'std': 9.69739903612216e-19}, 'total_time': {'mean': 26.355614185333252, 'std': 0.1626360273871314}, 'average_epoch_time': {'mean': 0.262518648147583, 'std': 0.001472292779273656}, 'train_mse': {'mean': 292.1167510986328, 'std': 48.1816007916428}, 'train_mae': {'mean': 13.195129013061523, 'std': 1.2978093567940319}, 'train_rmse': {'mean': 17.04516094700534, 'std': 1.4050086251144724}, 'train_r2': {'mean': 0.9660290837287903, 'std': 0.005603147551602977}, 'val_mse': {'mean': 434.7310852050781, 'std': 14.57786574239545}, 'val_mae': {'mean': 16.57951965332031, 'std': 0.46665338396259365}, 'val_rmse': {'mean': 20.847856856807674, 'std': 0.34991013691982037}, 'val_r2': {'mean': 0.9381376504898071, 'std': 0.0020744404238507903}, 'test_mse': {'mean': 616.2310791015625, 'std': 37.85215147658061}, 'test_mae': {'mean': 19.448498153686522, 'std': 0.8107666874366908}, 'test_rmse': {'mean': 24.81465646032606, 'std': 0.7614984057440538}, 'test_r2': {'mean': 0.9282792806625366, 'std': 0.004405466474848014}, 'total_params': {'mean': 3941145.0, 'std': 0.0}, 'trainable_params': {'mean': 3941145.0, 'std': 0.0}, 'flops': {'mean': 77208848.0, 'std': 0.0}, 'macs': {'mean': 38535104.0, 'std': 0.0}}, 'total_params': 3941145, 'trainable_params': 3941145, 'flops': 77208848.0, 'macs': 38535104.0, 'summary_path': 'logs/Regression/Moneyball_cleaned/CNN_hybrid/FeatureWrap/best_model/trial_11/winner_multi_seed_summary.txt'}\n"
     ]
    }
   ],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     attributes=attributes,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "     path_vision=path_vision, path_mlp=path_mlp,\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: BIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"BIE\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes — Train: (862, 72), Val: (185, 72), Test: (185, 72)\n",
      "Numerical features: 8 — ['Year', 'RA', 'W', 'OBP', 'SLG', 'BA', 'OOBP', 'OSLG']\n",
      "Categorical features: 6 — ['Team', 'League', 'Playoffs', 'RankSeason', 'RankPlayoffs', 'G']\n",
      "Total features: 72\n",
      "Images shape (C,H,W): (3, 72, 72)\n",
      "Attributes: 72\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6, 8, 9, 12, 18, 24, 36, 72]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = [8, 16, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vision=f\"./logs/{task_type}/{dataset_name}/{vision_name}/{name}/best_model/trial_84\"\n",
    "path_mlp=f\"./logs/{task_type}/{dataset_name}/mlp/best_model/trial_38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-16 17:34:30,216] A new study created in memory with name: no-name-fad0e737-bf2b-4792-9474-f6c4f20fdbb4\n",
      "[I 2025-12-16 17:35:00,676] Trial 0 finished with value: 86.1462379701299 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.1989233051605483, 'max_lr': 2.6254626712152412e-05, 'div_factor': 420, 'final_div_factor': 134, 'weight_decay': 1.4897183163940669e-06, 'pct_start': 0.20982068409536475}. Best is trial 0 with value: 86.1462379701299.\n",
      "[I 2025-12-16 17:35:30,758] Trial 1 finished with value: 40.737690757358685 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.2912242218212105, 'max_lr': 1.6632873090856905e-05, 'div_factor': 668, 'final_div_factor': 126, 'weight_decay': 4.12546592252506e-06, 'pct_start': 0.20449003000747082}. Best is trial 1 with value: 40.737690757358685.\n",
      "[I 2025-12-16 17:36:01,134] Trial 2 finished with value: 22.523572314676308 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.10432188073220806, 'max_lr': 0.004966546998220236, 'div_factor': 302, 'final_div_factor': 253, 'weight_decay': 2.7070872390898014e-06, 'pct_start': 0.22552208191594408}. Best is trial 2 with value: 22.523572314676308.\n",
      "[I 2025-12-16 17:36:31,252] Trial 3 finished with value: 25.319641160877953 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.18288504936461378, 'max_lr': 0.00037790324048807104, 'div_factor': 544, 'final_div_factor': 277, 'weight_decay': 0.0010207582866932582, 'pct_start': 0.3233856110862626}. Best is trial 2 with value: 22.523572314676308.\n",
      "[I 2025-12-16 17:37:01,576] Trial 4 finished with value: 632.2828678684881 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.2682772064812959, 'max_lr': 1.3230708958632205e-05, 'div_factor': 992, 'final_div_factor': 229, 'weight_decay': 0.000296212633154987, 'pct_start': 0.3924487920271006}. Best is trial 2 with value: 22.523572314676308.\n",
      "[I 2025-12-16 17:37:31,548] Trial 5 finished with value: 22.153327006590626 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.21883127976846278, 'max_lr': 0.0006780935364518619, 'div_factor': 99, 'final_div_factor': 373, 'weight_decay': 0.0032498928314500194, 'pct_start': 0.22801619051400634}. Best is trial 5 with value: 22.153327006590626.\n",
      "[I 2025-12-16 17:38:01,265] Trial 6 finished with value: 21.678502923718508 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.27406379480917636, 'max_lr': 0.006039594387429501, 'div_factor': 302, 'final_div_factor': 105, 'weight_decay': 0.00010603385586445948, 'pct_start': 0.3223565266241736}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:38:30,965] Trial 7 finished with value: 24.329797078457236 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.2151635566887086, 'max_lr': 0.002843722095943586, 'div_factor': 160, 'final_div_factor': 337, 'weight_decay': 0.0039007643394280873, 'pct_start': 0.12683106704649239}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:39:00,684] Trial 8 finished with value: 23.848853688712694 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.22563112330926788, 'max_lr': 0.0006243705836495852, 'div_factor': 443, 'final_div_factor': 240, 'weight_decay': 0.007048569121793974, 'pct_start': 0.34444815817446245}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:39:30,785] Trial 9 finished with value: 38.690713530207255 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.24015880244518933, 'max_lr': 0.00018761594361623075, 'div_factor': 969, 'final_div_factor': 568, 'weight_decay': 1.071423783475957e-06, 'pct_start': 0.29016511397821365}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:40:00,460] Trial 10 finished with value: 22.465786840641496 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512]', 'fusion_dropout': 0.03857292324813097, 'max_lr': 0.00988037502541179, 'div_factor': 678, 'final_div_factor': 855, 'weight_decay': 3.587586543346222e-05, 'pct_start': 0.10494560384878227}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:40:30,283] Trial 11 finished with value: 22.57803106057447 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.14186188217611337, 'max_lr': 0.001246370090214741, 'div_factor': 14, 'final_div_factor': 496, 'weight_decay': 5.152534850240895e-05, 'pct_start': 0.27317357008458837}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:41:00,838] Trial 12 finished with value: 26.83249727767153 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.2859203636678787, 'max_lr': 8.86127622744447e-05, 'div_factor': 190, 'final_div_factor': 473, 'weight_decay': 0.00040182106604245846, 'pct_start': 0.36507662586345524}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:41:30,044] Trial 13 finished with value: 25.600201176806575 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.15213341829185226, 'max_lr': 0.0015722185247351004, 'div_factor': 49, 'final_div_factor': 692, 'weight_decay': 1.9881091401156963e-05, 'pct_start': 0.168004262869963}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:41:59,738] Trial 14 finished with value: 28.759240829783195 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64]', 'fusion_dropout': 0.24203191472249652, 'max_lr': 9.888640713959302e-05, 'div_factor': 271, 'final_div_factor': 398, 'weight_decay': 0.0012535892586533937, 'pct_start': 0.3007692102649573}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:42:28,803] Trial 15 finished with value: 24.28875969777481 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[]', 'fusion_dropout': 0.007958359594317121, 'max_lr': 0.008146587604730396, 'div_factor': 305, 'final_div_factor': 648, 'weight_decay': 0.0001806458367946527, 'pct_start': 0.24803628218823848}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:42:58,180] Trial 16 finished with value: 23.322540984394262 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,64]', 'fusion_dropout': 0.1679764681253709, 'max_lr': 0.0010031871431334931, 'div_factor': 126, 'final_div_factor': 990, 'weight_decay': 9.637581773448316e-06, 'pct_start': 0.16352564266724506}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:43:27,694] Trial 17 finished with value: 25.08067549335793 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32,32]', 'fusion_dropout': 0.1183246904805818, 'max_lr': 0.003122808492315525, 'div_factor': 369, 'final_div_factor': 400, 'weight_decay': 0.002024510643685818, 'pct_start': 0.25802016994096205}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:43:57,109] Trial 18 finished with value: 27.121604320143703 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128]', 'fusion_dropout': 0.26431892638700355, 'max_lr': 0.00031328379907369006, 'div_factor': 539, 'final_div_factor': 101, 'weight_decay': 9.993182620953651e-05, 'pct_start': 0.3091876532824484}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:44:27,014] Trial 19 finished with value: 21.753212877821348 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.08764224433278424, 'max_lr': 0.0021391297822220693, 'div_factor': 196, 'final_div_factor': 744, 'weight_decay': 0.0006290792056846855, 'pct_start': 0.34392014253007086}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:44:56,592] Trial 20 finished with value: 23.241432498241423 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.07598945756202394, 'max_lr': 0.003076900354516343, 'div_factor': 230, 'final_div_factor': 787, 'weight_decay': 0.0006218422523163084, 'pct_start': 0.3826930027984371}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:45:26,059] Trial 21 finished with value: 22.53676602252209 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,32]', 'fusion_dropout': 0.06588369967639064, 'max_lr': 0.0016224032946494638, 'div_factor': 104, 'final_div_factor': 626, 'weight_decay': 0.0030266717177565338, 'pct_start': 0.3390820103122816}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:45:56,324] Trial 22 finished with value: 24.17838748524638 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128,64]', 'fusion_dropout': 0.25949247998603253, 'max_lr': 0.005433104210012545, 'div_factor': 203, 'final_div_factor': 797, 'weight_decay': 0.00015847337390417798, 'pct_start': 0.3586734376114801}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:46:25,790] Trial 23 finished with value: 23.118767635671837 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.11007541894902192, 'max_lr': 0.0007259991829631726, 'div_factor': 104, 'final_div_factor': 913, 'weight_decay': 0.009455119651581185, 'pct_start': 0.24184379858137284}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:46:55,557] Trial 24 finished with value: 22.35048180387247 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128]', 'fusion_dropout': 0.20789094208101627, 'max_lr': 0.004626263084328249, 'div_factor': 350, 'final_div_factor': 540, 'weight_decay': 0.0005742851231586433, 'pct_start': 0.27643047410610583}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:47:26,191] Trial 25 finished with value: 24.59411357140731 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.0715418292405317, 'max_lr': 0.00039193092539249464, 'div_factor': 249, 'final_div_factor': 721, 'weight_decay': 0.0014366603148612048, 'pct_start': 0.310368909693464}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:47:55,731] Trial 26 finished with value: 21.88706837854333 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,128]', 'fusion_dropout': 0.2989502051397628, 'max_lr': 0.0021459654865364124, 'div_factor': 83, 'final_div_factor': 183, 'weight_decay': 6.543974250879861e-05, 'pct_start': 0.32626557125072553}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:48:25,059] Trial 27 finished with value: 35.05660879652366 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32,16]', 'fusion_dropout': 0.29956012985285807, 'max_lr': 0.002514630770994912, 'div_factor': 441, 'final_div_factor': 202, 'weight_decay': 5.637974899479429e-05, 'pct_start': 0.3400186748420162}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:48:54,449] Trial 28 finished with value: 22.247098926478735 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,32]', 'fusion_dropout': 0.03568435894638061, 'max_lr': 0.0020037596963637214, 'div_factor': 16, 'final_div_factor': 172, 'weight_decay': 1.5940128147701167e-05, 'pct_start': 0.36567362234321377}. Best is trial 6 with value: 21.678502923718508.\n",
      "[I 2025-12-16 17:49:24,453] Trial 29 finished with value: 21.65613656062461 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.27913775196813323, 'max_lr': 0.005903010716267813, 'div_factor': 351, 'final_div_factor': 163, 'weight_decay': 0.00010892386641409295, 'pct_start': 0.32250116834709575}. Best is trial 29 with value: 21.65613656062461.\n",
      "[I 2025-12-16 17:49:53,924] Trial 30 finished with value: 21.216126494662028 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.1970852748742833, 'max_lr': 0.005911245397080099, 'div_factor': 621, 'final_div_factor': 303, 'weight_decay': 0.0002198197552599572, 'pct_start': 0.3820281861437943}. Best is trial 30 with value: 21.216126494662028.\n",
      "[I 2025-12-16 17:50:23,665] Trial 31 finished with value: 21.705670662663607 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.1918378746829421, 'max_lr': 0.0066663935064212206, 'div_factor': 641, 'final_div_factor': 312, 'weight_decay': 0.00021319654639260206, 'pct_start': 0.39909980006693563}. Best is trial 30 with value: 21.216126494662028.\n",
      "[I 2025-12-16 17:50:53,567] Trial 32 finished with value: 20.848216526221584 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.1905909635306487, 'max_lr': 0.007467823276529894, 'div_factor': 638, 'final_div_factor': 313, 'weight_decay': 0.0001839261515506892, 'pct_start': 0.3949266361465979}. Best is trial 32 with value: 20.848216526221584.\n",
      "[I 2025-12-16 17:51:22,899] Trial 33 finished with value: 22.804948450459715 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.1815225303775412, 'max_lr': 0.004216274706970681, 'div_factor': 793, 'final_div_factor': 120, 'weight_decay': 0.00010668451761351075, 'pct_start': 0.3784738471726366}. Best is trial 32 with value: 20.848216526221584.\n",
      "[I 2025-12-16 17:51:52,413] Trial 34 finished with value: 20.79373817088069 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.24602156867199185, 'max_lr': 0.009369857570691986, 'div_factor': 620, 'final_div_factor': 289, 'weight_decay': 2.648153439775945e-05, 'pct_start': 0.3763777969453526}. Best is trial 34 with value: 20.79373817088069.\n",
      "[I 2025-12-16 17:52:22,167] Trial 35 finished with value: 21.618450253369733 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.2532213765972559, 'max_lr': 0.009149487411820263, 'div_factor': 741, 'final_div_factor': 293, 'weight_decay': 5.798196480721933e-06, 'pct_start': 0.3750865887443252}. Best is trial 34 with value: 20.79373817088069.\n",
      "[I 2025-12-16 17:52:52,127] Trial 36 finished with value: 25.83844035540845 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[64,64]', 'fusion_dropout': 0.23521973692546827, 'max_lr': 0.009631568169766054, 'div_factor': 753, 'final_div_factor': 287, 'weight_decay': 3.5278832627587444e-06, 'pct_start': 0.38424964116902643}. Best is trial 34 with value: 20.79373817088069.\n",
      "[I 2025-12-16 17:53:22,134] Trial 37 finished with value: 21.411978162652105 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.25372924422483534, 'max_lr': 0.004284302038980681, 'div_factor': 594, 'final_div_factor': 457, 'weight_decay': 7.581492496581373e-06, 'pct_start': 0.39930155889158675}. Best is trial 34 with value: 20.79373817088069.\n",
      "[I 2025-12-16 17:53:52,316] Trial 38 finished with value: 29.86212110442707 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.1935591790275773, 'max_lr': 3.731972032507283e-05, 'div_factor': 608, 'final_div_factor': 447, 'weight_decay': 2.9251948893435623e-05, 'pct_start': 0.3940218428279691}. Best is trial 34 with value: 20.79373817088069.\n",
      "[I 2025-12-16 17:54:21,901] Trial 39 finished with value: 22.913853402131537 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[512,256]', 'fusion_dropout': 0.21143446419473644, 'max_lr': 0.0038110662289098434, 'div_factor': 841, 'final_div_factor': 429, 'weight_decay': 2.100053048652461e-06, 'pct_start': 0.3563291328628993}. Best is trial 34 with value: 20.79373817088069.\n",
      "[I 2025-12-16 17:54:51,746] Trial 40 finished with value: 20.087807733330433 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.1474103954497869, 'max_lr': 0.006688288664302725, 'div_factor': 590, 'final_div_factor': 349, 'weight_decay': 7.172937510867053e-06, 'pct_start': 0.39377741704711483}. Best is trial 40 with value: 20.087807733330433.\n",
      "[I 2025-12-16 17:55:21,670] Trial 41 finished with value: 21.0243969843296 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.1648575149627523, 'max_lr': 0.006798935256045323, 'div_factor': 583, 'final_div_factor': 346, 'weight_decay': 6.527206437738265e-06, 'pct_start': 0.39612469787283955}. Best is trial 40 with value: 20.087807733330433.\n",
      "[I 2025-12-16 17:55:51,838] Trial 42 finished with value: 20.925398059189863 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.1371835769005791, 'max_lr': 0.006636748596912125, 'div_factor': 498, 'final_div_factor': 339, 'weight_decay': 1.434618747102067e-05, 'pct_start': 0.3717661585335516}. Best is trial 40 with value: 20.087807733330433.\n",
      "[I 2025-12-16 17:56:21,528] Trial 43 finished with value: 20.093216148866716 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.13124102175979374, 'max_lr': 0.007784670394680046, 'div_factor': 480, 'final_div_factor': 354, 'weight_decay': 1.3401035941111845e-05, 'pct_start': 0.36813730242990295}. Best is trial 40 with value: 20.087807733330433.\n",
      "[I 2025-12-16 17:56:50,887] Trial 44 finished with value: 21.41063267748972 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.13975577274518722, 'max_lr': 0.007425881715350049, 'div_factor': 485, 'final_div_factor': 243, 'weight_decay': 1.384813042713865e-05, 'pct_start': 0.3531289452258445}. Best is trial 40 with value: 20.087807733330433.\n",
      "[I 2025-12-16 17:57:20,763] Trial 45 finished with value: 22.650315763786136 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[128,128,64]', 'fusion_dropout': 0.12150041972825693, 'max_lr': 0.0035155822878002703, 'div_factor': 496, 'final_div_factor': 358, 'weight_decay': 2.398778628908438e-05, 'pct_start': 0.37259336381221475}. Best is trial 40 with value: 20.087807733330433.\n",
      "[I 2025-12-16 17:57:50,292] Trial 46 finished with value: 21.74917074793097 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256,256]', 'fusion_dropout': 0.13136607614225468, 'max_lr': 0.009450520714434725, 'div_factor': 676, 'final_div_factor': 257, 'weight_decay': 1.2839234562944546e-05, 'pct_start': 0.3660447469605651}. Best is trial 40 with value: 20.087807733330433.\n",
      "[I 2025-12-16 17:58:20,304] Trial 47 finished with value: 177.90539642777844 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.15718113656704502, 'max_lr': 2.325379098980954e-05, 'div_factor': 524, 'final_div_factor': 506, 'weight_decay': 4.240194214581425e-06, 'pct_start': 0.3286128394548214}. Best is trial 40 with value: 20.087807733330433.\n",
      "[I 2025-12-16 17:58:50,081] Trial 48 finished with value: 22.88056652941599 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[256]', 'fusion_dropout': 0.09434489063998658, 'max_lr': 0.004955515961944556, 'div_factor': 395, 'final_div_factor': 394, 'weight_decay': 3.8088032875435574e-05, 'pct_start': 0.3484287022996957}. Best is trial 40 with value: 20.087807733330433.\n",
      "[I 2025-12-16 17:59:20,099] Trial 49 finished with value: 24.530987508547863 and parameters: {'activation': 'relu', 'fusion_hidden_dims': '[32]', 'fusion_dropout': 0.17643084346529747, 'max_lr': 0.007450624217889762, 'div_factor': 470, 'final_div_factor': 326, 'weight_decay': 9.89436532858739e-06, 'pct_start': 0.19642384287526893}. Best is trial 40 with value: 20.087807733330433.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    attributes=attributes,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    epochs=epochs,\n",
    "    path_vision=path_vision,\n",
    "    path_mlp=path_mlp\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating top-5 trials once at 100 epochs (seed=0)...\n",
      "\n",
      "→ Single-pass full run (Trial 40, ValObjective: 20.0878)\n",
      "\n",
      "Best Trial: 40\n",
      "  Best Score: 20.0878\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.1474103954497869\n",
      "    max_lr: 0.006688288664302725\n",
      "    div_factor: 590\n",
      "    final_div_factor: 349\n",
      "    weight_decay: 7.172937510867053e-06\n",
      "    pct_start: 0.39377741704711483\n",
      "  Params: total=3,480,017  trainable=3,480,017\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3 M     \n",
      "fwd MACs:                                                               841.55 MMACs\n",
      "fwd FLOPs:                                                              1.69 GFLOPS\n",
      "fwd+bwd MACs:                                                           2.52 GMACs\n",
      "fwd+bwd FLOPs:                                                          5.07 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3 M = 100% Params, 841.55 MMACs = 100% MACs, 1.69 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "      (0): Conv2d(864 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 8.96 MFLOPS = 0.53% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0% Params, 0 MACs = 0% MACs, 331.78 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 165.89 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3 M = 99.97% Params, 837.07 MMACs = 99.47% MACs, 1.68 GFLOPS = 99.44% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      13.01 K = 0.43% Params, 66.69 MMACs = 7.92% MACs, 134.37 MFLOPS = 7.95% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.23% Params, 35.83 MMACs = 4.26% MACs, 71.66 MFLOPS = 4.24% FLOPs, 32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        816 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 8.21 MFLOPS = 0.49% FLOPs\n",
      "        (0): Conv2d(768 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 7.96 MFLOPS = 0.47% FLOPs, 32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      32.54 K = 1.09% Params, 41.8 MMACs = 4.97% MACs, 84.11 MFLOPS = 4.98% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 0.35% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 3.11 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      129.6 K = 4.32% Params, 41.8 MMACs = 4.97% MACs, 83.86 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 1.38% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.16% Params, 1.49 MMACs = 0.18% MACs, 3.05 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.15% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      517.25 K = 17.25% Params, 41.8 MMACs = 4.97% MACs, 83.73 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 5.53% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.63% Params, 1.49 MMACs = 0.18% MACs, 3.02 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.61% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.55 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  180.74 K\n",
      "fwd MACs:                                                               180.48 KMACs\n",
      "fwd FLOPs:                                                              361.22 KFLOPS\n",
      "fwd+bwd MACs:                                                           541.44 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.08 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  180.74 K = 100% Params, 180.48 KMACs = 100% MACs, 361.22 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(180.48 K = 99.86% Params, 180.22 KMACs = 99.86% MACs, 360.45 KFLOPS = 99.79% FLOPs, in_features=704, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1474103954497869, inplace=False)\n",
      "  (3): Linear(257 = 0.14% Params, 256 MACs = 0.14% MACs, 512 FLOPS = 0.14% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_40/best_model.pth\n",
      "   val_rmse=20.430042\n",
      "   params: total=3,480,017, trainable=3,480,017\n",
      "→ Single-pass full run (Trial 43, ValObjective: 20.0932)\n",
      "\n",
      "Best Trial: 43\n",
      "  Best Score: 20.0932\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.13124102175979374\n",
      "    max_lr: 0.007784670394680046\n",
      "    div_factor: 480\n",
      "    final_div_factor: 354\n",
      "    weight_decay: 1.3401035941111845e-05\n",
      "    pct_start: 0.36813730242990295\n",
      "  Params: total=3,480,017  trainable=3,480,017\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3 M     \n",
      "fwd MACs:                                                               841.55 MMACs\n",
      "fwd FLOPs:                                                              1.69 GFLOPS\n",
      "fwd+bwd MACs:                                                           2.52 GMACs\n",
      "fwd+bwd FLOPs:                                                          5.07 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3 M = 100% Params, 841.55 MMACs = 100% MACs, 1.69 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "      (0): Conv2d(864 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 8.96 MFLOPS = 0.53% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0% Params, 0 MACs = 0% MACs, 331.78 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 165.89 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3 M = 99.97% Params, 837.07 MMACs = 99.47% MACs, 1.68 GFLOPS = 99.44% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      13.01 K = 0.43% Params, 66.69 MMACs = 7.92% MACs, 134.37 MFLOPS = 7.95% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.23% Params, 35.83 MMACs = 4.26% MACs, 71.66 MFLOPS = 4.24% FLOPs, 32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        816 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 8.21 MFLOPS = 0.49% FLOPs\n",
      "        (0): Conv2d(768 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 7.96 MFLOPS = 0.47% FLOPs, 32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      32.54 K = 1.09% Params, 41.8 MMACs = 4.97% MACs, 84.11 MFLOPS = 4.98% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 0.35% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 3.11 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      129.6 K = 4.32% Params, 41.8 MMACs = 4.97% MACs, 83.86 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 1.38% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.16% Params, 1.49 MMACs = 0.18% MACs, 3.05 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.15% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      517.25 K = 17.25% Params, 41.8 MMACs = 4.97% MACs, 83.73 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 5.53% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.63% Params, 1.49 MMACs = 0.18% MACs, 3.02 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.61% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.55 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  180.74 K\n",
      "fwd MACs:                                                               180.48 KMACs\n",
      "fwd FLOPs:                                                              361.22 KFLOPS\n",
      "fwd+bwd MACs:                                                           541.44 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.08 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  180.74 K = 100% Params, 180.48 KMACs = 100% MACs, 361.22 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(180.48 K = 99.86% Params, 180.22 KMACs = 99.86% MACs, 360.45 KFLOPS = 99.79% FLOPs, in_features=704, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.13124102175979374, inplace=False)\n",
      "  (3): Linear(257 = 0.14% Params, 256 MACs = 0.14% MACs, 512 FLOPS = 0.14% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_43/best_model.pth\n",
      "   val_rmse=23.650964\n",
      "   params: total=3,480,017, trainable=3,480,017\n",
      "→ Single-pass full run (Trial 34, ValObjective: 20.7937)\n",
      "\n",
      "Best Trial: 34\n",
      "  Best Score: 20.7937\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.24602156867199185\n",
      "    max_lr: 0.009369857570691986\n",
      "    div_factor: 620\n",
      "    final_div_factor: 289\n",
      "    weight_decay: 2.648153439775945e-05\n",
      "    pct_start: 0.3763777969453526\n",
      "  Params: total=3,480,017  trainable=3,480,017\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3 M     \n",
      "fwd MACs:                                                               841.55 MMACs\n",
      "fwd FLOPs:                                                              1.69 GFLOPS\n",
      "fwd+bwd MACs:                                                           2.52 GMACs\n",
      "fwd+bwd FLOPs:                                                          5.07 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3 M = 100% Params, 841.55 MMACs = 100% MACs, 1.69 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "      (0): Conv2d(864 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 8.96 MFLOPS = 0.53% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0% Params, 0 MACs = 0% MACs, 331.78 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 165.89 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3 M = 99.97% Params, 837.07 MMACs = 99.47% MACs, 1.68 GFLOPS = 99.44% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      13.01 K = 0.43% Params, 66.69 MMACs = 7.92% MACs, 134.37 MFLOPS = 7.95% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.23% Params, 35.83 MMACs = 4.26% MACs, 71.66 MFLOPS = 4.24% FLOPs, 32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        816 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 8.21 MFLOPS = 0.49% FLOPs\n",
      "        (0): Conv2d(768 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 7.96 MFLOPS = 0.47% FLOPs, 32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      32.54 K = 1.09% Params, 41.8 MMACs = 4.97% MACs, 84.11 MFLOPS = 4.98% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 0.35% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 3.11 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      129.6 K = 4.32% Params, 41.8 MMACs = 4.97% MACs, 83.86 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 1.38% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.16% Params, 1.49 MMACs = 0.18% MACs, 3.05 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.15% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      517.25 K = 17.25% Params, 41.8 MMACs = 4.97% MACs, 83.73 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 5.53% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.63% Params, 1.49 MMACs = 0.18% MACs, 3.02 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.61% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.55 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  180.74 K\n",
      "fwd MACs:                                                               180.48 KMACs\n",
      "fwd FLOPs:                                                              361.22 KFLOPS\n",
      "fwd+bwd MACs:                                                           541.44 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.08 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  180.74 K = 100% Params, 180.48 KMACs = 100% MACs, 361.22 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(180.48 K = 99.86% Params, 180.22 KMACs = 99.86% MACs, 360.45 KFLOPS = 99.79% FLOPs, in_features=704, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.24602156867199185, inplace=False)\n",
      "  (3): Linear(257 = 0.14% Params, 256 MACs = 0.14% MACs, 512 FLOPS = 0.14% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_34/best_model.pth\n",
      "   val_rmse=21.333838\n",
      "   params: total=3,480,017, trainable=3,480,017\n",
      "→ Single-pass full run (Trial 32, ValObjective: 20.8482)\n",
      "\n",
      "Best Trial: 32\n",
      "  Best Score: 20.8482\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.1905909635306487\n",
      "    max_lr: 0.007467823276529894\n",
      "    div_factor: 638\n",
      "    final_div_factor: 313\n",
      "    weight_decay: 0.0001839261515506892\n",
      "    pct_start: 0.3949266361465979\n",
      "  Params: total=3,480,017  trainable=3,480,017\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3 M     \n",
      "fwd MACs:                                                               841.55 MMACs\n",
      "fwd FLOPs:                                                              1.69 GFLOPS\n",
      "fwd+bwd MACs:                                                           2.52 GMACs\n",
      "fwd+bwd FLOPs:                                                          5.07 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3 M = 100% Params, 841.55 MMACs = 100% MACs, 1.69 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "      (0): Conv2d(864 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 8.96 MFLOPS = 0.53% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0% Params, 0 MACs = 0% MACs, 331.78 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 165.89 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3 M = 99.97% Params, 837.07 MMACs = 99.47% MACs, 1.68 GFLOPS = 99.44% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      13.01 K = 0.43% Params, 66.69 MMACs = 7.92% MACs, 134.37 MFLOPS = 7.95% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.23% Params, 35.83 MMACs = 4.26% MACs, 71.66 MFLOPS = 4.24% FLOPs, 32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        816 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 8.21 MFLOPS = 0.49% FLOPs\n",
      "        (0): Conv2d(768 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 7.96 MFLOPS = 0.47% FLOPs, 32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      32.54 K = 1.09% Params, 41.8 MMACs = 4.97% MACs, 84.11 MFLOPS = 4.98% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 0.35% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 3.11 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      129.6 K = 4.32% Params, 41.8 MMACs = 4.97% MACs, 83.86 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 1.38% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.16% Params, 1.49 MMACs = 0.18% MACs, 3.05 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.15% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      517.25 K = 17.25% Params, 41.8 MMACs = 4.97% MACs, 83.73 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 5.53% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.63% Params, 1.49 MMACs = 0.18% MACs, 3.02 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.61% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.55 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  180.74 K\n",
      "fwd MACs:                                                               180.48 KMACs\n",
      "fwd FLOPs:                                                              361.22 KFLOPS\n",
      "fwd+bwd MACs:                                                           541.44 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.08 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  180.74 K = 100% Params, 180.48 KMACs = 100% MACs, 361.22 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(180.48 K = 99.86% Params, 180.22 KMACs = 99.86% MACs, 360.45 KFLOPS = 99.79% FLOPs, in_features=704, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1905909635306487, inplace=False)\n",
      "  (3): Linear(257 = 0.14% Params, 256 MACs = 0.14% MACs, 512 FLOPS = 0.14% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_42/best_model.pth\n",
      "   val_rmse=21.968901\n",
      "   params: total=3,480,017, trainable=3,480,017\n",
      "\n",
      "Winner after single-pass: Trial 40 (trial_40) by val_rmse=20.430042\n",
      "\n",
      "Re-running winner with seeds [0, 1, 2, 3, 4] at 100 epochs...\n",
      "\n",
      "\n",
      "Best Trial: 40\n",
      "  Best Score: 20.0878\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.1474103954497869\n",
      "    max_lr: 0.006688288664302725\n",
      "    div_factor: 590\n",
      "    final_div_factor: 349\n",
      "    weight_decay: 7.172937510867053e-06\n",
      "    pct_start: 0.39377741704711483\n",
      "  Params: total=3,480,017  trainable=3,480,017\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3 M     \n",
      "fwd MACs:                                                               841.55 MMACs\n",
      "fwd FLOPs:                                                              1.69 GFLOPS\n",
      "fwd+bwd MACs:                                                           2.52 GMACs\n",
      "fwd+bwd FLOPs:                                                          5.07 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3 M = 100% Params, 841.55 MMACs = 100% MACs, 1.69 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "      (0): Conv2d(864 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 8.96 MFLOPS = 0.53% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0% Params, 0 MACs = 0% MACs, 331.78 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 165.89 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3 M = 99.97% Params, 837.07 MMACs = 99.47% MACs, 1.68 GFLOPS = 99.44% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      13.01 K = 0.43% Params, 66.69 MMACs = 7.92% MACs, 134.37 MFLOPS = 7.95% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.23% Params, 35.83 MMACs = 4.26% MACs, 71.66 MFLOPS = 4.24% FLOPs, 32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        816 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 8.21 MFLOPS = 0.49% FLOPs\n",
      "        (0): Conv2d(768 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 7.96 MFLOPS = 0.47% FLOPs, 32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      32.54 K = 1.09% Params, 41.8 MMACs = 4.97% MACs, 84.11 MFLOPS = 4.98% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 0.35% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 3.11 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      129.6 K = 4.32% Params, 41.8 MMACs = 4.97% MACs, 83.86 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 1.38% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.16% Params, 1.49 MMACs = 0.18% MACs, 3.05 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.15% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      517.25 K = 17.25% Params, 41.8 MMACs = 4.97% MACs, 83.73 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 5.53% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.63% Params, 1.49 MMACs = 0.18% MACs, 3.02 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.61% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.55 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  180.74 K\n",
      "fwd MACs:                                                               180.48 KMACs\n",
      "fwd FLOPs:                                                              361.22 KFLOPS\n",
      "fwd+bwd MACs:                                                           541.44 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.08 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  180.74 K = 100% Params, 180.48 KMACs = 100% MACs, 361.22 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(180.48 K = 99.86% Params, 180.22 KMACs = 99.86% MACs, 360.45 KFLOPS = 99.79% FLOPs, in_features=704, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1474103954497869, inplace=False)\n",
      "  (3): Linear(257 = 0.14% Params, 256 MACs = 0.14% MACs, 512 FLOPS = 0.14% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_40_seed0/best_model.pth\n",
      "   Seed 0: val_rmse=20.430042, test_loss=616.660172, test_rmse=24.825847, val_loss=419.679108\n",
      "\n",
      "Best Trial: 40\n",
      "  Best Score: 20.0878\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.1474103954497869\n",
      "    max_lr: 0.006688288664302725\n",
      "    div_factor: 590\n",
      "    final_div_factor: 349\n",
      "    weight_decay: 7.172937510867053e-06\n",
      "    pct_start: 0.39377741704711483\n",
      "  Params: total=3,480,017  trainable=3,480,017\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3 M     \n",
      "fwd MACs:                                                               841.55 MMACs\n",
      "fwd FLOPs:                                                              1.69 GFLOPS\n",
      "fwd+bwd MACs:                                                           2.52 GMACs\n",
      "fwd+bwd FLOPs:                                                          5.07 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3 M = 100% Params, 841.55 MMACs = 100% MACs, 1.69 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "      (0): Conv2d(864 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 8.96 MFLOPS = 0.53% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0% Params, 0 MACs = 0% MACs, 331.78 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 165.89 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3 M = 99.97% Params, 837.07 MMACs = 99.47% MACs, 1.68 GFLOPS = 99.44% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      13.01 K = 0.43% Params, 66.69 MMACs = 7.92% MACs, 134.37 MFLOPS = 7.95% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.23% Params, 35.83 MMACs = 4.26% MACs, 71.66 MFLOPS = 4.24% FLOPs, 32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        816 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 8.21 MFLOPS = 0.49% FLOPs\n",
      "        (0): Conv2d(768 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 7.96 MFLOPS = 0.47% FLOPs, 32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      32.54 K = 1.09% Params, 41.8 MMACs = 4.97% MACs, 84.11 MFLOPS = 4.98% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 0.35% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 3.11 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      129.6 K = 4.32% Params, 41.8 MMACs = 4.97% MACs, 83.86 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 1.38% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.16% Params, 1.49 MMACs = 0.18% MACs, 3.05 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.15% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      517.25 K = 17.25% Params, 41.8 MMACs = 4.97% MACs, 83.73 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 5.53% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.63% Params, 1.49 MMACs = 0.18% MACs, 3.02 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.61% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.55 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  180.74 K\n",
      "fwd MACs:                                                               180.48 KMACs\n",
      "fwd FLOPs:                                                              361.22 KFLOPS\n",
      "fwd+bwd MACs:                                                           541.44 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.08 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  180.74 K = 100% Params, 180.48 KMACs = 100% MACs, 361.22 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(180.48 K = 99.86% Params, 180.22 KMACs = 99.86% MACs, 360.45 KFLOPS = 99.79% FLOPs, in_features=704, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1474103954497869, inplace=False)\n",
      "  (3): Linear(257 = 0.14% Params, 256 MACs = 0.14% MACs, 512 FLOPS = 0.14% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_40_seed1/best_model.pth\n",
      "   Seed 1: val_rmse=19.836244, test_loss=608.490316, test_rmse=24.645411, val_loss=397.331535\n",
      "\n",
      "Best Trial: 40\n",
      "  Best Score: 20.0878\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.1474103954497869\n",
      "    max_lr: 0.006688288664302725\n",
      "    div_factor: 590\n",
      "    final_div_factor: 349\n",
      "    weight_decay: 7.172937510867053e-06\n",
      "    pct_start: 0.39377741704711483\n",
      "  Params: total=3,480,017  trainable=3,480,017\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3 M     \n",
      "fwd MACs:                                                               841.55 MMACs\n",
      "fwd FLOPs:                                                              1.69 GFLOPS\n",
      "fwd+bwd MACs:                                                           2.52 GMACs\n",
      "fwd+bwd FLOPs:                                                          5.07 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3 M = 100% Params, 841.55 MMACs = 100% MACs, 1.69 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "      (0): Conv2d(864 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 8.96 MFLOPS = 0.53% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0% Params, 0 MACs = 0% MACs, 331.78 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 165.89 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3 M = 99.97% Params, 837.07 MMACs = 99.47% MACs, 1.68 GFLOPS = 99.44% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      13.01 K = 0.43% Params, 66.69 MMACs = 7.92% MACs, 134.37 MFLOPS = 7.95% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.23% Params, 35.83 MMACs = 4.26% MACs, 71.66 MFLOPS = 4.24% FLOPs, 32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        816 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 8.21 MFLOPS = 0.49% FLOPs\n",
      "        (0): Conv2d(768 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 7.96 MFLOPS = 0.47% FLOPs, 32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      32.54 K = 1.09% Params, 41.8 MMACs = 4.97% MACs, 84.11 MFLOPS = 4.98% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 0.35% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 3.11 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      129.6 K = 4.32% Params, 41.8 MMACs = 4.97% MACs, 83.86 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 1.38% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.16% Params, 1.49 MMACs = 0.18% MACs, 3.05 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.15% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      517.25 K = 17.25% Params, 41.8 MMACs = 4.97% MACs, 83.73 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 5.53% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.63% Params, 1.49 MMACs = 0.18% MACs, 3.02 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.61% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.55 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  180.74 K\n",
      "fwd MACs:                                                               180.48 KMACs\n",
      "fwd FLOPs:                                                              361.22 KFLOPS\n",
      "fwd+bwd MACs:                                                           541.44 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.08 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  180.74 K = 100% Params, 180.48 KMACs = 100% MACs, 361.22 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(180.48 K = 99.86% Params, 180.22 KMACs = 99.86% MACs, 360.45 KFLOPS = 99.79% FLOPs, in_features=704, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1474103954497869, inplace=False)\n",
      "  (3): Linear(257 = 0.14% Params, 256 MACs = 0.14% MACs, 512 FLOPS = 0.14% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_40_seed2/best_model.pth\n",
      "   Seed 2: val_rmse=24.874800, test_loss=778.743637, test_rmse=27.908514, val_loss=617.865845\n",
      "\n",
      "Best Trial: 40\n",
      "  Best Score: 20.0878\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.1474103954497869\n",
      "    max_lr: 0.006688288664302725\n",
      "    div_factor: 590\n",
      "    final_div_factor: 349\n",
      "    weight_decay: 7.172937510867053e-06\n",
      "    pct_start: 0.39377741704711483\n",
      "  Params: total=3,480,017  trainable=3,480,017\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3 M     \n",
      "fwd MACs:                                                               841.55 MMACs\n",
      "fwd FLOPs:                                                              1.69 GFLOPS\n",
      "fwd+bwd MACs:                                                           2.52 GMACs\n",
      "fwd+bwd FLOPs:                                                          5.07 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3 M = 100% Params, 841.55 MMACs = 100% MACs, 1.69 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "      (0): Conv2d(864 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 8.96 MFLOPS = 0.53% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0% Params, 0 MACs = 0% MACs, 331.78 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 165.89 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3 M = 99.97% Params, 837.07 MMACs = 99.47% MACs, 1.68 GFLOPS = 99.44% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      13.01 K = 0.43% Params, 66.69 MMACs = 7.92% MACs, 134.37 MFLOPS = 7.95% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.23% Params, 35.83 MMACs = 4.26% MACs, 71.66 MFLOPS = 4.24% FLOPs, 32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        816 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 8.21 MFLOPS = 0.49% FLOPs\n",
      "        (0): Conv2d(768 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 7.96 MFLOPS = 0.47% FLOPs, 32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      32.54 K = 1.09% Params, 41.8 MMACs = 4.97% MACs, 84.11 MFLOPS = 4.98% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 0.35% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 3.11 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      129.6 K = 4.32% Params, 41.8 MMACs = 4.97% MACs, 83.86 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 1.38% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.16% Params, 1.49 MMACs = 0.18% MACs, 3.05 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.15% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      517.25 K = 17.25% Params, 41.8 MMACs = 4.97% MACs, 83.73 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 5.53% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.63% Params, 1.49 MMACs = 0.18% MACs, 3.02 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.61% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.55 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  180.74 K\n",
      "fwd MACs:                                                               180.48 KMACs\n",
      "fwd FLOPs:                                                              361.22 KFLOPS\n",
      "fwd+bwd MACs:                                                           541.44 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.08 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  180.74 K = 100% Params, 180.48 KMACs = 100% MACs, 361.22 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(180.48 K = 99.86% Params, 180.22 KMACs = 99.86% MACs, 360.45 KFLOPS = 99.79% FLOPs, in_features=704, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1474103954497869, inplace=False)\n",
      "  (3): Linear(257 = 0.14% Params, 256 MACs = 0.14% MACs, 512 FLOPS = 0.14% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_40_seed3/best_model.pth\n",
      "   Seed 3: val_rmse=21.228574, test_loss=655.059336, test_rmse=25.619360, val_loss=452.531423\n",
      "\n",
      "Best Trial: 40\n",
      "  Best Score: 20.0878\n",
      "  Best Hyperparameters:\n",
      "    activation: relu\n",
      "    fusion_hidden_dims: [256]\n",
      "    fusion_dropout: 0.1474103954497869\n",
      "    max_lr: 0.006688288664302725\n",
      "    div_factor: 590\n",
      "    final_div_factor: 349\n",
      "    weight_decay: 7.172937510867053e-06\n",
      "    pct_start: 0.39377741704711483\n",
      "  Params: total=3,480,017  trainable=3,480,017\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  3 M     \n",
      "fwd MACs:                                                               841.55 MMACs\n",
      "fwd FLOPs:                                                              1.69 GFLOPS\n",
      "fwd+bwd MACs:                                                           2.52 GMACs\n",
      "fwd+bwd FLOPs:                                                          5.07 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNetBackboneAnySize(\n",
      "  3 M = 100% Params, 841.55 MMACs = 100% MACs, 1.69 GFLOPS = 100% FLOPs\n",
      "  (stem): UnifiedStem(\n",
      "    928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "    (net): Sequential(\n",
      "      928 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 9.46 MFLOPS = 0.56% FLOPs\n",
      "      (0): Conv2d(864 = 0.03% Params, 4.48 MMACs = 0.53% MACs, 8.96 MFLOPS = 0.53% FLOPs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0% Params, 0 MACs = 0% MACs, 331.78 KFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 165.89 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    3 M = 99.97% Params, 837.07 MMACs = 99.47% MACs, 1.68 GFLOPS = 99.44% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      13.01 K = 0.43% Params, 66.69 MMACs = 7.92% MACs, 134.37 MFLOPS = 7.95% FLOPs\n",
      "      (conv1): Conv2d(6.91 K = 0.23% Params, 35.83 MMACs = 4.26% MACs, 71.66 MFLOPS = 4.24% FLOPs, 32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        816 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 8.21 MFLOPS = 0.49% FLOPs\n",
      "        (0): Conv2d(768 = 0.03% Params, 3.98 MMACs = 0.47% MACs, 7.96 MFLOPS = 0.47% FLOPs, 32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      10.46 K = 0.35% Params, 53.75 MMACs = 6.39% MACs, 108.24 MFLOPS = 6.41% FLOPs\n",
      "      (conv1): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(5.18 K = 0.17% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48 = 0% Params, 0 MACs = 0% MACs, 248.83 KFLOPS = 0.01% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      32.54 K = 1.09% Params, 41.8 MMACs = 4.97% MACs, 84.11 MFLOPS = 4.98% FLOPs\n",
      "      (conv1): Conv2d(10.37 K = 0.35% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        1.25 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 3.11 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(1.15 K = 0.04% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      41.66 K = 1.39% Params, 53.75 MMACs = 6.39% MACs, 107.87 MFLOPS = 6.39% FLOPs\n",
      "      (conv1): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(20.74 K = 0.69% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96 = 0% Params, 0 MACs = 0% MACs, 124.42 KFLOPS = 0.01% FLOPs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      129.6 K = 4.32% Params, 41.8 MMACs = 4.97% MACs, 83.86 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(41.47 K = 1.38% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        4.8 K = 0.16% Params, 1.49 MMACs = 0.18% MACs, 3.05 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(4.61 K = 0.15% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      166.27 K = 5.54% Params, 53.75 MMACs = 6.39% MACs, 107.68 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(82.94 K = 2.77% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(192 = 0.01% Params, 0 MACs = 0% MACs, 62.21 KFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      517.25 K = 17.25% Params, 41.8 MMACs = 4.97% MACs, 83.73 MFLOPS = 4.96% FLOPs\n",
      "      (conv1): Conv2d(165.89 K = 5.53% Params, 13.44 MMACs = 1.6% MACs, 26.87 MFLOPS = 1.59% FLOPs, 96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down): Sequential(\n",
      "        18.82 K = 0.63% Params, 1.49 MMACs = 0.18% MACs, 3.02 MFLOPS = 0.18% FLOPs\n",
      "        (0): Conv2d(18.43 K = 0.61% Params, 1.49 MMACs = 0.18% MACs, 2.99 MFLOPS = 0.18% FLOPs, 96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): BasicBlock(\n",
      "      664.32 K = 22.15% Params, 53.75 MMACs = 6.39% MACs, 107.59 MFLOPS = 6.37% FLOPs\n",
      "      (conv1): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(331.78 K = 11.06% Params, 26.87 MMACs = 3.19% MACs, 53.75 MFLOPS = 3.18% FLOPs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(384 = 0.01% Params, 0 MACs = 0% MACs, 31.1 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 15.55 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (flat): Flatten(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, start_dim=1, end_dim=-1)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  300.03 K\n",
      "fwd MACs:                                                               299.01 KMACs\n",
      "fwd FLOPs:                                                              599.04 KFLOPS\n",
      "fwd+bwd MACs:                                                           897.02 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.8 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  300.03 K = 100% Params, 299.01 KMACs = 100% MACs, 599.04 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(37.38 K = 12.46% Params, 36.86 KMACs = 12.33% MACs, 73.73 KFLOPS = 12.31% FLOPs, in_features=72, out_features=512, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      "  (3): Linear(262.66 K = 87.54% Params, 262.14 KMACs = 87.67% MACs, 524.29 KFLOPS = 87.52% FLOPs, in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 512 FLOPS = 0.09% FLOPs)\n",
      "  (5): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.06627243990133297, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  180.74 K\n",
      "fwd MACs:                                                               180.48 KMACs\n",
      "fwd FLOPs:                                                              361.22 KFLOPS\n",
      "fwd+bwd MACs:                                                           541.44 KMACs\n",
      "fwd+bwd FLOPs:                                                          1.08 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Sequential(\n",
      "  180.74 K = 100% Params, 180.48 KMACs = 100% MACs, 361.22 KFLOPS = 100% FLOPs\n",
      "  (0): Linear(180.48 K = 99.86% Params, 180.22 KMACs = 99.86% MACs, 360.45 KFLOPS = 99.79% FLOPs, in_features=704, out_features=256, bias=True)\n",
      "  (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 256 FLOPS = 0.07% FLOPs)\n",
      "  (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1474103954497869, inplace=False)\n",
      "  (3): Linear(257 = 0.14% Params, 256 MACs = 0.14% MACs, 512 FLOPS = 0.14% FLOPs, in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Best model saved to logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_40_seed4/best_model.pth\n",
      "   Seed 4: val_rmse=22.774929, test_loss=712.784470, test_rmse=26.630137, val_loss=523.237015\n",
      "\n",
      "Winner aggregated val_rmse: 21.828918 ± 2.028568\n",
      "Model params: total=3,480,017, trainable=3,480,017\n",
      "Saved multi-seed summary to: logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_40/winner_multi_seed_summary.txt\n",
      "{'winner_trial_number': 40, 'winner_trial_name': 'trial_40', 'primary_metric': 'val_rmse', 'aggregates': {'train_loss': {'mean': 361.40313421178746, 'std': 108.76207902786336}, 'val_loss': {'mean': 482.1289850870768, 'std': 89.55581433990483}, 'test_loss': {'mean': 674.3475860595703, 'std': 71.43700706376222}, 'min_lr': {'mean': 1.1336082481869026e-05, 'std': 0.0}, 'max_lr': {'mean': 0.006688288664302726, 'std': 9.69739903612216e-19}, 'total_time': {'mean': 58.569864463806155, 'std': 0.7279188165195724}, 'average_epoch_time': {'mean': 0.5837131657600403, 'std': 0.007367884370516536}, 'train_mse': {'mean': 361.32094421386716, 'std': 108.88180379209186}, 'train_mae': {'mean': 14.858864974975585, 'std': 2.144066285037819}, 'train_rmse': {'mean': 18.84375311495743, 'std': 2.7914854370924425}, 'train_r2': {'mean': 0.9579811811447143, 'std': 0.012662117323209168}, 'val_mse': {'mean': 479.7937316894531, 'std': 90.85487098126181}, 'val_mae': {'mean': 17.48644905090332, 'std': 1.546589864309817}, 'val_rmse': {'mean': 21.828918025620418, 'std': 2.028567696737423}, 'val_r2': {'mean': 0.9317252516746521, 'std': 0.012928663145547855}, 'test_mse': {'mean': 673.6239868164063, 'std': 71.25814595755331}, 'test_mae': {'mean': 20.563999938964844, 'std': 1.1610569784761333}, 'test_rmse': {'mean': 25.925853869136084, 'std': 1.357427701853567}, 'test_r2': {'mean': 0.921599543094635, 'std': 0.008293452191771987}, 'total_params': {'mean': 3480017.0, 'std': 0.0}, 'trainable_params': {'mean': 3480017.0, 'std': 0.0}, 'flops': {'mean': 1690171840.0, 'std': 0.0}, 'macs': {'mean': 842029312.0, 'std': 0.0}}, 'total_params': 3480017, 'trainable_params': 3480017, 'flops': 1690171840.0, 'macs': 842029312.0, 'summary_path': 'logs/Regression/Moneyball_cleaned/CNN_hybrid/BIE/best_model/trial_40/winner_multi_seed_summary.txt'}\n"
     ]
    }
   ],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     attributes=attributes,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "     path_vision=path_vision, path_mlp=path_mlp,\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
