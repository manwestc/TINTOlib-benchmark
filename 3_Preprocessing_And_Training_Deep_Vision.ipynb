{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "# Local application/library imports\n",
    "from utils import load_search_space\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    RocCurveDisplay, PrecisionRecallDisplay,\n",
    "    ConfusionMatrixDisplay, roc_auc_score, average_precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'boston'        \n",
    "dataset_subpath = 'Regression/boston'       \n",
    "task_type = 'Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'cmc'        \n",
    "dataset_subpath = 'Multiclass/cmc'       \n",
    "task_type = 'Multiclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'preprocessed_heloc'        \n",
    "dataset_subpath = 'Binary/heloc'       \n",
    "task_type = 'Binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"./data/{dataset_subpath}/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce = True if len(df) > 20000 else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target_tensor(y, task):\n",
    "    task = task.lower()\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "    elif isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "        \n",
    "    if task == \"regression\" or task == \"binary\":\n",
    "        return torch.as_tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    elif task == \"multiclass\":\n",
    "        return torch.as_tensor(y, dtype=torch.long)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from typing import Optional, Tuple, Union\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def _read_split_rgb(images_folder: str, split: str, problem_type: str) -> np.ndarray:\n",
    "    \"\"\"Read RGB uint8 images for a split based on <split>/<problem_type>.csv (column 'images').\"\"\"\n",
    "    csv_path = os.path.join(images_folder, split, f\"{problem_type}.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    img_paths = [os.path.join(images_folder, split, p) for p in df[\"images\"].tolist()]\n",
    "    imgs = []\n",
    "    for p in img_paths:\n",
    "        im = cv2.imread(p, cv2.IMREAD_COLOR)\n",
    "        if im is None:\n",
    "            raise FileNotFoundError(f\"Could not read image: {p}\")\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # HxWx3 uint8\n",
    "        imgs.append(im)\n",
    "    return np.stack(imgs, axis=0)  # [N,H,W,3] uint8 (assumes same size as you stated)\n",
    "\n",
    "def _pad_constant_right_bottom_batch(\n",
    "    imgs_uint8: np.ndarray,\n",
    "    target_size: Union[int, Tuple[int, int]],\n",
    "    fill_rgb01: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Constant pad (right/bottom) to target size with fill = TRAIN mean RGB in [0,1].\n",
    "    Input:  imgs_uint8 [N,H,W,3] uint8\n",
    "    Output: float32 [N,3,Ht,Wt] in [0,1]\n",
    "    \"\"\"\n",
    "    if isinstance(target_size, int):\n",
    "        tw, th = target_size, target_size\n",
    "    else:\n",
    "        tw, th = int(target_size[0]), int(target_size[1])\n",
    "\n",
    "    N = imgs_uint8.shape[0]\n",
    "    out = np.empty((N, 3, th, tw), dtype=np.float32)\n",
    "    fill = fill_rgb01.reshape(1, 1, 3)  # (1,1,3) in [0,1]\n",
    "\n",
    "    for i in range(N):\n",
    "        im01 = imgs_uint8[i].astype(np.float32) / 255.0  # [H,W,3] in [0,1]\n",
    "        h, w, _ = im01.shape\n",
    "        if w > tw or h > th:\n",
    "            raise ValueError(f\"Image {w}x{h} larger than target {tw}x{th}. Increase target_size or resize upstream.\")\n",
    "        canvas = np.empty((th, tw, 3), dtype=np.float32)\n",
    "        canvas[:] = fill\n",
    "        canvas[:h, :w, :] = im01\n",
    "        out[i] = np.transpose(canvas, (2, 0, 1))\n",
    "    return out\n",
    "\n",
    "def load_and_preprocess_data(\n",
    "    df, dataset_name, images_folder,\n",
    "    problem_type, task_type,\n",
    "    seed: int = 42, batch_size: int = 32, device: str = 'cpu',\n",
    "    pad_images: bool = False, target_size: Optional[Union[int, Tuple[int, int]]] = None,\n",
    "):\n",
    "    task_type = task_type.lower()\n",
    "\n",
    "    # ----- Config -----\n",
    "    with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "        config = json.load(f)\n",
    "    categorical_cols = config[\"categorical_cols\"]\n",
    "    numerical_cols = config[\"numerical_cols\"]\n",
    "    encoding = config[\"encoding\"]\n",
    "\n",
    "    # ----- Features / target -----\n",
    "    X = df[numerical_cols + categorical_cols].copy()\n",
    "    y = df.iloc[:, -1].copy()\n",
    "\n",
    "    le = None\n",
    "    if encoding.get(\"target\") == \"label\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        label_mapping = None\n",
    "\n",
    "    # ----- Splits (70/15/15) -----\n",
    "    if task_type == \"regression\":\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "        X_val_raw,   X_test_raw, y_val,  y_test  = train_test_split(X_temp_raw, y_temp, test_size=0.5, random_state=seed)\n",
    "    else:\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "        X_val_raw,   X_test_raw, y_val,  y_test  = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed, stratify=y_temp\n",
    "        )\n",
    "\n",
    "    # ----- Class weights (optional) -----\n",
    "    class_weight = None\n",
    "    if task_type in [\"binary\", \"multiclass\"]:\n",
    "        cw_vals = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "        classes_sorted = np.sort(np.unique(y_train))\n",
    "        if task_type == \"binary\":\n",
    "            wd = dict(zip(classes_sorted, cw_vals))\n",
    "            pos_weight = wd[1] / wd[0]\n",
    "            class_weight = torch.tensor(pos_weight, dtype=torch.float32)\n",
    "            print(f\"Binary pos_weight (for BCEWithLogitsLoss): {class_weight.item():.6f}\")\n",
    "        else:\n",
    "            class_weight = torch.tensor(cw_vals, dtype=torch.float32)\n",
    "            print(f\"Multiclass class weights (for CrossEntropyLoss): {class_weight.tolist()}\")\n",
    "\n",
    "    # ----- ColumnTransformer (fit on TRAIN only) -----\n",
    "    transformers = []\n",
    "    if encoding.get(\"numerical_features\") == \"minmax\":\n",
    "        transformers.append((\"num\", MinMaxScaler(), numerical_cols))\n",
    "    elif encoding.get(\"numerical_features\") == \"standard\":\n",
    "        transformers.append((\"num\", StandardScaler(), numerical_cols))\n",
    "    if categorical_cols and encoding.get(\"categorical_features\") == \"onehot\":\n",
    "        transformers.append((\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), categorical_cols))\n",
    "\n",
    "    if transformers:\n",
    "        preprocessor = ColumnTransformer(transformers=transformers)\n",
    "        X_train = preprocessor.fit_transform(X_train_raw)\n",
    "        X_val   = preprocessor.transform(X_val_raw)\n",
    "        X_test  = preprocessor.transform(X_test_raw)\n",
    "\n",
    "        if \"cat\" in preprocessor.named_transformers_:\n",
    "            cat_feature_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    "            all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "        else:\n",
    "            all_feature_names = numerical_cols + categorical_cols\n",
    "\n",
    "        X_train_num = pd.DataFrame(X_train, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val_num   = pd.DataFrame(X_val,   columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test_num  = pd.DataFrame(X_test,  columns=all_feature_names, index=X_test_raw.index)\n",
    "    else:\n",
    "        all_feature_names = numerical_cols + categorical_cols\n",
    "        X_train_num = pd.DataFrame(X_train_raw, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val_num   = pd.DataFrame(X_val_raw,   columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test_num  = pd.DataFrame(X_test_raw,  columns=all_feature_names, index=X_test_raw.index)\n",
    "\n",
    "    print(f\"Shapes — Train: {X_train_num.shape}, Val: {X_val_num.shape}, Test: {X_test_num.shape}\")\n",
    "    print(f\"Numerical features: {len(numerical_cols)} — {numerical_cols}\")\n",
    "    print(f\"Categorical features: {len(categorical_cols)} — {categorical_cols}\")\n",
    "    print(f\"Total features: {X_train_num.shape[1]}\")\n",
    "    if label_mapping:\n",
    "        print(f\"Target label mapping: {label_mapping}\")\n",
    "\n",
    "    # ----- Images (uint8 RGB) -----\n",
    "    X_train_img_u8 = _read_split_rgb(images_folder, \"train\", problem_type)\n",
    "    X_val_img_u8   = _read_split_rgb(images_folder, \"val\",   problem_type)\n",
    "    X_test_img_u8  = _read_split_rgb(images_folder, \"test\",  problem_type)\n",
    "\n",
    "    # ----- Optional padding with TRAIN mean (no normalization) -----\n",
    "    if pad_images:\n",
    "        if target_size is None:\n",
    "            raise ValueError(\"pad_images=True requires target_size (int or (W,H)).\")\n",
    "        train_mean_rgb01 = (X_train_img_u8.astype(np.float32) / 255.0).reshape(-1, 3).mean(axis=0).astype(np.float32)\n",
    "        X_train_arr = _pad_constant_right_bottom_batch(X_train_img_u8, target_size, train_mean_rgb01)\n",
    "        X_val_arr   = _pad_constant_right_bottom_batch(X_val_img_u8,   target_size, train_mean_rgb01)\n",
    "        X_test_arr  = _pad_constant_right_bottom_batch(X_test_img_u8,  target_size, train_mean_rgb01)\n",
    "\n",
    "        if isinstance(target_size, int):\n",
    "            tw = th = int(target_size)\n",
    "        else:\n",
    "            tw, th = int(target_size[0]), int(target_size[1])\n",
    "        imgs_shape = (3, th, tw)\n",
    "    else:\n",
    "        # Scale to [0,1] and convert to NCHW.\n",
    "        X_train_arr = (X_train_img_u8.astype(np.float32) / 255.0).transpose(0, 3, 1, 2)\n",
    "        X_val_arr   = (X_val_img_u8.astype(np.float32)   / 255.0).transpose(0, 3, 1, 2)\n",
    "        X_test_arr  = (X_test_img_u8.astype(np.float32)  / 255.0).transpose(0, 3, 1, 2)\n",
    "        _, C, H, W = X_train_arr.shape\n",
    "        imgs_shape = (C, H, W)\n",
    "\n",
    "    # ----- Tensors & DataLoaders -----\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor   = torch.as_tensor(X_val_num.values,   dtype=torch.float32)\n",
    "    X_test_num_tensor  = torch.as_tensor(X_test_num.values,  dtype=torch.float32)\n",
    "\n",
    "    X_train_img_tensor = torch.from_numpy(X_train_arr)  # float32 [0,1], NCHW\n",
    "    X_val_img_tensor   = torch.from_numpy(X_val_arr)\n",
    "    X_test_img_tensor  = torch.from_numpy(X_test_arr)\n",
    "\n",
    "    y_train_tensor = prepare_target_tensor(y_train, task_type)\n",
    "    y_val_tensor   = prepare_target_tensor(y_val,   task_type)\n",
    "    y_test_tensor  = prepare_target_tensor(y_test,  task_type)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset   = TensorDataset(X_val_img_tensor,   y_val_tensor)\n",
    "    test_dataset  = TensorDataset(X_test_img_tensor,  y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    attributes = X_train_num.shape[1]\n",
    "    print(\"Images shape (C,H,W):\", imgs_shape)\n",
    "    print(\"Attributes:\", attributes)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape, le, class_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Check to include both divisors if they are not the same\n",
    "                divisors.append(n // i)\n",
    "    divisors.sort()\n",
    "    return divisors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit_pytorch.vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMLP(nn.Module):\n",
    "    def __init__(self, imgs_shape, params, task, num_classes=None):\n",
    "        super(ViTMLP, self).__init__()\n",
    "\n",
    "        # Vision Transformer branch\n",
    "        self.vit = ViT(\n",
    "            image_size=imgs_shape,\n",
    "            patch_size=params[\"patch_size\"],\n",
    "            dim=params[\"dim\"],\n",
    "            depth=params[\"depth\"],\n",
    "            heads=params[\"heads\"],\n",
    "            mlp_dim=params[\"mlp_dim\"]*params[\"dim\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            emb_dropout=params[\"emb_dropout\"]\n",
    "        )\n",
    "\n",
    "        # MLP branch\n",
    "        mlp_layers = []\n",
    "        input_dim = params[\"dim\"]\n",
    "        for hidden_dim in params[\"mlp_hidden_dims\"]:\n",
    "            mlp_layers.append(nn.Linear(input_dim, int(params[\"dim\"]*hidden_dim)))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            input_dim = int(params[\"dim\"]*hidden_dim)\n",
    "\n",
    "        # Determine output layer\n",
    "        output_dim = 1 if task in ['regression', 'binary'] else num_classes\n",
    "        mlp_layers.append(nn.Linear(input_dim, output_dim))\n",
    "        self.mlp = nn.Sequential(*mlp_layers) \n",
    "\n",
    "        # Change identity to something else if needed\n",
    "        self.activation = nn.Identity()\n",
    "\n",
    "    def forward(self, vit_input):\n",
    "        x = self.vit(vit_input)\n",
    "        x = self.mlp(x)\n",
    "        return self.activation(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Reuse your get_act helper\n",
    "def get_act(name: str):\n",
    "    return nn.ReLU if str(name).lower() == \"relu\" else nn.GELU\n",
    "\n",
    "# ---------------- Stem ----------------\n",
    "class UnifiedStem(nn.Module):\n",
    "    \"\"\"\n",
    "    - '3x3' stem: safe for tiny images (3x3, 5x5, 32x32).\n",
    "    - '7x7' stem (+ optional maxpool): classic ImageNet style for large images.\n",
    "    Only apply 7x7+stride2 when max(H,W) >= 64; otherwise fallback to 3x3.\n",
    "    \"\"\"\n",
    "    def __init__(self, C, stem_width, stem_type=\"3x3\", use_maxpool=True, H=None, W=None):\n",
    "        super().__init__()\n",
    "        large = (max(H or 0, W or 0) >= 64)\n",
    "        if stem_type == \"7x7\" and large:\n",
    "            layers = [\n",
    "                nn.Conv2d(C, stem_width, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "                nn.BatchNorm2d(stem_width),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1) if use_maxpool else nn.Identity(),\n",
    "            ]\n",
    "        else:\n",
    "            layers = [\n",
    "                nn.Conv2d(C, stem_width, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(stem_width),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --------------- Basic Block ---------------\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.down is not None:\n",
    "            identity = self.down(identity)\n",
    "        out = self.relu(out + identity)\n",
    "        return out\n",
    "\n",
    "# --------------- ResNet Backbone (features only) ---------------\n",
    "class ResNetBackboneAnySize(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic ResNet (BasicBlocks), size-agnostic via AdaptiveAvgPool2d(1).\n",
    "    Outputs a feature vector (no classifier).\n",
    "    \"\"\"\n",
    "    def __init__(self, params, imgs_shape):\n",
    "        super().__init__()\n",
    "        C, H, W = imgs_shape\n",
    "        assert params[\"in_channels\"] == C, \"in_channels must match imgs_shape[0]\"\n",
    "\n",
    "        # knobs\n",
    "        stem_type     = params[\"stem_type\"]\n",
    "        use_maxpool   = params[\"use_maxpool\"]\n",
    "        stem_width    = params[\"stem_width\"]\n",
    "        blocks_ps     = params[\"blocks_per_stage\"]  # e.g., \"[2,2,2,2]\"\n",
    "        n_stages      = len(blocks_ps)\n",
    "        base_width    = params[\"base_width\"]\n",
    "        width_mul     = params[\"width_mul\"]\n",
    "\n",
    "        # stem\n",
    "        self.stem = UnifiedStem(C, stem_width, stem_type=stem_type, use_maxpool=use_maxpool, H=H, W=W)\n",
    "\n",
    "        # stage widths\n",
    "        B = int(base_width * width_mul)\n",
    "        all_out = [B, B*2, B*4, B*8][:n_stages]\n",
    "        blocks_ps = [max(1, int(x)) for x in list(blocks_ps)[:n_stages]]\n",
    "\n",
    "        # approximate current spatial size after stem\n",
    "        curH, curW = H, W\n",
    "        if stem_type == \"7x7\" and max(H, W) >= 64:\n",
    "            curH = max(1, curH // 2)\n",
    "            curW = max(1, curW // 2)\n",
    "            if use_maxpool:\n",
    "                curH = max(1, curH // 2)\n",
    "                curW = max(1, curW // 2)\n",
    "\n",
    "        in_planes = stem_width\n",
    "        layers = []\n",
    "\n",
    "        def can_downsample(h, w):\n",
    "            return (h >= 4 and w >= 4)\n",
    "\n",
    "        for si in range(n_stages):\n",
    "            out_planes = all_out[si]\n",
    "            n_blocks   = blocks_ps[si]\n",
    "            stride = 2 if (si > 0 and can_downsample(curH, curW)) else 1\n",
    "            layers.append(BasicBlock(in_planes, out_planes, stride=stride))\n",
    "            in_planes = out_planes\n",
    "            if stride == 2:\n",
    "                curH = max(1, curH // 2)\n",
    "                curW = max(1, curW // 2)\n",
    "            for _ in range(n_blocks - 1):\n",
    "                layers.append(BasicBlock(in_planes, out_planes, stride=1))\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        # infer feature dim\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, C, max(1, H), max(1, W))\n",
    "            x = self.stem(dummy)\n",
    "            x = self.features(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.flat(x)\n",
    "            self.feat_dim = x.shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat(x)\n",
    "        return x  # (B, feat_dim)\n",
    "\n",
    "# CNN\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN backbone -> MLP head (mirrors ViTMLP: encoder + MLP head).\n",
    "    Uses your ResNetBackboneAnySize.\n",
    "    \"\"\"\n",
    "    def __init__(self, imgs_shape, params, task, num_classes=None, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.task = task.lower()\n",
    "        act = get_act(params.get(\"activation\", \"relu\"))\n",
    "\n",
    "        # Backbone (reuse your class; params must include cnn knobs)\n",
    "        self.backbone = ResNetBackboneAnySize(params, imgs_shape)\n",
    "        feat_dim = self.backbone.feat_dim\n",
    "            \n",
    "        mlp_layers = []\n",
    "        input_dim = feat_dim\n",
    "        for hidden_dim in params[\"mlp_hidden_dims\"]:\n",
    "            mlp_layers.append(nn.Linear(input_dim, int(feat_dim*hidden_dim)))\n",
    "            mlp_layers.append(act())\n",
    "            input_dim = int(feat_dim*hidden_dim)\n",
    "\n",
    "        out_dim = 1 if self.task in (\"regression\", \"binary\") else num_classes\n",
    "        mlp_layers.append(nn.Linear(input_dim, out_dim))\n",
    "        self.head = nn.Sequential(*mlp_layers)\n",
    "\n",
    "        self.activation = nn.Identity()\n",
    "\n",
    "    def forward(self, img_input):\n",
    "        x = self.backbone(img_input)\n",
    "        x = self.head(x)\n",
    "        return self.activation(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet50(nn.Module):\n",
    "    def __init__(self, task_type, num_classes=None):\n",
    "        super(resnet50, self).__init__()\n",
    "\n",
    "        self.resnet = ResNet50(\n",
    "            task_type=task_type,\n",
    "            num_classes=num_classes,\n",
    "            weights=None\n",
    "        )\n",
    "\n",
    "        # Change identity to something else if needed\n",
    "        self.activation = nn.Identity()\n",
    "\n",
    "    def forward(self, resnet_input):\n",
    "        x = self.resnet(resnet_input)\n",
    "\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "\n",
    "from models.utils import get_loss_fn, calculate_metrics, calculate_metrics_from_numpy, get_class_weighted_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "def compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, \n",
    "                    model_name, image_name, trial_name=None, task='regression', epochs=200, max_lr=1, \n",
    "                    div_factor=10, final_div_factor=1, device='cuda', weight_decay=1e-2, pct_start=0.3, save_model=False, class_weights=None, save_dir=None, study=None, patch=None, verbose=False):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if class_weights != None:\n",
    "        loss_fn = get_class_weighted_loss_fn(task, class_weights)\n",
    "    else:\n",
    "        loss_fn = get_loss_fn(task)\n",
    "\n",
    "    # Compute min_lr from max_lr and div_factor\n",
    "    min_lr = max_lr / div_factor\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=div_factor, final_div_factor=final_div_factor, total_steps=total_steps, pct_start=pct_start, anneal_strategy=\"cos\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "    #early_stopping_counter = 0\n",
    "    #patience = 10  # Early stopping patience\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    if task == 'regression':\n",
    "        history.update({'train_mse': [],  'val_mse': [], 'train_mae': [],  'val_mae': [], 'train_rmse': [], 'val_rmse': [], 'train_r2': [], 'val_r2': []})\n",
    "    elif task in ['binary', 'multiclass']:\n",
    "        history.update({'train_accuracy': [], 'val_accuracy': [], 'train_precision': [], 'val_precision': [], 'train_recall': [], 'val_recall': [], 'train_f1': [], 'val_f1': []})\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "\n",
    "        for img_data, targets in train_loader:\n",
    "            img_data, targets = img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        if task == 'multiclass':\n",
    "            y_train_pred = np.vstack(train_preds)\n",
    "            y_train_true = train_targets\n",
    "        else:\n",
    "            y_train_pred = np.concatenate(train_preds)\n",
    "            y_train_true = np.concatenate(train_targets)\n",
    "        train_metrics = calculate_metrics_from_numpy(y_train_true, y_train_pred, task)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for img_data, targets in val_loader:\n",
    "                img_data, targets = img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                outputs = model(img_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        if task == 'multiclass':\n",
    "            y_val_pred = np.vstack(val_preds)\n",
    "            y_val_true = val_targets\n",
    "        else:\n",
    "            y_val_pred = np.concatenate(val_preds)\n",
    "            y_val_true = np.concatenate(val_targets)\n",
    "        \n",
    "        val_metrics = calculate_metrics_from_numpy(y_val_true, y_val_pred, task)\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "        for k, v in train_metrics.items():\n",
    "            history[f'train_{k}'].append(v)\n",
    "        for k, v in val_metrics.items():\n",
    "            history[f'val_{k}'].append(v)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "            #early_stopping_counter = 0\n",
    "        #else:\n",
    "        #    early_stopping_counter += 1\n",
    "        #    if early_stopping_counter >= patience:\n",
    "        #        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        #        break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Recompute metrics using the best model\n",
    "    train_metrics, y_true_train, y_pred_train, y_prob_train = calculate_metrics(model, train_loader, device, class_weights, task)\n",
    "    val_metrics, y_true_val, y_pred_val, y_prob_val  = calculate_metrics(model, val_loader, device, class_weights, task)\n",
    "    test_metrics, y_true_test, y_pred_test, y_prob_test = calculate_metrics(model, test_loader, device, class_weights, task)\n",
    "\n",
    "    # Store recomputed metrics\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "\n",
    "    # Add task-specific metrics\n",
    "    for k in train_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'train_{k}'] = train_metrics[k]\n",
    "    for k in val_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'val_{k}'] = val_metrics[k]\n",
    "    for k in test_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'test_{k}'] = test_metrics[k]\n",
    "        \n",
    "    if verbose:   \n",
    "        print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "        print(f\"Best model found at epoch {best_epoch}/{epochs}\")\n",
    "        print(f\"Best Train Loss: {metrics['train_loss']:.4f}, Best Val Loss: {metrics['val_loss']:.4f}\")\n",
    "        print(metrics)\n",
    "    \n",
    "    if save_model:\n",
    "        if model_name == \"CNN\":\n",
    "            save_path = os.path.join(save_dir, f\"{model_name}/{image_name}/best_model/{trial_name}\")\n",
    "        else:\n",
    "            save_path = os.path.join(save_dir, f\"{model_name}/{image_name}/best_model/{trial_name}\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        plot_metric(history['train_loss'], history['val_loss'], 'Loss', save_path)\n",
    "        if task == 'regression':\n",
    "            plot_metric(history['train_mse'], history['val_mse'], 'MSE', save_path)\n",
    "            plot_metric(history['train_rmse'], history['val_rmse'], 'RMSE', save_path)\n",
    "        else:\n",
    "            plot_metric(history['train_accuracy'], history['val_accuracy'], 'Accuracy', save_path)\n",
    "            plot_metric(history['train_f1'], history['val_f1'], 'F1', save_path)\n",
    "\n",
    "        plot_learning_rate(history['learning_rate'], save_path)\n",
    "\n",
    "        # Save metrics\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        with open(f'{save_path}/best_model_metrics.txt', 'w') as f:\n",
    "            for key, value in metrics.items():\n",
    "                f.write(f'{key}: {value}\\n')\n",
    "\n",
    "        # Save model\n",
    "        torch.save(best_model, f\"{save_path}/best_model.pth\")\n",
    "        print(f\"Best model saved to {save_path}/best_model.pth\")\n",
    "\n",
    "        # Additional plots for classification\n",
    "        if task in [\"binary\"]:\n",
    "            plot_extra(\"Train\", y_true_train, y_pred_train, y_prob_train, save_path)\n",
    "            plot_extra(\"Validation\", y_true_val, y_pred_val, y_prob_val, save_path)\n",
    "            plot_extra(\"Test\", y_true_test, y_pred_test, y_prob_test, save_path)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_extra(split_name, y_true, y_pred, y_prob, save_path):\n",
    "    y_true = y_true.ravel()\n",
    "    y_pred = y_pred.ravel()\n",
    "\n",
    "    # ROC Curve\n",
    "    RocCurveDisplay.from_predictions(y_true, y_prob)\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
    "    plt.title(f\"{split_name} ROC Curve (AUC = {auc_score:.2f})\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_roc_curve.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    PrecisionRecallDisplay.from_predictions(y_true, y_prob)\n",
    "    avg_prec = average_precision_score(y_true, y_prob)\n",
    "    plt.title(f\"{split_name} PR Curve (AP = {avg_prec:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_pr_curve.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Normalized confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true').plot(cmap='Blues')\n",
    "    plt.title(f\"{split_name} Confusion Matrix (Normalized)\")\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_confusion_matrix_normalized.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Raw confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize=None).plot(cmap='Blues')\n",
    "    plt.title(f\"{split_name} Confusion Matrix (Counts)\")\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_confusion_matrix_counts.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "\n",
    "def plot_metric(train_metric, val_metric, metric_name, save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(train_metric, label=f'Train {metric_name}')\n",
    "    plt.plot(val_metric, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric_name} vs. Epoch')\n",
    "    save_path = f\"{save_path}/{metric_name.lower()}_plot.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(\"all\")\n",
    "\n",
    "def plot_learning_rate(learning_rates, save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate vs. Epoch')\n",
    "    save_path = f\"{save_path}/learning_rate_plot.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir =  os.path.join(\"logs\", task_type, dataset_name)\n",
    "model_name = \"vit\"\n",
    "\n",
    "# Load config\n",
    "with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "batch_size = config[\"batch_size\"]\n",
    "epochs = 50\n",
    "n_trials = 100\n",
    "\n",
    "if task_type.lower() == 'multiclass':\n",
    "    num_classes = df.iloc[:,-1].nunique()\n",
    "else:\n",
    "    num_classes = 1\n",
    "\n",
    "device='cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir =  os.path.join(\"logs\", task_type, dataset_name)\n",
    "model_name = \"CNN\"\n",
    "\n",
    "# Load config\n",
    "with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "batch_size = config[\"batch_size\"]\n",
    "epochs = 50\n",
    "n_trials = 100\n",
    "\n",
    "if task_type.lower() == 'multiclass':\n",
    "    num_classes = df.iloc[:,-1].nunique()\n",
    "else:\n",
    "    num_classes = 1\n",
    "\n",
    "device='cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ small helpers ------------------\n",
    "def _as_list(x):\n",
    "    return json.loads(x) if isinstance(x, str) else list(x)\n",
    "\n",
    "def _align_blocks(blocks, n_stages):\n",
    "    blocks = list(blocks)\n",
    "    if len(blocks) < n_stages:\n",
    "        blocks = blocks + [blocks[-1]] * (n_stages - len(blocks))  # pad with last\n",
    "    elif len(blocks) > n_stages:\n",
    "        blocks = blocks[:n_stages]                                 # truncate\n",
    "    return [max(1, int(b)) for b in blocks]\n",
    "\n",
    "def _can_stride2(h, w):\n",
    "    # allow stride-2 only if AFTER halving we still have >= 2x2\n",
    "    return (h // 2) >= 2 and (w // 2) >= 2\n",
    "\n",
    "def _est_min_size_safe(H, W, stem_type, use_maxpool, n_stages):\n",
    "    h, w = int(H), int(W)\n",
    "\n",
    "    # stem downsampling only for large inputs with 7x7\n",
    "    if stem_type == \"7x7\" and max(H, W) >= 64:\n",
    "        h //= 2; w //= 2\n",
    "        if use_maxpool:\n",
    "            h //= 2; w //= 2\n",
    "\n",
    "    # per-stage: at most one stride-2 at stage start, only if safe\n",
    "    for si in range(n_stages):\n",
    "        if si > 0 and _can_stride2(h, w):\n",
    "            h //= 2; w //= 2\n",
    "\n",
    "    return h, w\n",
    "\n",
    "\n",
    "def _count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model_name, image_name, task_type, \n",
    "              train_loader, val_loader, test_loader,\n",
    "              divisors, imgs_shape, num_classes=None,\n",
    "              device='cuda', save_dir=None, class_weight=None, epochs=100, path_vit=None):\n",
    "    \n",
    "    task = task_type.lower()\n",
    "    \n",
    "    if model_name == \"vit\":\n",
    "\n",
    "        params = load_search_space(model_name, trial)\n",
    "\n",
    "        params[\"patch_size\"] = trial.suggest_categorical(\"patch_size\", divisors)\n",
    "\n",
    "        params[\"mlp_hidden_dims\"] = json.loads(params[\"mlp_hidden_dims\"])\n",
    "\n",
    "        # prune invalid attention shapes early\n",
    "        if params[\"dim\"] % params[\"heads\"] != 0:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        with open(f\"configs/optuna_search/{model_name}.json\", \"r\") as f:\n",
    "            full_config = json.load(f)\n",
    "\n",
    "        config = full_config[model_name][\"fit\"]  # Access the model key\n",
    "\n",
    "        # Build and train model\n",
    "        model = ViTMLP(imgs_shape[1], params, task, num_classes)\n",
    "    else:\n",
    "        # --- CNN branch (ResNet-style) ---\n",
    "        params = load_search_space(model_name, trial)\n",
    "        \n",
    "        with open(f\"{path_vit}/best_params.json\", \"r\") as f:\n",
    "                best_vit = json.load(f)\n",
    "\n",
    "        best_vit[\"total_params\"]\n",
    "\n",
    "        # parse head dims safely (keeps your JSON format)\n",
    "        params[\"mlp_hidden_dims\"] = json.loads(params[\"mlp_hidden_dims\"])\n",
    "        \n",
    "                    \n",
    "        params[\"blocks_per_stage\"] = json.loads(params[\"blocks_per_stage\"])\n",
    "        \n",
    "        # types\n",
    "        stem_type  = params[\"stem_type\"]\n",
    "        use_maxpool= params[\"use_maxpool\"]\n",
    "        stem_width = params[\"stem_width\"]\n",
    "        base_width = params[\"base_width\"]\n",
    "        width_mul  = float(params[\"width_mul\"])\n",
    "        blocks_list = params[\"blocks_per_stage\"]\n",
    "        n_stages   = len(blocks_list)\n",
    "\n",
    "        # 1) forbid 7x7 stem on tiny images\n",
    "        _, H, W = imgs_shape  # imgs_shape = (C,H,W)\n",
    "        if stem_type == \"7x7\" and max(H, W) < 64:\n",
    "            raise optuna.TrialPruned(\"7x7 stem on small images (<64)\")\n",
    "            \n",
    "        # Forbid 3×3 on very large images (too fine, too heavy)\n",
    "        if stem_type == \"3x3\" and max(H, W) >= 224:\n",
    "            raise optuna.TrialPruned(\"3x3 stem on very large images (>224)\")\n",
    "\n",
    "        # 2) worst-case collapse check (avoid < 2x2)\n",
    "        minH, minW = _est_min_size_safe(H, W, stem_type, use_maxpool, n_stages)\n",
    "        if min(minH, minW) < 2:\n",
    "            raise optuna.TrialPruned(f\"downsampling collapses spatial to {minH}x{minW} < 2x2\")\n",
    "\n",
    "        with open(f\"configs/optuna_search/{model_name}.json\", \"r\") as f:\n",
    "            full_config = json.load(f)\n",
    "        config = full_config[model_name][\"fit\"]\n",
    "\n",
    "        # Build and train model\n",
    "        model = CNN(imgs_shape, params, task, num_classes)\n",
    "        \n",
    "        # 5) capacity-match prune vs best ViT params\n",
    "        num_params_vit = float(best_vit[\"total_params\"])\n",
    "        num_params_cnn = _count_params(model)\n",
    "\n",
    "        tol = 0.25  # allow up to +25%\n",
    "        if num_params_cnn > (1.0 + tol) * num_params_vit:\n",
    "            raise optuna.TrialPruned(\n",
    "                f\"params {num_params_cnn} > {int((1.0+tol)*100)}% of {int(num_params_vit)}\"\n",
    "            )\n",
    "    \n",
    "    metrics = compile_and_fit(\n",
    "        model,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=f\"trial_{trial.number}\",\n",
    "        image_name=image_name,\n",
    "        task=task,  # assumed to be defined externally\n",
    "        max_lr=trial.suggest_float(\"max_lr\", config[\"max_lr\"][1], config[\"max_lr\"][2], log=True),\n",
    "        div_factor=trial.suggest_int(\"div_factor\", config[\"div_factor\"][1], config[\"div_factor\"][2]),\n",
    "        final_div_factor=trial.suggest_int(\"final_div_factor\", config[\"final_div_factor\"][1], config[\"final_div_factor\"][2]),\n",
    "        weight_decay=trial.suggest_float(\"weight_decay\", config[\"weight_decay\"][1], config[\"weight_decay\"][2], log=True),\n",
    "        pct_start=trial.suggest_float(\"pct_start\", config[\"pct_start\"][1], config[\"pct_start\"][2]),\n",
    "        epochs=epochs,\n",
    "        save_model=False,\n",
    "        class_weights=class_weight\n",
    "    )\n",
    "\n",
    "    save_dir = os.path.join(save_dir, model_name, image_name, \"optuna\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if task == 'regression':\n",
    "        score = metrics[\"val_rmse\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-RMSE: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    elif task == 'binary':\n",
    "        score = metrics[\"val_roc_auc\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-AUC: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    elif task == 'multiclass':\n",
    "        score = metrics[\"val_accuracy\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-Accuracy: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "    \n",
    "    return score\n",
    "def objective(trial, model_name, image_name, task_type, \n",
    "              train_loader, val_loader, test_loader,\n",
    "              divisors, imgs_shape, num_classes=None,\n",
    "              device='cuda', save_dir=None, class_weight=None, epochs=100, path_vit=None):\n",
    "    \n",
    "    task = task_type.lower()\n",
    "    \n",
    "    if model_name == \"vit\":\n",
    "\n",
    "        params = load_search_space(model_name, trial)\n",
    "\n",
    "        params[\"patch_size\"] = trial.suggest_categorical(\"patch_size\", divisors)\n",
    "\n",
    "        params[\"mlp_hidden_dims\"] = json.loads(params[\"mlp_hidden_dims\"])\n",
    "\n",
    "        # prune invalid attention shapes early\n",
    "        if params[\"dim\"] % params[\"heads\"] != 0:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        with open(f\"configs/optuna_search/{model_name}.json\", \"r\") as f:\n",
    "            full_config = json.load(f)\n",
    "\n",
    "        config = full_config[model_name][\"fit\"]  # Access the model key\n",
    "\n",
    "        # Build and train model\n",
    "        model = ViTMLP(imgs_shape[1], params, task, num_classes)\n",
    "    else:\n",
    "        # --- CNN branch (ResNet-style) ---\n",
    "        params = load_search_space(model_name, trial)\n",
    "        \n",
    "        with open(f\"{path_vit}/best_params.json\", \"r\") as f:\n",
    "                best_vit = json.load(f)\n",
    "\n",
    "        best_vit[\"total_params\"]\n",
    "\n",
    "        # parse head dims safely (keeps your JSON format)\n",
    "        params[\"mlp_hidden_dims\"] = json.loads(params[\"mlp_hidden_dims\"])\n",
    "        \n",
    "                    \n",
    "        params[\"blocks_per_stage\"] = json.loads(params[\"blocks_per_stage\"])\n",
    "        \n",
    "        # types\n",
    "        stem_type  = params[\"stem_type\"]\n",
    "        use_maxpool= params[\"use_maxpool\"]\n",
    "        stem_width = params[\"stem_width\"]\n",
    "        base_width = params[\"base_width\"]\n",
    "        width_mul  = float(params[\"width_mul\"])\n",
    "        blocks_list = params[\"blocks_per_stage\"]\n",
    "        n_stages   = len(blocks_list)\n",
    "\n",
    "        # 1) forbid 7x7 stem on tiny images\n",
    "        _, H, W = imgs_shape  # imgs_shape = (C,H,W)\n",
    "        if stem_type == \"7x7\" and max(H, W) < 64:\n",
    "            raise optuna.TrialPruned(\"7x7 stem on small images (<64)\")\n",
    "            \n",
    "        # Forbid 3×3 on very large images (too fine, too heavy)\n",
    "        if stem_type == \"3x3\" and max(H, W) >= 224:\n",
    "            raise optuna.TrialPruned(\"3x3 stem on very large images (>224)\")\n",
    "\n",
    "        # 2) worst-case collapse check (avoid < 2x2)\n",
    "        minH, minW = _est_min_size_safe(H, W, stem_type, use_maxpool, n_stages)\n",
    "        if min(minH, minW) < 2:\n",
    "            raise optuna.TrialPruned(f\"downsampling collapses spatial to {minH}x{minW} < 2x2\")\n",
    "\n",
    "        with open(f\"configs/optuna_search/{model_name}.json\", \"r\") as f:\n",
    "            full_config = json.load(f)\n",
    "        config = full_config[model_name][\"fit\"]\n",
    "\n",
    "        # Build and train model\n",
    "        model = CNN(imgs_shape, params, task, num_classes)\n",
    "        \n",
    "        # 5) capacity-match prune vs best ViT params\n",
    "        num_params_vit = float(best_vit[\"total_params\"])\n",
    "        num_params_cnn = _count_params(model)\n",
    "\n",
    "        tol = 0.25  # allow up to +25%\n",
    "        if num_params_cnn > (1.0 + tol) * num_params_vit:\n",
    "            raise optuna.TrialPruned(\n",
    "                f\"params {num_params_cnn} > {int((1.0+tol)*100)}% of {int(num_params_vit)}\"\n",
    "            )\n",
    "    \n",
    "    metrics = compile_and_fit(\n",
    "        model,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=f\"trial_{trial.number}\",\n",
    "        image_name=image_name,\n",
    "        task=task,  # assumed to be defined externally\n",
    "        max_lr=trial.suggest_float(\"max_lr\", config[\"max_lr\"][1], config[\"max_lr\"][2], log=True),\n",
    "        div_factor=trial.suggest_int(\"div_factor\", config[\"div_factor\"][1], config[\"div_factor\"][2]),\n",
    "        final_div_factor=trial.suggest_int(\"final_div_factor\", config[\"final_div_factor\"][1], config[\"final_div_factor\"][2]),\n",
    "        weight_decay=trial.suggest_float(\"weight_decay\", config[\"weight_decay\"][1], config[\"weight_decay\"][2], log=True),\n",
    "        pct_start=trial.suggest_float(\"pct_start\", config[\"pct_start\"][1], config[\"pct_start\"][2]),\n",
    "        epochs=epochs,\n",
    "        save_model=False,\n",
    "        class_weights=class_weight\n",
    "    )\n",
    "\n",
    "    save_dir = os.path.join(save_dir, model_name, image_name, \"optuna\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if task == 'regression':\n",
    "        score = metrics[\"val_rmse\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-RMSE: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    elif task == 'binary':\n",
    "        score = metrics[\"val_roc_auc\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-AUC: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    elif task == 'multiclass':\n",
    "        score = metrics[\"val_accuracy\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-Accuracy: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === benchmark_eval.py (vision-only; no frozen branches) ======================\n",
    "from numbers import Number\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# NEW: optional calflops (robust fallback if not installed)\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    from calflops import calculate_flops as _calflops_calc\n",
    "    _HAVE_CALFLOPS = True\n",
    "except Exception:\n",
    "    _HAVE_CALFLOPS = False\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Config (adjust as needed)\n",
    "# -----------------------------------------------------------------------------\n",
    "TOP_K = 5\n",
    "SINGLE_PASS_SEED = 0              # seed for one-time eval of top-K\n",
    "FINAL_SEEDS = [0, 1, 2, 3, 4]     # seeds for the final winner\n",
    "FULL_EPOCHS = 100\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def _count_params(model: nn.Module, trainable_only: bool = False) -> int:\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def _ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def _is_minimize_study(study):\n",
    "    try:\n",
    "        return study.direction == optuna.study.StudyDirection.MINIMIZE\n",
    "    except Exception:\n",
    "        try:\n",
    "            return study.directions[0] == optuna.study.StudyDirection.MINIMIZE\n",
    "        except Exception:\n",
    "            return True  # fallback\n",
    "\n",
    "def primary_val_key_for_task(task_type: str):\n",
    "    t = task_type.lower()\n",
    "    if t == \"regression\":   # lower is better\n",
    "        return \"val_rmse\", True\n",
    "    if t == \"binary\":       # higher is better\n",
    "        return \"val_roc_auc\", False\n",
    "    if t == \"multiclass\":   # higher is better\n",
    "        return \"val_accuracy\", False\n",
    "    return \"val_loss\", True\n",
    "\n",
    "def _sort_trials(trials, minimize: bool):\n",
    "    return sorted(trials, key=lambda t: t.value, reverse=not minimize)\n",
    "\n",
    "def _metric_or_default(m: dict, key: str, minimize: bool):\n",
    "    if key in m and isinstance(m[key], (Number, np.floating, np.integer)):\n",
    "        return float(m[key])\n",
    "    # worst side if missing\n",
    "    return (np.inf if minimize else -np.inf)\n",
    "\n",
    "def _parse_if_json_list(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return json.loads(x)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return x\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# NEW: small helpers for FLOPs/MACs\n",
    "# -----------------------------------------------------------------------------\n",
    "def _humanize(n: float, unit: str = \"\") -> str:\n",
    "    try:\n",
    "        n = float(n)\n",
    "        for u in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "            if abs(n) < 1000.0:\n",
    "                return f\"{n:.3f}{u}{unit}\"\n",
    "            n /= 1000.0\n",
    "        return f\"{n:.3f}E{unit}\"\n",
    "    except Exception:\n",
    "        return str(n)\n",
    "\n",
    "def _try_compute_flops(model: nn.Module, imgs_shape, batch_size: int = 1):\n",
    "    \"\"\"\n",
    "    Returns dict with numeric and pretty strings, or None if not available.\n",
    "    {\n",
    "      'flops': <float>, 'macs': <float>, 'params_from_calflops': <float>,\n",
    "      'flops_str': 'x.xxG', 'macs_str': 'x.xxG'\n",
    "    }\n",
    "    \"\"\"\n",
    "    if not _HAVE_CALFLOPS:\n",
    "        return None\n",
    "    try:\n",
    "        # imgs_shape is expected (C,H,W). Be defensive if it's something else.\n",
    "        if isinstance(imgs_shape, (list, tuple)) and len(imgs_shape) >= 3:\n",
    "            C, H, W = imgs_shape[-3], imgs_shape[-2], imgs_shape[-1]\n",
    "        else:\n",
    "            # fallback to common defaults if shape is unknown\n",
    "            C, H, W = 3, 224, 224\n",
    "        flops, macs, params_cf = _calflops_calc(\n",
    "            model=model,\n",
    "            input_shape=(batch_size, int(C), int(H), int(W)),\n",
    "            output_as_string=False\n",
    "        )\n",
    "        # calflops may return ints (ops), we store as float for consistency\n",
    "        out = {\n",
    "            \"flops\": float(flops),\n",
    "            \"macs\": float(macs),\n",
    "            \"params_from_calflops\": float(params_cf),\n",
    "            \"flops_str\": _humanize(flops),\n",
    "            \"macs_str\": _humanize(macs),\n",
    "        }\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Build + train + return metrics (with param counts) for a trial\n",
    "# -----------------------------------------------------------------------------\n",
    "def evaluate_best_model(\n",
    "    best_trial,\n",
    "    train_loader, val_loader, test_loader,\n",
    "    dataset_name, image_name, task_type,\n",
    "    save_dir, imgs_shape, trial_name,\n",
    "    class_weight=None, num_classes=None, epochs=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Vision-only evaluation (no frozen branches).\n",
    "      - If model_name == \"vit\": build ViTMLP directly from best_trial params.\n",
    "      - Else: build CNN from best_trial params.\n",
    "    Saves param counts + FLOPs alongside run and returns metrics (augmented).\n",
    "    \"\"\"\n",
    "    task = task_type.lower()\n",
    "    best_params = best_trial.params\n",
    "\n",
    "    print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "    print(f\"  Best Score: {best_trial.value:.4f}\")\n",
    "    print(\"  Best Hyperparameters:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"    {k}: {v}\")\n",
    "\n",
    "    # ---------------- Build model from trial params ----------------\n",
    "    if model_name == \"vit\":\n",
    "        # ViT-only baseline\n",
    "        architecture_params = {\n",
    "            k: v for k, v in best_params.items()\n",
    "            if k in [\"patch_size\", \"dim\", \"depth\", \"heads\", \"mlp_dim\",\n",
    "                     \"mlp_hidden_dims\", \"dropout\", \"emb_dropout\"]\n",
    "        }\n",
    "        if isinstance(architecture_params.get(\"mlp_hidden_dims\"), str):\n",
    "            architecture_params[\"mlp_hidden_dims\"] = json.loads(architecture_params[\"mlp_hidden_dims\"])\n",
    "\n",
    "        patch = architecture_params[\"patch_size\"]\n",
    "        model = ViTMLP(imgs_shape[1], architecture_params, task, num_classes)\n",
    "\n",
    "        fit_params = {\n",
    "            \"max_lr\": best_params[\"max_lr\"],\n",
    "            \"div_factor\": best_params[\"div_factor\"],\n",
    "            \"final_div_factor\": best_params[\"final_div_factor\"],\n",
    "            \"weight_decay\": best_params[\"weight_decay\"],\n",
    "            \"pct_start\": best_params[\"pct_start\"],\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        architecture_params = {\n",
    "            k: v for k, v in best_params.items()\n",
    "            if k in [\"in_channels\", \"activation\", \"stem_type\", \"use_maxpool\", \"stem_width\",\n",
    "                     \"n_stages\", \"blocks_per_stage\", \"base_width\", \"width_mul\", \"mlp_hidden_dims\"]\n",
    "        }\n",
    "        if isinstance(architecture_params.get(\"mlp_hidden_dims\"), str):\n",
    "            architecture_params[\"mlp_hidden_dims\"] = json.loads(architecture_params[\"mlp_hidden_dims\"])\n",
    "        if isinstance(architecture_params.get(\"blocks_per_stage\"), str):\n",
    "            architecture_params[\"blocks_per_stage\"] = json.loads(architecture_params[\"blocks_per_stage\"])\n",
    "\n",
    "        patch = None\n",
    "        model = CNN(imgs_shape, architecture_params, task, num_classes)\n",
    "\n",
    "        fit_params = {\n",
    "            \"max_lr\": best_params[\"max_lr\"],\n",
    "            \"div_factor\": best_params[\"div_factor\"],\n",
    "            \"final_div_factor\": best_params[\"final_div_factor\"],\n",
    "            \"weight_decay\": best_params[\"weight_decay\"],\n",
    "            \"pct_start\": best_params[\"pct_start\"],\n",
    "        }\n",
    "\n",
    "    # ---------------- Count params ----------------\n",
    "    total_params = _count_params(model, trainable_only=False)\n",
    "    trainable_params = _count_params(model, trainable_only=True)\n",
    "    print(f\"  Params: total={total_params:,}  trainable={trainable_params:,}\")\n",
    "\n",
    "    # ---------------- NEW: FLOPs/MACs ----------------\n",
    "    flops_info = _try_compute_flops(model, imgs_shape, batch_size=1)\n",
    "    if flops_info is not None:\n",
    "        print(f\"  FLOPs: {flops_info['flops_str']}  MACs: {flops_info['macs_str']}\")\n",
    "\n",
    "    base_dir = _ensure_dir(os.path.join(save_dir, f\"{model_name}/{image_name}/best_model/{trial_name}\"))\n",
    "    # Save full best params + counts (+ FLOPs) so you keep exact tuned config\n",
    "    best_params_with_counts = dict(best_params)\n",
    "    best_params_with_counts[\"total_params\"] = int(total_params)\n",
    "    best_params_with_counts[\"trainable_params\"] = int(trainable_params)\n",
    "    if flops_info is not None:\n",
    "        best_params_with_counts[\"flops\"] = flops_info[\"flops\"]\n",
    "        best_params_with_counts[\"macs\"] = flops_info[\"macs\"]\n",
    "        best_params_with_counts[\"flops_str\"] = flops_info[\"flops_str\"]\n",
    "        best_params_with_counts[\"macs_str\"] = flops_info[\"macs_str\"]\n",
    "    with open(os.path.join(base_dir, \"best_params.json\"), \"w\") as f:\n",
    "        json.dump(best_params_with_counts, f, indent=4)\n",
    "\n",
    "    # ---------------- Train & evaluate ----------------\n",
    "    metrics = compile_and_fit(\n",
    "        model,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        image_name=image_name,\n",
    "        model_name=model_name,\n",
    "        trial_name=trial_name,\n",
    "        task=task,\n",
    "        max_lr=fit_params[\"max_lr\"],\n",
    "        div_factor=fit_params[\"div_factor\"],\n",
    "        final_div_factor=fit_params[\"final_div_factor\"],\n",
    "        weight_decay=fit_params[\"weight_decay\"],\n",
    "        pct_start=fit_params[\"pct_start\"],\n",
    "        epochs=epochs,\n",
    "        save_model=True,\n",
    "        class_weights=class_weight,\n",
    "        save_dir=save_dir,\n",
    "        patch=(patch if patch is not None else \"\")\n",
    "    )\n",
    "\n",
    "    # Augment metrics with parameter counts + FLOPs\n",
    "    metrics[\"total_params\"] = int(total_params)\n",
    "    metrics[\"trainable_params\"] = int(trainable_params)\n",
    "    if flops_info is not None:\n",
    "        metrics[\"flops\"] = flops_info[\"flops\"]\n",
    "        metrics[\"macs\"] = flops_info[\"macs\"]\n",
    "        metrics[\"flops_str\"] = flops_info[\"flops_str\"]\n",
    "        metrics[\"macs_str\"] = flops_info[\"macs_str\"]\n",
    "    return metrics\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Top-K → single-pass → winner → multi-seed driver\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_topk_and_multiseed(\n",
    "    study, model_name, dataset_name, name, task_type, save_dir,\n",
    "    imgs_shape, num_classes, class_weight,\n",
    "    train_loader, val_loader, test_loader\n",
    "):\n",
    "    \"\"\"\n",
    "    Same signature as your hybrid driver, but vision-only (no frozen components).\n",
    "    \"\"\"\n",
    "    minimize = _is_minimize_study(study)\n",
    "    completed = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    if not completed:\n",
    "        raise RuntimeError(\"No completed trials in the study.\")\n",
    "\n",
    "    top_trials = _sort_trials(completed, minimize)[:TOP_K]\n",
    "    primary_key, primary_minimize = primary_val_key_for_task(task_type)\n",
    "    maximize_primary = not primary_minimize\n",
    "\n",
    "    print(f\"\\nEvaluating top-{len(top_trials)} trials once at {FULL_EPOCHS} epochs (seed={SINGLE_PASS_SEED})...\\n\")\n",
    "\n",
    "    # Single-pass over top-K\n",
    "    single_pass_results = []  # list of (trial, trial_name, metrics_dict)\n",
    "    for trial in top_trials:\n",
    "        if model_name == \"vit\":\n",
    "            best_patch = trial.params.get(\"patch_size\", None)\n",
    "            trial_name = f\"trial_{trial.number}_patch{best_patch}\"\n",
    "            header = f\"(Trial {trial.number}, ValObjective: {trial.value:.4f}, patch_size={best_patch})\"\n",
    "        else:\n",
    "            trial_name = f\"trial_{trial.number}\"\n",
    "            header = f\"(Trial {trial.number}, ValObjective: {trial.value:.4f})\"\n",
    "\n",
    "        print(f\"→ Single-pass full run {header}\")\n",
    "        set_model_seed(SINGLE_PASS_SEED)\n",
    "\n",
    "        metrics = evaluate_best_model(\n",
    "            best_trial=trial,\n",
    "            train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "            dataset_name=dataset_name, image_name=name, task_type=task_type,\n",
    "            save_dir=save_dir, imgs_shape=imgs_shape,\n",
    "            trial_name=trial_name, class_weight=class_weight, num_classes=num_classes,\n",
    "            epochs=FULL_EPOCHS\n",
    "        )\n",
    "        if not isinstance(metrics, dict):\n",
    "            raise TypeError(f\"evaluate_best_model must return dict, got: {type(metrics)}\")\n",
    "\n",
    "        # brief printout (metric + params)\n",
    "        if primary_key in metrics:\n",
    "            print(f\"   {primary_key}={float(metrics[primary_key]):.6f}\")\n",
    "        tp = metrics.get(\"total_params\"); trp = metrics.get(\"trainable_params\")\n",
    "        if tp is not None:\n",
    "            print(f\"   params: total={tp:,}, trainable={trp:,}\")\n",
    "        # NEW: echo FLOPs if available\n",
    "        if \"flops_str\" in metrics and \"macs_str\" in metrics:\n",
    "            print(f\"   flops={metrics['flops_str']}, macs={metrics['macs_str']}\")\n",
    "        single_pass_results.append((trial, trial_name, metrics))\n",
    "\n",
    "    # Winner by primary metric\n",
    "    if maximize_primary:\n",
    "        winner_tuple = max(single_pass_results, key=lambda x: _metric_or_default(x[2], primary_key, primary_minimize))\n",
    "    else:\n",
    "        winner_tuple = min(single_pass_results, key=lambda x: _metric_or_default(x[2], primary_key, primary_minimize))\n",
    "\n",
    "    best_trial, best_trial_name, best_single_metrics = winner_tuple\n",
    "    best_primary_val = _metric_or_default(best_single_metrics, primary_key, primary_minimize)\n",
    "    print(f\"\\nWinner after single-pass: Trial {best_trial.number} ({best_trial_name}) \"\n",
    "          f\"by {primary_key}={best_primary_val:.6f}\")\n",
    "\n",
    "    # Multi-seed evaluation of winner\n",
    "    print(f\"\\nRe-running winner with seeds {FINAL_SEEDS} at {FULL_EPOCHS} epochs...\\n\")\n",
    "    winner_save_path = _ensure_dir(os.path.join(save_dir, f\"{model_name}/{name}/best_model/{best_trial_name}\"))\n",
    "\n",
    "    per_seed_metrics = []\n",
    "    numeric_keys = None\n",
    "\n",
    "    for s in FINAL_SEEDS:\n",
    "        set_model_seed(s)\n",
    "        m = evaluate_best_model(\n",
    "            best_trial=best_trial,\n",
    "            train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "            dataset_name=dataset_name, image_name=name, task_type=task_type,\n",
    "            save_dir=save_dir, imgs_shape=imgs_shape,\n",
    "            trial_name=f\"{best_trial_name}_seed{s}\", class_weight=class_weight, num_classes=num_classes,\n",
    "            epochs=FULL_EPOCHS,\n",
    "        )\n",
    "        if not isinstance(m, dict):\n",
    "            raise TypeError(f\"evaluate_best_model must return dict, got: {type(m)}\")\n",
    "\n",
    "        if numeric_keys is None:\n",
    "            numeric_keys = [k for k, v in m.items() if isinstance(v, (Number, np.floating, np.integer))]\n",
    "        per_seed_metrics.append(m)\n",
    "\n",
    "        # quick line\n",
    "        pk_val = _metric_or_default(m, primary_key, primary_minimize)\n",
    "        extras = []\n",
    "        for k in [\"test_loss\", \"test_accuracy\", \"test_roc_auc\", \"test_rmse\", \"val_loss\"]:\n",
    "            if k in m and isinstance(m[k], (Number, np.floating, np.integer)):\n",
    "                extras.append(f\"{k}={float(m[k]):.6f}\")\n",
    "        print(f\"   Seed {s}: {primary_key}={pk_val:.6f}\" + (\", \" + \", \".join(extras) if extras else \"\"))\n",
    "\n",
    "    # Aggregate across seeds\n",
    "    aggregates = {}\n",
    "    for k in numeric_keys or []:\n",
    "        vals = [float(m[k]) for m in per_seed_metrics if k in m]\n",
    "        if not vals:\n",
    "            continue\n",
    "        mean_k = float(np.mean(vals))\n",
    "        std_k = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "        aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "    # Param counts (same across seeds) & FLOPs (take from single-pass winner)\n",
    "    winner_total_params = best_single_metrics.get(\"total_params\")\n",
    "    winner_train_params = best_single_metrics.get(\"trainable_params\")\n",
    "    winner_flops_str = best_single_metrics.get(\"flops_str\")\n",
    "    winner_macs_str  = best_single_metrics.get(\"macs_str\")\n",
    "    winner_flops_num = best_single_metrics.get(\"flops\")\n",
    "    winner_macs_num  = best_single_metrics.get(\"macs\")\n",
    "\n",
    "    # Save summary\n",
    "    out_file = os.path.join(winner_save_path, \"winner_multi_seed_summary.txt\")\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# Final winner multi-seed evaluation (vision-only)\\n\")\n",
    "        f.write(f\"trial_number: {best_trial.number}\\n\")\n",
    "        if model_name == \"vit\":\n",
    "            f.write(f\"patch_size: {best_trial.params.get('patch_size', None)}\\n\")\n",
    "        f.write(f\"primary_metric: {primary_key}\\n\")\n",
    "        f.write(f\"seeds: {FINAL_SEEDS}\\n\")\n",
    "        # NEW: FLOPs/MACs summary (pretty + numeric if available)\n",
    "        if winner_flops_str is not None and winner_macs_str is not None:\n",
    "            f.write(\"compute:\\n\")\n",
    "            f.write(f\"  flops: {winner_flops_str}\\n\")\n",
    "            f.write(f\"  macs: {winner_macs_str}\\n\")\n",
    "            if winner_flops_num is not None and winner_macs_num is not None:\n",
    "                f.write(f\"  flops_num: {winner_flops_num}\\n\")\n",
    "                f.write(f\"  macs_num: {winner_macs_num}\\n\")\n",
    "        f.write(\"per_seed_metrics:\\n\")\n",
    "        for s, m in zip(FINAL_SEEDS, per_seed_metrics):\n",
    "            f.write(f\"  - seed: {s}\\n\")\n",
    "            for k in (numeric_keys or []):\n",
    "                if k in m:\n",
    "                    f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "        f.write(\"aggregates:\\n\")\n",
    "        for k, mm in aggregates.items():\n",
    "            f.write(f\"  {k}:\\n\")\n",
    "            f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "            f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "    # Console summary\n",
    "    if primary_key in aggregates:\n",
    "        print(f\"\\nWinner aggregated {primary_key}: {aggregates[primary_key]['mean']:.6f} \"\n",
    "              f\"± {aggregates[primary_key]['std']:.6f}\")\n",
    "    elif \"val_loss\" in aggregates:\n",
    "        print(f\"\\nWinner aggregated val_loss: {aggregates['val_loss']['mean']:.6f} \"\n",
    "              f\"± {aggregates['val_loss']['std']:.6f}\")\n",
    "    if winner_total_params is not None:\n",
    "        print(f\"Model params: total={winner_total_params:,}, trainable={winner_train_params:,}\")\n",
    "    # NEW: echo compute again for clarity\n",
    "    if winner_flops_str and winner_macs_str:\n",
    "        print(f\"Compute: FLOPs={winner_flops_str}, MACs={winner_macs_str}\")\n",
    "    print(f\"Saved multi-seed summary to: {out_file}\")\n",
    "\n",
    "    return {\n",
    "        \"winner_trial_number\": best_trial.number,\n",
    "        \"winner_trial_name\": best_trial_name,\n",
    "        \"primary_metric\": primary_key,\n",
    "        \"aggregates\": aggregates,\n",
    "        \"total_params\": winner_total_params,\n",
    "        \"trainable_params\": winner_train_params,\n",
    "        \"flops\": winner_flops_num,\n",
    "        \"macs\": winner_macs_num,\n",
    "        \"summary_path\": out_file,\n",
    "    }\n",
    "# ========================== end benchmark_eval.py =============================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_model_seed(seed: int):\n",
    "    # Python built-in RNG\n",
    "    random.seed(seed)\n",
    "    # NumPy RNG\n",
    "    np.random.seed(seed)\n",
    "    # Torch RNG\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you use multi-GPU\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_patch_sizes_infer_size(\n",
    "    divisors: list[int],\n",
    "    max_tokens: int = 196\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    From `divisors`, assume the image side length S = max(divisors).\n",
    "    Return those p in divisors such that:\n",
    "      - p divides S (implicitly true since p ∈ divisors)\n",
    "      - tokens = (S//p)^2 ≤ max_tokens\n",
    "      - exclude p == S (the whole image patch)\n",
    "    \"\"\"\n",
    "    if not divisors:\n",
    "        return []\n",
    "    S = max(divisors)\n",
    "    valid = []\n",
    "    for p in divisors:\n",
    "        if p <= 0:\n",
    "            continue\n",
    "        if p == S:\n",
    "            continue  # exclude whole-image patch\n",
    "        # infer tokens\n",
    "        tokens = (S // p) ** 2\n",
    "        if tokens <= max_tokens:\n",
    "            valid.append(p)\n",
    "    return sorted(set(valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def reduce_dataloader(train_loader, fraction=0.25, stratify=True, seed=42):\n",
    "    \"\"\"\n",
    "    Return a new DataLoader that draws from ~fraction of the original train dataset.\n",
    "    For classification (TensorDataset(..., y)), uses a stratified subsample.\n",
    "    \"\"\"\n",
    "    assert 0 < fraction <= 1.0\n",
    "    ds = train_loader.dataset\n",
    "    n = len(ds)\n",
    "    num_keep = max(1, int(round(n * fraction)))\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    # Try stratified pick if labels are available (TensorDataset last tensor is y)\n",
    "    subset_idx = None\n",
    "    if stratify and hasattr(ds, \"tensors\") and len(ds.tensors) >= 2:\n",
    "        y = ds.tensors[-1].cpu().numpy().ravel()\n",
    "        try:\n",
    "            from sklearn.model_selection import StratifiedShuffleSplit\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, train_size=fraction, random_state=seed)\n",
    "            chosen, _ = next(sss.split(idx, y))\n",
    "            subset_idx = idx[chosen]\n",
    "        except Exception:\n",
    "            subset_idx = None  # fallback to random below\n",
    "\n",
    "    # Fallback: random subset with a fixed seed\n",
    "    if subset_idx is None:\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        subset_idx = torch.randperm(n, generator=g)[:num_keep].tolist()\n",
    "\n",
    "    # Build subset dataset and a new DataLoader (reuse original loader settings)\n",
    "    subset = Subset(ds, subset_idx)  # official Subset utility\n",
    "    new_loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=train_loader.batch_size,\n",
    "        shuffle=True,                               # shuffle within the subset\n",
    "        num_workers=getattr(train_loader, \"num_workers\", 0),\n",
    "        pin_memory=getattr(train_loader, \"pin_memory\", False),\n",
    "        drop_last=getattr(train_loader, \"drop_last\", False),\n",
    "        persistent_workers=getattr(train_loader, \"persistent_workers\", False),\n",
    "    )\n",
    "    return new_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: TINTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "name = f\"TINTO_blur\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vit = f\"./logs/{task_type}/{dataset_name}/vit/{name}/best_model/trial_84_patch4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None, \n",
    "    epochs=epochs,\n",
    "    path_vit=path_vit\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: IGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"IGTD\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=True, target_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vit = f\"./logs/{task_type}/{dataset_name}/vit/{name}/best_model/trial_64_patch4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None, \n",
    "    epochs=epochs,\n",
    "    path_vit=path_vit\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: REFINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"REFINED\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=True, target_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vit = f\"./logs/{task_type}/{dataset_name}/vit/{name}/best_model/trial_49_patch1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None, \n",
    "    epochs=epochs,\n",
    "    path_vit=path_vit\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: BarGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"BarGraph\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=True, target_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vit = f\"./logs/{task_type}/{dataset_name}/vit/{name}/best_model/trial_61_patch10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None, \n",
    "    epochs=epochs,\n",
    "    path_vit=path_vit\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: DistanceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"DistanceMatrix\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=True, target_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vit = f\"./logs/{task_type}/{dataset_name}/vit/{name}/best_model/trial_92_patch20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None, \n",
    "    epochs=epochs,\n",
    "    path_vit=path_vit\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"Combination\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=True, target_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vit = f\"./logs/{task_type}/{dataset_name}/vit/{name}/best_model/trial_91_patch20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None, \n",
    "    epochs=epochs,\n",
    "    path_vit=path_vit\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: SuperTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"SuperTML\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vit = f\"./logs/{task_type}/{dataset_name}/vit/{name}/best_model/trial_61_patch32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None, \n",
    "    epochs=epochs,\n",
    "    path_vit=path_vit\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: FeatureWrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"FeatureWrap\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device, pad_images=True, target_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vit = f\"./logs/{task_type}/{dataset_name}/vit/{name}/best_model/trial_74_patch4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None, \n",
    "    epochs=epochs,\n",
    "    path_vit=path_vit\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT: BIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "if task_type.lower() == \"regression\":\n",
    "    problem_type = \"regression\"\n",
    "else:\n",
    "    problem_type = \"supervised\"\n",
    "\n",
    "name = f\"BIE\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"SyntheticImages/{task_type}/{dataset_name}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, images_folder, problem_type, task_type, seed=SEED, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "divisors = find_divisors(imgs_shape[1])\n",
    "filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisors = filter_patch_sizes_infer_size(divisors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vit = f\"./logs/{task_type}/{dataset_name}/vit/{name}/best_model/trial_69_patch16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    image_name=name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    divisors=divisors,\n",
    "    imgs_shape=imgs_shape,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None, \n",
    "    epochs=epochs,\n",
    "    path_vit=path_vit\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_topk_and_multiseed(\n",
    "     study=study,\n",
    "     model_name=model_name,\n",
    "     dataset_name=dataset_name,\n",
    "     name=name,\n",
    "     task_type=task_type,\n",
    "     save_dir=save_dir,\n",
    "     imgs_shape=imgs_shape,\n",
    "     num_classes=num_classes,\n",
    "     class_weight=None,\n",
    "     train_loader=train_loader, val_loader=val_loader, test_loader=test_loader\n",
    " )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
