{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Third-party libraries\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Scikit-learn - core modules\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Scikit-learn - metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, average_precision_score, balanced_accuracy_score,\n",
    "    ConfusionMatrixDisplay, f1_score, log_loss,\n",
    "    matthews_corrcoef, mean_squared_error, precision_score,\n",
    "    PrecisionRecallDisplay, r2_score, recall_score, roc_auc_score, RocCurveDisplay\n",
    ")\n",
    "\n",
    "# Local application/library imports\n",
    "from utils import load_search_space, get_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'boston'        \n",
    "dataset_subpath = 'Regression/boston'       \n",
    "task_type = 'Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'connect-4'        \n",
    "dataset_subpath = 'Multiclass/connect-4'       \n",
    "task_type = 'Multiclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'nomao'        \n",
    "dataset_subpath = 'Binary/nomao'       \n",
    "task_type = 'Binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"./data/{dataset_subpath}/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce = True if len(df) > 20000 else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target_tensor(y, task):\n",
    "    task = task.lower()\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "    elif isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "        \n",
    "    if task == \"regression\" or task == \"binary\":\n",
    "        return torch.as_tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    elif task == \"multiclass\":\n",
    "        return torch.as_tensor(y, dtype=torch.long)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, dataset_name, task_type, model_type=\"default\", seed=42):\n",
    "    task_type = task_type.lower()\n",
    "    model_type = model_type.lower()\n",
    "\n",
    "    # Load config\n",
    "    with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    categorical_cols = config[\"categorical_cols\"]\n",
    "    numerical_cols = config[\"numerical_cols\"]\n",
    "    encoding = config[\"encoding\"]\n",
    "\n",
    "    # Extract features and target\n",
    "    X = df[numerical_cols + categorical_cols].copy()\n",
    "    y = df.iloc[:, -1].copy()\n",
    "\n",
    "    # Encode target if needed\n",
    "    le = None\n",
    "    if encoding.get(\"target\") == \"label\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        label_mapping = None\n",
    "\n",
    "    # Split raw data before transformation\n",
    "    if task_type == \"regression\":\n",
    "        # For regression, we can use a simple split\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed\n",
    "        )\n",
    "        X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed\n",
    "        )\n",
    "    else:\n",
    "        # For classification, we need stratified splits\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed, stratify=y\n",
    "        )\n",
    "        X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed, stratify=y_temp\n",
    "        )\n",
    "\n",
    "    # Ensure y_* are Series with index matching the X_*\n",
    "    y_train = pd.Series(y_train, index=X_train_raw.index)\n",
    "    y_val = pd.Series(y_val, index=X_val_raw.index)\n",
    "    y_test = pd.Series(y_test, index=X_test_raw.index)\n",
    "\n",
    "    # Compute class weights for classification\n",
    "    class_weight = None\n",
    "    if task_type in [\"binary\", \"multiclass\"]:\n",
    "        class_weight_values = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "        # Create the class weight dictionary with keys as native Python int (not numpy.int32)\n",
    "        class_weight = dict(zip([int(key) for key in np.unique(y_train)], class_weight_values))\n",
    "        print(f\"Class weights: {class_weight}\")\n",
    "\n",
    "    # CATBOOST path (no transformation, native categorical handling)\n",
    "    if model_type == \"catboost\":\n",
    "        for col in categorical_cols:\n",
    "            X_train_raw[col] = X_train_raw[col].astype(str)\n",
    "            X_val_raw[col] = X_val_raw[col].astype(str)\n",
    "            X_test_raw[col] = X_test_raw[col].astype(str)\n",
    "        print(f\"Shapes — Train: {X_train_raw.shape}, Val: {X_val_raw.shape}, Test: {X_test_raw.shape}\")\n",
    "        print(f\"Numerical features: {len(numerical_cols)} — {numerical_cols}\")\n",
    "        print(f\"Categorical features: {len(categorical_cols)} — {categorical_cols}\")\n",
    "        print(f\"Total features: {X_train_raw.shape[1]}\")\n",
    "        if label_mapping:\n",
    "            print(f\"Target label mapping: {label_mapping}\")\n",
    "        return (\n",
    "            X_train_raw, X_val_raw, X_test_raw,\n",
    "            y_train, y_val, y_test,\n",
    "            categorical_cols, le, class_weight\n",
    "        )\n",
    "    \n",
    "    if model_type == \"lightgbm\":\n",
    "        for col in categorical_cols:\n",
    "            X_train_raw[col] = X_train_raw[col].astype(\"category\")\n",
    "            X_val_raw[col] = X_val_raw[col].astype(\"category\")\n",
    "            X_test_raw[col] = X_test_raw[col].astype(\"category\")\n",
    "        print(f\"Shapes — Train: {X_train_raw.shape}, Val: {X_val_raw.shape}, Test: {X_test_raw.shape}\")\n",
    "        print(f\"Numerical features: {len(numerical_cols)} — {numerical_cols}\")\n",
    "        print(f\"Categorical features: {len(categorical_cols)} — {categorical_cols}\")\n",
    "        print(f\"Total features: {X_train_raw.shape[1]}\")\n",
    "        if label_mapping:\n",
    "            print(f\"Target label mapping: {label_mapping}\")\n",
    "        return (\n",
    "            X_train_raw, X_val_raw, X_test_raw,\n",
    "            y_train, y_val, y_test,\n",
    "            categorical_cols, le, class_weight\n",
    "        )\n",
    "\n",
    "    # Transform numerical and categorical features\n",
    "    transformers = []\n",
    "\n",
    "    if encoding[\"numerical_features\"] == \"minmax\":\n",
    "        transformers.append((\"num\", MinMaxScaler(), numerical_cols))\n",
    "    elif encoding[\"numerical_features\"] == \"standard\":\n",
    "        transformers.append((\"num\", StandardScaler(), numerical_cols))\n",
    "\n",
    "    if categorical_cols and encoding[\"categorical_features\"] == \"onehot\":\n",
    "        transformers.append((\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), categorical_cols))\n",
    "\n",
    "    if transformers:\n",
    "        preprocessor = ColumnTransformer(transformers=transformers)\n",
    "        X_train = preprocessor.fit_transform(X_train_raw)\n",
    "        X_val = preprocessor.transform(X_val_raw)\n",
    "        X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "        # Recover transformed column names\n",
    "        if \"cat\" in preprocessor.named_transformers_:\n",
    "            cat_feature_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    "            all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "        else:\n",
    "            all_feature_names = numerical_cols + categorical_cols\n",
    "\n",
    "        X_train = pd.DataFrame(X_train, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val = pd.DataFrame(X_val, columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test = pd.DataFrame(X_test, columns=all_feature_names, index=X_test_raw.index)\n",
    "    else:\n",
    "        all_feature_names = numerical_cols + categorical_cols  # or keep original order\n",
    "        X_train = pd.DataFrame(X_train_raw, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val = pd.DataFrame(X_val_raw, columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test = pd.DataFrame(X_test_raw, columns=all_feature_names, index=X_test_raw.index)\n",
    "\n",
    "    print(f\"Shapes — Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"Numerical features: {len(numerical_cols)} — {numerical_cols}\")\n",
    "    print(f\"Categorical features: {len(categorical_cols)} — {categorical_cols}\")\n",
    "    print(f\"Total features: {X_train.shape[1]}\")\n",
    "    if label_mapping:\n",
    "        print(f\"Target label mapping: {label_mapping}\")\n",
    "\n",
    "    return (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        None, le, class_weight\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data_deep(df, dataset_name, task_type, seed=42, batch_size=32, device='cpu'):\n",
    "    task_type = task_type.lower()\n",
    "\n",
    "    # Load config\n",
    "    with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    categorical_cols = config[\"categorical_cols\"]\n",
    "    numerical_cols = config[\"numerical_cols\"]\n",
    "    encoding = config[\"encoding\"]\n",
    "\n",
    "    # Extract features and target\n",
    "    X = df[numerical_cols + categorical_cols].copy()\n",
    "    y = df.iloc[:, -1].copy()\n",
    "\n",
    "    # Encode target if needed\n",
    "    le = None\n",
    "    if encoding.get(\"target\") == \"label\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        label_mapping = None\n",
    "\n",
    "    # Split raw data before transformation\n",
    "    if task_type == \"regression\":\n",
    "        # For regression, we can use a simple split\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed\n",
    "        )\n",
    "        X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed\n",
    "        )\n",
    "    else:\n",
    "        # For classification, we need stratified splits\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed, stratify=y\n",
    "        )\n",
    "        X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed, stratify=y_temp\n",
    "        )\n",
    "\n",
    "    # Compute class weights for classification\n",
    "    class_weight = None\n",
    "    if task_type in [\"binary\", \"multiclass\"]:\n",
    "        # Compute raw weights\n",
    "        class_weight_values = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "        classes_sorted = np.sort(np.unique(y_train))\n",
    "        \n",
    "        if task_type == \"binary\":\n",
    "            # Compute pos_weight = weight for class 1 / weight for class 0\n",
    "            weight_dict = dict(zip(classes_sorted, class_weight_values))\n",
    "            pos_weight = weight_dict[1] / weight_dict[0]\n",
    "            class_weight = torch.tensor(pos_weight, dtype=torch.float32).to(device)\n",
    "            print(f\"Binary pos_weight (for BCEWithLogitsLoss): {class_weight.item()}\")\n",
    "\n",
    "        elif task_type == \"multiclass\":\n",
    "            class_weight = torch.tensor(class_weight_values, dtype=torch.float32).to(device)\n",
    "            print(f\"Multiclass class weights (for CrossEntropyLoss): {class_weight.tolist()}\")\n",
    "\n",
    "    # Transform numerical and categorical features\n",
    "    transformers = []\n",
    "\n",
    "    if encoding[\"numerical_features\"] == \"minmax\":\n",
    "        transformers.append((\"num\", MinMaxScaler(), numerical_cols))\n",
    "    elif encoding[\"numerical_features\"] == \"standard\":\n",
    "        transformers.append((\"num\", StandardScaler(), numerical_cols))\n",
    "\n",
    "    if categorical_cols and encoding[\"categorical_features\"] == \"onehot\":\n",
    "        transformers.append((\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), categorical_cols))\n",
    "\n",
    "    if transformers:\n",
    "        preprocessor = ColumnTransformer(transformers=transformers)\n",
    "        X_train = preprocessor.fit_transform(X_train_raw)\n",
    "        X_val = preprocessor.transform(X_val_raw)\n",
    "        X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "        # Recover transformed column names\n",
    "        if \"cat\" in preprocessor.named_transformers_:\n",
    "            cat_feature_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    "            all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "        else:\n",
    "            all_feature_names = numerical_cols + categorical_cols\n",
    "\n",
    "        X_train_num = pd.DataFrame(X_train, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val_num = pd.DataFrame(X_val, columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test_num = pd.DataFrame(X_test, columns=all_feature_names, index=X_test_raw.index)\n",
    "    else:\n",
    "        all_feature_names = numerical_cols + categorical_cols  # or keep original order\n",
    "        X_train_num = pd.DataFrame(X_train_raw, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val_num = pd.DataFrame(X_val_raw, columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test_num = pd.DataFrame(X_test_raw, columns=all_feature_names, index=X_test_raw.index)\n",
    "\n",
    "\n",
    "    print(f\"Shapes — Train: {X_train_num.shape}, Val: {X_val_num.shape}, Test: {X_test_num.shape}\")\n",
    "    print(f\"Numerical features: {len(numerical_cols)} — {numerical_cols}\")\n",
    "    print(f\"Categorical features: {len(categorical_cols)} — {categorical_cols}\")\n",
    "    print(f\"Total features: {X_train_num.shape[1]}\")\n",
    "    if label_mapping:\n",
    "        print(f\"Target label mapping: {label_mapping}\")\n",
    "    \n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "\n",
    "    print(\"Attributes: \", attributes)\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.as_tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.as_tensor(X_test_num.values, dtype=torch.float32)\n",
    "    y_train_tensor = prepare_target_tensor(y_train, task_type)\n",
    "    y_val_tensor = prepare_target_tensor(y_val, task_type)\n",
    "    y_test_tensor = prepare_target_tensor(y_test, task_type)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    #X_train_img_tensor = X_train_img_tensor / 255.0\n",
    "    #X_val_img_tensor = X_val_img_tensor / 255.0\n",
    "    #X_test_img_tensor = X_test_img_tensor / 255.0\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset( X_train_num_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_num_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_num_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes,  le, class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, name, task_type, \n",
    "              X_train, y_train, \n",
    "              X_val, y_val, \n",
    "              metric_name, categorical_cols = None, num_classes=None, SEED=42, device='cuda', save_dir=None, class_weight=None):\n",
    "    params = load_search_space(name, trial)\n",
    "\n",
    "    if name == \"catboost\":\n",
    "        params[\"cat_features\"] = categorical_cols\n",
    "\n",
    "    model = get_model(name, params, task_type, num_classes, SEED, device, class_weight=class_weight)\n",
    "\n",
    "    if name == \"lightgbm\":\n",
    "        model.fit(X_train, y_train, categorical_feature=categorical_cols)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Choose metric based on task\n",
    "    metric_name = metric_name.lower()\n",
    "    if metric_name == \"f1\":\n",
    "        score = f1_score(y_val, y_pred, average='macro')  # F1 Score\n",
    "        metric_name = \"F1\"\n",
    "\n",
    "    elif metric_name == \"accuracy\":\n",
    "        score = accuracy_score(y_val, y_pred)  # Accuracy\n",
    "        metric_name = \"Accuracy\"\n",
    "\n",
    "    elif metric_name == \"mse\":\n",
    "        score = mean_squared_error(y_val, y_pred)  # MSE\n",
    "        metric_name = \"MSE\"\n",
    "\n",
    "    elif metric_name == \"rmse\":\n",
    "        score = np.sqrt(mean_squared_error(y_val, y_pred))  # RMSE\n",
    "        metric_name = \"RMSE\"\n",
    "\n",
    "    elif metric_name == \"auc\":\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_val)\n",
    "            if y_proba.shape[1] == 2:\n",
    "                # Binary classification\n",
    "                y_score = y_proba[:, 1]\n",
    "                score = roc_auc_score(y_val, y_score)\n",
    "            else:\n",
    "                # Multiclass\n",
    "                score = roc_auc_score(y_val, y_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "        else:\n",
    "            raise ValueError(\"Model does not support predict_proba, required for AUC.\")\n",
    "        metric_name = \"AUC\"\n",
    "\n",
    "    save_dir = os.path.join(save_dir, name, \"optuna\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "        f.write(f\"Trial {trial.number} - VAL-{metric_name}: {score:.4f}, Params: {params}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model(study, name, task_type,\n",
    "                        X_train, y_train,\n",
    "                        X_val, y_val,\n",
    "                        X_test, y_test,\n",
    "                        categorical_cols=None, num_classes=None, SEED=42, device='cuda',\n",
    "                        save_dir=None, class_weight=None):\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    if name == \"catboost\":\n",
    "        best_params[\"cat_features\"] = categorical_cols\n",
    "\n",
    "    model = get_model(name, best_params, task_type, num_classes, SEED, device, class_weight=class_weight)\n",
    "\n",
    "    save_path = os.path.join(save_dir, name, \"best_model\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Train\n",
    "    start_train = time.time()\n",
    "    if name == \"lightgbm\":\n",
    "        model.fit(X_train, y_train, categorical_feature=categorical_cols)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    log = {\"training_time_s\": train_time}\n",
    "    task_type = task_type.lower()\n",
    "\n",
    "    def evaluate_split(X, y, split_key: str):\n",
    "        start_pred = time.time()\n",
    "        y_pred = model.predict(X)\n",
    "        pred_time = time.time() - start_pred\n",
    "\n",
    "        out = {\n",
    "            f\"{split_key}_inference_time_s\": pred_time,\n",
    "            f\"{split_key}_accuracy\": accuracy_score(y, y_pred),\n",
    "            f\"{split_key}_f1\": f1_score(\n",
    "                y, y_pred,\n",
    "                average=(\"macro\" if task_type == \"multiclass\" else \"binary\")\n",
    "            ),\n",
    "            f\"{split_key}_recall\": recall_score(\n",
    "                y, y_pred,\n",
    "                average=(\"macro\" if task_type == \"multiclass\" else \"binary\")\n",
    "            ),\n",
    "            f\"{split_key}_precision\": precision_score(\n",
    "                y, y_pred,\n",
    "                average=(\"macro\" if task_type == \"multiclass\" else \"binary\")\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Binary-only extras (probability-based + MCC)\n",
    "        if task_type == \"binary\":\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_prob = model.predict_proba(X)[:, 1]\n",
    "                out[f\"{split_key}_roc_auc\"] = roc_auc_score(y, y_prob)\n",
    "                out[f\"{split_key}_avg_precision\"] = average_precision_score(y, y_prob)\n",
    "                out[f\"{split_key}_log_loss\"] = log_loss(y, y_prob)\n",
    "            out[f\"{split_key}_mcc\"] = matthews_corrcoef(y, y_pred)\n",
    "\n",
    "        # Console print with same snake_case keys\n",
    "        print(f\"\\n--- {split_key} results ---\")\n",
    "        for k in sorted(out.keys()):\n",
    "            print(f\"{k}: {out[k]:.6f}\")\n",
    "\n",
    "        return out\n",
    "\n",
    "    if task_type == \"regression\":\n",
    "        def eval_reg(X, y, split_key: str):\n",
    "            y_pred = model.predict(X)\n",
    "            res = {\n",
    "                f\"{split_key}_mse\": mean_squared_error(y, y_pred),\n",
    "                f\"{split_key}_rmse\": np.sqrt(mean_squared_error(y, y_pred)),\n",
    "                f\"{split_key}_r2\": r2_score(y, y_pred),\n",
    "            }\n",
    "            print(f\"\\n--- {split_key} regression results ---\")\n",
    "            for k in sorted(res.keys()):\n",
    "                print(f\"{k}: {res[k]:.6f}\")\n",
    "            return res\n",
    "\n",
    "        log.update(eval_reg(X_train, y_train, \"train\"))\n",
    "        log.update(eval_reg(X_val, y_val, \"val\"))\n",
    "        log.update(eval_reg(X_test, y_test, \"test\"))\n",
    "\n",
    "    else:  # binary or multiclass\n",
    "        log.update(evaluate_split(X_train, y_train, \"train\"))\n",
    "        log.update(evaluate_split(X_val, y_val, \"val\"))\n",
    "        log.update(evaluate_split(X_test, y_test, \"test\"))\n",
    "\n",
    "    # Save model + metrics + params\n",
    "    model_file = os.path.join(save_path, \"best_model.joblib\")\n",
    "    joblib.dump(model, model_file)\n",
    "\n",
    "    log_file = os.path.join(save_path, \"best_model_metrics.txt\")\n",
    "    with open(log_file, \"w\") as f:\n",
    "        for k in sorted(log.keys()):\n",
    "            v = log[k]\n",
    "            f.write(f\"{k}: {v:.6f}\\n\" if isinstance(v, (int, float, np.floating)) else f\"{k}: {v}\\n\")\n",
    "\n",
    "    params_file = os.path.join(save_path, \"best_params.json\")\n",
    "    with open(params_file, \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    print(\"\\nBest Parameters:\")\n",
    "    print(best_params)\n",
    "    print(f\"\\nModel saved: {model_file}\")\n",
    "    print(f\"Metrics saved: {log_file}\")\n",
    "    return log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_model_seed(seed: int):\n",
    "    # Python built-in RNG\n",
    "    random.seed(seed)\n",
    "    # NumPy RNG\n",
    "    np.random.seed(seed)\n",
    "    # Torch RNG\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you use multi-GPU\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "save_dir =  os.path.join(\"logs\", task_type, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric and direction based on task_type\n",
    "if task_type.lower() == 'regression':\n",
    "    metric_name = \"RMSE\"  # or any other regression metric\n",
    "    direction = \"minimize\"  # Lower RMSE is better\n",
    "elif task_type.lower() == 'binary':\n",
    "    metric_name = \"AUC\"  # or any other binary classification metric\n",
    "    direction = \"maximize\"  # Higher AUC is better\n",
    "elif task_type.lower() == 'multiclass':\n",
    "    metric_name = \"Accuracy\"  # or any other multiclass classification metric\n",
    "    direction = \"maximize\"  # Higher accuracy is better\n",
    "else:\n",
    "    raise ValueError(f\"Unknown task_type: {task_type}\")\n",
    "\n",
    "print(metric_name, direction)\n",
    "\n",
    "if task_type.lower() == 'regression' or task_type.lower() == 'binary':\n",
    "    num_classes = None\n",
    "else:\n",
    "    num_classes = df.iloc[:, -1].nunique()\n",
    "    print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def reduce_dataloader(train_loader, fraction=0.25, stratify=True, seed=42):\n",
    "    \"\"\"\n",
    "    Return a new DataLoader that draws from ~fraction of the original train dataset.\n",
    "    For classification (TensorDataset(..., y)), uses a stratified subsample.\n",
    "    \"\"\"\n",
    "    assert 0 < fraction <= 1.0\n",
    "    ds = train_loader.dataset\n",
    "    n = len(ds)\n",
    "    num_keep = max(1, int(round(n * fraction)))\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    # Try stratified pick if labels are available (TensorDataset last tensor is y)\n",
    "    subset_idx = None\n",
    "    if stratify and hasattr(ds, \"tensors\") and len(ds.tensors) >= 2:\n",
    "        y = ds.tensors[-1].cpu().numpy().ravel()\n",
    "        try:\n",
    "            from sklearn.model_selection import StratifiedShuffleSplit\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, train_size=fraction, random_state=seed)\n",
    "            chosen, _ = next(sss.split(idx, y))\n",
    "            subset_idx = idx[chosen]\n",
    "        except Exception:\n",
    "            subset_idx = None  # fallback to random below\n",
    "\n",
    "    # Fallback: random subset with a fixed seed\n",
    "    if subset_idx is None:\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        subset_idx = torch.randperm(n, generator=g)[:num_keep].tolist()\n",
    "\n",
    "    # Build subset dataset and a new DataLoader (reuse original loader settings)\n",
    "    subset = Subset(ds, subset_idx)  # official Subset utility\n",
    "    new_loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=train_loader.batch_size,\n",
    "        shuffle=True,                               # shuffle within the subset\n",
    "        num_workers=getattr(train_loader, \"num_workers\", 0),\n",
    "        pin_memory=getattr(train_loader, \"pin_memory\", False),\n",
    "        drop_last=getattr(train_loader, \"drop_last\", False),\n",
    "        persistent_workers=getattr(train_loader, \"persistent_workers\", False),\n",
    "    )\n",
    "    return new_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def _clean_name(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"[\\[\\]<>]\", \"_\", s)        # prohibidos por XGBoost\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)             # espacios -> _\n",
    "    s = re.sub(r\"[^0-9a-zA-Z_]\", \"_\", s)   # resto raro -> _\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")   # compactar\n",
    "    return s or \"col\"\n",
    "\n",
    "def sanitize_after_preprocess(X_train, X_val, X_test, categorical_cols=None):\n",
    "    # si no son DataFrames (p. ej., numpy), no hay nombres que sanear\n",
    "    if not hasattr(X_train, \"columns\"):\n",
    "        return X_train, X_val, X_test, categorical_cols, {}\n",
    "\n",
    "    # --- 1) construir mapping viejo->nuevo desde TRAIN (con unicidad) ---\n",
    "    old_cols = list(map(str, X_train.columns))\n",
    "    base = [_clean_name(c) for c in old_cols]\n",
    "\n",
    "    seen = {}\n",
    "    new_cols = []\n",
    "    for c in base:\n",
    "        if c in seen:\n",
    "            k = seen[c]\n",
    "            seen[c] = k + 1\n",
    "            new_cols.append(f\"{c}__{k}\")\n",
    "        else:\n",
    "            seen[c] = 1\n",
    "            new_cols.append(c)\n",
    "\n",
    "    colmap = dict(zip(old_cols, new_cols))\n",
    "\n",
    "    # --- 2) renombrar TRAIN ---\n",
    "    X_train = X_train.copy()\n",
    "    X_train.columns = new_cols\n",
    "\n",
    "    # --- 3) función para VAL / TEST: mapear, limpiar los no mapeados y realinear ---\n",
    "    def _apply_to_split(df):\n",
    "        if not hasattr(df, \"columns\"):\n",
    "            return df  # ej. numpy\n",
    "        df = df.copy()\n",
    "        raw_cols = list(map(str, df.columns))\n",
    "        mapped = [colmap.get(c, _clean_name(c)) for c in raw_cols]\n",
    "        df.columns = mapped\n",
    "        # realinear al orden/ set de TRAIN; faltantes -> 0\n",
    "        df = df.reindex(columns=new_cols, fill_value=0)\n",
    "        return df\n",
    "\n",
    "    X_val  = _apply_to_split(X_val)\n",
    "    X_test = _apply_to_split(X_test)\n",
    "\n",
    "    # --- 4) actualizar categorical_cols (si procede) ---\n",
    "    updated_cats = categorical_cols\n",
    "    if categorical_cols is not None:\n",
    "        if len(categorical_cols) > 0:\n",
    "            if isinstance(categorical_cols[0], str):\n",
    "                # nombres -> mapear por colmap (o limpiar si no estaba en train)\n",
    "                updated_cats = [colmap.get(c, _clean_name(c)) for c in categorical_cols]\n",
    "                # mantener solo las que existen en train tras reindex\n",
    "                updated_cats = [c for c in updated_cats if c in new_cols]\n",
    "            else:\n",
    "                # índices -> convertir a nombres ya saneados\n",
    "                updated_cats = [new_cols[i] for i in categorical_cols if 0 <= i < len(new_cols)]\n",
    "\n",
    "    return X_train, X_val, X_test, updated_cats, colmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_indices_from_loader(loader) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return indices into the ORIGINAL base dataset used by this loader.\n",
    "    Works for nested torch.utils.data.Subset(Subset(...)).\n",
    "    Order is preserved exactly as stored in the Subset.\n",
    "    \"\"\"\n",
    "    ds = loader.dataset\n",
    "    idx = None  # indices into the current ds\n",
    "    while isinstance(ds, Subset):\n",
    "        cur = np.asarray(ds.indices)\n",
    "        idx = cur if idx is None else cur[idx]  # compose through nesting\n",
    "        ds = ds.dataset\n",
    "    if idx is None:\n",
    "        idx = np.arange(len(ds))\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, label_encoder, class_weight  = load_and_preprocess_data_deep(df, dataset_name, task_type, seed=SEED, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loader = reduce_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_idx = get_subset_indices_from_loader(new_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, categorical_cols, label_encoder, class_weight = preprocess_data(df, dataset_name=dataset_name, task_type=task_type, model_type=\"xgboost\", seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Sanea NOMBRES después del preprocess\n",
    "X_train, X_val, X_test, categorical_cols, colmap = sanitize_after_preprocess(\n",
    "    X_train, X_val, X_test, categorical_cols\n",
    ")\n",
    "\n",
    "# (opcional) ver cambios\n",
    "changed = {k: v for k, v in colmap.items() if k != v}\n",
    "print(f\"Saneadas {len(changed)} columnas problemáticas.\" if changed else \"No había columnas problemáticas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_red = X_train.iloc[subset_idx].reset_index(drop=True)\n",
    "y_train_red = y_train.iloc[subset_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=direction)\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    name=\"xgboost\",\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    X_train =X_train_red if reduce else X_train,\n",
    "    y_train = y_train, \n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    metric_name=metric_name,\n",
    "    SEED=SEED,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "), n_trials=100)\n",
    "\n",
    "# Print best result summary\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "print(f\"  Score: {best_trial.value:.4f}\")\n",
    "print(\"  Best Hyperparameters:\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "\n",
    "# seeds & aggregation\n",
    "model_seeds = [0, 1, 2, 3, 4]\n",
    "numeric_keys = None\n",
    "per_seed_metrics = []\n",
    "\n",
    "# Where to save the summary file (the same folder you used before)\n",
    "summary_dir = os.path.join(save_dir, f\"xgboost/best_model\")\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "out_file = os.path.join(summary_dir, \"best_results_mean.txt\")\n",
    "\n",
    "for s in model_seeds:\n",
    "    set_model_seed(s)  # your util to set np/torch/python seeds if needed\n",
    "    metrics = evaluate_best_model(\n",
    "        study,                 # Optuna study with .best_params\n",
    "        \"xgboost\",                  # \"xgboost\" / \"lightgbm\" / \"catboost\" / etc.\n",
    "        task_type,\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        X_test, y_test,\n",
    "        categorical_cols=categorical_cols,\n",
    "        num_classes=num_classes,\n",
    "        SEED=s,                # <<< pass the seed into the model\n",
    "        device=device,\n",
    "        save_dir=save_dir,\n",
    "        class_weight=None\n",
    "    )\n",
    "    if not isinstance(metrics, dict):\n",
    "        raise TypeError(f\"evaluate_best_model must return dict, got {type(metrics)}\")\n",
    "\n",
    "    if numeric_keys is None:\n",
    "        numeric_keys = [k for k, v in metrics.items() if isinstance(v, (Number, np.floating, np.integer))]\n",
    "    per_seed_metrics.append(metrics)\n",
    "\n",
    "    # brief print\n",
    "    brief = \", \".join(f\"{k}={float(metrics[k]):.6f}\" for k in numeric_keys[:6])\n",
    "    print(f\"Seed {s}: {brief}\")\n",
    "\n",
    "# Aggregate mean/std\n",
    "aggregates = {}\n",
    "for k in numeric_keys:\n",
    "    vals = [float(m[k]) for m in per_seed_metrics]\n",
    "    mean_k = float(np.mean(vals))\n",
    "    std_k  = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "    aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "# Save YAML-like txt (same style as your example)\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Best trial re-evaluation across model seeds\\n\")\n",
    "    # If you really want to include patch_size (only exists for ViT/CNN), guard it:\n",
    "    if hasattr(study, \"best_params\") and \"patch_size\" in study.best_params:\n",
    "        f.write(f\"patch_size: {study.best_params['patch_size']}\\n\")\n",
    "    f.write(f\"seeds: {model_seeds}\\n\")\n",
    "    f.write(\"per_seed_metrics:\\n\")\n",
    "    for s, m in zip(model_seeds, per_seed_metrics):\n",
    "        f.write(f\"  - seed: {s}\\n\")\n",
    "        for k in numeric_keys:\n",
    "            f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "    f.write(\"aggregates:\\n\")\n",
    "    for k, mm in aggregates.items():\n",
    "        f.write(f\"  {k}:\\n\")\n",
    "        f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "        f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "# Console summary (pick a sensible key)\n",
    "pref_key = None\n",
    "if task_type.lower() == \"binary\":\n",
    "    for cand in [\"Test AUC\", \"Test Accuracy\", \"Val AUC\", \"Val Accuracy\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "elif task_type.lower() == \"multiclass\":\n",
    "    for cand in [\"Test Accuracy\", \"Val Accuracy\", \"Test F1\", \"Val F1\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "else:  # regression\n",
    "    for cand in [\"Test RMSE\", \"Val RMSE\", \"Test R2\", \"Val R2\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "\n",
    "if pref_key:\n",
    "    print(f\"→ xgboost: {pref_key} = {aggregates[pref_key]['mean']:.6f} ± {aggregates[pref_key]['std']:.6f}\")\n",
    "print(f\"Saved to: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, categorical_cols, label_encoder, class_weight = preprocess_data(df, dataset_name=dataset_name, task_type=task_type, model_type=\"catboost\", seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "categorical_cols = config[\"categorical_cols\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Sanea NOMBRES después del preprocess\n",
    "X_train, X_val, X_test, categorical_cols, colmap = sanitize_after_preprocess(\n",
    "    X_train, X_val, X_test, categorical_cols\n",
    ")\n",
    "\n",
    "# (opcional) ver cambios\n",
    "changed = {k: v for k, v in colmap.items() if k != v}\n",
    "print(f\"Saneadas {len(changed)} columnas problemáticas.\" if changed else \"No había columnas problemáticas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Sanea NOMBRES después del preprocess\n",
    "X_train, X_val, X_test, categorical_cols, colmap = sanitize_after_preprocess(\n",
    "    X_train, X_val, X_test, categorical_cols\n",
    ")\n",
    "\n",
    "# (opcional) ver cambios\n",
    "changed = {k: v for k, v in colmap.items() if k != v}\n",
    "print(f\"Saneadas {len(changed)} columnas problemáticas.\" if changed else \"No había columnas problemáticas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_red = X_train.iloc[subset_idx].reset_index(drop=True)\n",
    "y_train_red = y_train.iloc[subset_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=direction)\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    name=\"catboost\",\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    X_train =X_train_red if reduce else X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    metric_name=metric_name,\n",
    "    SEED=SEED,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    categorical_cols=categorical_cols\n",
    "), n_trials=100)\n",
    "\n",
    "# Print best result summary\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "print(f\"  AUC Score: {best_trial.value:.4f}\")\n",
    "print(\"  Best Hyperparameters:\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "\n",
    "# seeds & aggregation\n",
    "model_seeds = [0, 1, 2, 3, 4]\n",
    "numeric_keys = None\n",
    "per_seed_metrics = []\n",
    "\n",
    "# Where to save the summary file (the same folder you used before)\n",
    "summary_dir = os.path.join(save_dir, f\"catboost/best_model\")\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "out_file = os.path.join(summary_dir, \"best_results_mean.txt\")\n",
    "\n",
    "for s in model_seeds:\n",
    "    set_model_seed(s)  # your util to set np/torch/python seeds if needed\n",
    "    metrics = evaluate_best_model(\n",
    "        study,                 # Optuna study with .best_params\n",
    "        \"catboost\",                  # \"xgboost\" / \"lightgbm\" / \"catboost\" / etc.\n",
    "        task_type,\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        X_test, y_test,\n",
    "        categorical_cols=categorical_cols,\n",
    "        num_classes=num_classes,\n",
    "        SEED=s,                # <<< pass the seed into the model\n",
    "        device=device,\n",
    "        save_dir=save_dir,\n",
    "        class_weight=None\n",
    "    )\n",
    "    if not isinstance(metrics, dict):\n",
    "        raise TypeError(f\"evaluate_best_model must return dict, got {type(metrics)}\")\n",
    "\n",
    "    if numeric_keys is None:\n",
    "        numeric_keys = [k for k, v in metrics.items() if isinstance(v, (Number, np.floating, np.integer))]\n",
    "    per_seed_metrics.append(metrics)\n",
    "\n",
    "    # brief print\n",
    "    brief = \", \".join(f\"{k}={float(metrics[k]):.6f}\" for k in numeric_keys[:6])\n",
    "    print(f\"Seed {s}: {brief}\")\n",
    "\n",
    "# Aggregate mean/std\n",
    "aggregates = {}\n",
    "for k in numeric_keys:\n",
    "    vals = [float(m[k]) for m in per_seed_metrics]\n",
    "    mean_k = float(np.mean(vals))\n",
    "    std_k  = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "    aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "# Save YAML-like txt (same style as your example)\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Best trial re-evaluation across model seeds\\n\")\n",
    "    # If you really want to include patch_size (only exists for ViT/CNN), guard it:\n",
    "    if hasattr(study, \"best_params\") and \"patch_size\" in study.best_params:\n",
    "        f.write(f\"patch_size: {study.best_params['patch_size']}\\n\")\n",
    "    f.write(f\"seeds: {model_seeds}\\n\")\n",
    "    f.write(\"per_seed_metrics:\\n\")\n",
    "    for s, m in zip(model_seeds, per_seed_metrics):\n",
    "        f.write(f\"  - seed: {s}\\n\")\n",
    "        for k in numeric_keys:\n",
    "            f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "    f.write(\"aggregates:\\n\")\n",
    "    for k, mm in aggregates.items():\n",
    "        f.write(f\"  {k}:\\n\")\n",
    "        f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "        f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "# Console summary (pick a sensible key)\n",
    "pref_key = None\n",
    "if task_type.lower() == \"binary\":\n",
    "    for cand in [\"Test AUC\", \"Test Accuracy\", \"Val AUC\", \"Val Accuracy\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "elif task_type.lower() == \"multiclass\":\n",
    "    for cand in [\"Test Accuracy\", \"Val Accuracy\", \"Test F1\", \"Val F1\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "else:  # regression\n",
    "    for cand in [\"Test RMSE\", \"Val RMSE\", \"Test R2\", \"Val R2\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "\n",
    "if pref_key:\n",
    "    print(f\"→ catboost: {pref_key} = {aggregates[pref_key]['mean']:.6f} ± {aggregates[pref_key]['std']:.6f}\")\n",
    "print(f\"Saved to: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, categorical_cols, label_encoder, class_weight = preprocess_data(df, dataset_name=dataset_name, task_type=task_type, model_type=\"lightgbm\", seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "categorical_cols = config[\"categorical_cols\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Sanea NOMBRES después del preprocess\n",
    "X_train, X_val, X_test, categorical_cols, colmap = sanitize_after_preprocess(\n",
    "    X_train, X_val, X_test, categorical_cols\n",
    ")\n",
    "\n",
    "# (opcional) ver cambios\n",
    "changed = {k: v for k, v in colmap.items() if k != v}\n",
    "print(f\"Saneadas {len(changed)} columnas problemáticas.\" if changed else \"No había columnas problemáticas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Sanea NOMBRES después del preprocess\n",
    "X_train, X_val, X_test, categorical_cols, colmap = sanitize_after_preprocess(\n",
    "    X_train, X_val, X_test, categorical_cols\n",
    ")\n",
    "\n",
    "# (opcional) ver cambios\n",
    "changed = {k: v for k, v in colmap.items() if k != v}\n",
    "print(f\"Saneadas {len(changed)} columnas problemáticas.\" if changed else \"No había columnas problemáticas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_red = X_train.iloc[subset_idx].reset_index(drop=True)\n",
    "y_train_red = y_train.iloc[subset_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=direction)\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    name=\"lightgbm\",\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    X_train =X_train_red if reduce else X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    metric_name=metric_name,\n",
    "    SEED=SEED,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=None,\n",
    "    categorical_cols=categorical_cols\n",
    "), n_trials=100)\n",
    "\n",
    "# Print best result summary\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "print(f\"  AUC Score: {best_trial.value:.4f}\")\n",
    "print(\"  Best Hyperparameters:\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "\n",
    "# seeds & aggregation\n",
    "model_seeds = [0, 1, 2, 3, 4]\n",
    "numeric_keys = None\n",
    "per_seed_metrics = []\n",
    "\n",
    "# Where to save the summary file (the same folder you used before)\n",
    "summary_dir = os.path.join(save_dir, f\"lightgbm/best_model\")\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "out_file = os.path.join(summary_dir, \"best_results_mean.txt\")\n",
    "\n",
    "for s in model_seeds:\n",
    "    set_model_seed(s)  # your util to set np/torch/python seeds if needed\n",
    "    metrics = evaluate_best_model(\n",
    "        study,                 # Optuna study with .best_params\n",
    "        \"lightgbm\",                  # \"xgboost\" / \"lightgbm\" / \"catboost\" / etc.\n",
    "        task_type,\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        X_test, y_test,\n",
    "        categorical_cols=categorical_cols,\n",
    "        num_classes=num_classes,\n",
    "        SEED=s,                # <<< pass the seed into the model\n",
    "        device=device,\n",
    "        save_dir=save_dir,\n",
    "        class_weight=None\n",
    "    )\n",
    "    if not isinstance(metrics, dict):\n",
    "        raise TypeError(f\"evaluate_best_model must return dict, got {type(metrics)}\")\n",
    "\n",
    "    if numeric_keys is None:\n",
    "        numeric_keys = [k for k, v in metrics.items() if isinstance(v, (Number, np.floating, np.integer))]\n",
    "    per_seed_metrics.append(metrics)\n",
    "\n",
    "    # brief print\n",
    "    brief = \", \".join(f\"{k}={float(metrics[k]):.6f}\" for k in numeric_keys[:6])\n",
    "    print(f\"Seed {s}: {brief}\")\n",
    "\n",
    "# Aggregate mean/std\n",
    "aggregates = {}\n",
    "for k in numeric_keys:\n",
    "    vals = [float(m[k]) for m in per_seed_metrics]\n",
    "    mean_k = float(np.mean(vals))\n",
    "    std_k  = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "    aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "# Save YAML-like txt (same style as your example)\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Best trial re-evaluation across model seeds\\n\")\n",
    "    # If you really want to include patch_size (only exists for ViT/CNN), guard it:\n",
    "    if hasattr(study, \"best_params\") and \"patch_size\" in study.best_params:\n",
    "        f.write(f\"patch_size: {study.best_params['patch_size']}\\n\")\n",
    "    f.write(f\"seeds: {model_seeds}\\n\")\n",
    "    f.write(\"per_seed_metrics:\\n\")\n",
    "    for s, m in zip(model_seeds, per_seed_metrics):\n",
    "        f.write(f\"  - seed: {s}\\n\")\n",
    "        for k in numeric_keys:\n",
    "            f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "    f.write(\"aggregates:\\n\")\n",
    "    for k, mm in aggregates.items():\n",
    "        f.write(f\"  {k}:\\n\")\n",
    "        f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "        f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "# Console summary (pick a sensible key)\n",
    "pref_key = None\n",
    "if task_type.lower() == \"binary\":\n",
    "    for cand in [\"Test AUC\", \"Test Accuracy\", \"Val AUC\", \"Val Accuracy\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "elif task_type.lower() == \"multiclass\":\n",
    "    for cand in [\"Test Accuracy\", \"Val Accuracy\", \"Test F1\", \"Val F1\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "else:  # regression\n",
    "    for cand in [\"Test RMSE\", \"Val RMSE\", \"Test R2\", \"Val R2\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "\n",
    "if pref_key:\n",
    "    print(f\"→ lightgbm: {pref_key} = {aggregates[pref_key]['mean']:.6f} ± {aggregates[pref_key]['std']:.6f}\")\n",
    "print(f\"Saved to: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
