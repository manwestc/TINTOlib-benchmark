{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "# Local application/library imports\n",
    "from utils import load_search_space\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    RocCurveDisplay, PrecisionRecallDisplay,\n",
    "    ConfusionMatrixDisplay, roc_auc_score, average_precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'boston'        \n",
    "dataset_subpath = 'Regression/boston'       \n",
    "task_type = 'Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'nomao'        \n",
    "dataset_subpath = 'Binary/nomao'       \n",
    "task_type = 'Binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'cnae-9'        \n",
    "dataset_subpath = 'Multiclass/cnae-9'       \n",
    "task_type = 'Multiclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"./data/{dataset_subpath}/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce = True if len(df) > 20000 else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target_tensor(y, task):\n",
    "    task = task.lower()\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "    elif isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "        \n",
    "    if task == \"regression\" or task == \"binary\":\n",
    "        return torch.as_tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    elif task == \"multiclass\":\n",
    "        return torch.as_tensor(y, dtype=torch.long)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(df, dataset_name, task_type, seed=42, batch_size=32, device='cpu'):\n",
    "    task_type = task_type.lower()\n",
    "\n",
    "    # Load config\n",
    "    with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    categorical_cols = config[\"categorical_cols\"]\n",
    "    numerical_cols = config[\"numerical_cols\"]\n",
    "    encoding = config[\"encoding\"]\n",
    "\n",
    "    # Extract features and target\n",
    "    X = df[numerical_cols + categorical_cols].copy()\n",
    "    y = df.iloc[:, -1].copy()\n",
    "\n",
    "    # Encode target if needed\n",
    "    le = None\n",
    "    if encoding.get(\"target\") == \"label\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        label_mapping = None\n",
    "\n",
    "    # Split raw data before transformation\n",
    "    if task_type == \"regression\":\n",
    "        # For regression, we can use a simple split\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed\n",
    "        )\n",
    "        X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed\n",
    "        )\n",
    "    else:\n",
    "        # For classification, we need stratified splits\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed, stratify=y\n",
    "        )\n",
    "        X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed, stratify=y_temp\n",
    "        )\n",
    "\n",
    "    # Compute class weights for classification\n",
    "    class_weight = None\n",
    "    if task_type in [\"binary\", \"multiclass\"]:\n",
    "        # Compute raw weights\n",
    "        class_weight_values = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "        classes_sorted = np.sort(np.unique(y_train))\n",
    "        \n",
    "        if task_type == \"binary\":\n",
    "            # Compute pos_weight = weight for class 1 / weight for class 0\n",
    "            weight_dict = dict(zip(classes_sorted, class_weight_values))\n",
    "            pos_weight = weight_dict[1] / weight_dict[0]\n",
    "            class_weight = torch.tensor(pos_weight, dtype=torch.float32).to(device)\n",
    "            print(f\"Binary pos_weight (for BCEWithLogitsLoss): {class_weight.item()}\")\n",
    "\n",
    "        elif task_type == \"multiclass\":\n",
    "            class_weight = torch.tensor(class_weight_values, dtype=torch.float32).to(device)\n",
    "            print(f\"Multiclass class weights (for CrossEntropyLoss): {class_weight.tolist()}\")\n",
    "\n",
    "    # Transform numerical and categorical features\n",
    "    transformers = []\n",
    "\n",
    "    if encoding[\"numerical_features\"] == \"minmax\":\n",
    "        transformers.append((\"num\", MinMaxScaler(), numerical_cols))\n",
    "    elif encoding[\"numerical_features\"] == \"standard\":\n",
    "        transformers.append((\"num\", StandardScaler(), numerical_cols))\n",
    "\n",
    "    if categorical_cols and encoding[\"categorical_features\"] == \"onehot\":\n",
    "        transformers.append((\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), categorical_cols))\n",
    "\n",
    "    if transformers:\n",
    "        preprocessor = ColumnTransformer(transformers=transformers)\n",
    "        X_train = preprocessor.fit_transform(X_train_raw)\n",
    "        X_val = preprocessor.transform(X_val_raw)\n",
    "        X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "        # Recover transformed column names\n",
    "        if \"cat\" in preprocessor.named_transformers_:\n",
    "            cat_feature_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    "            all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "        else:\n",
    "            all_feature_names = numerical_cols + categorical_cols\n",
    "\n",
    "        X_train_num = pd.DataFrame(X_train, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val_num = pd.DataFrame(X_val, columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test_num = pd.DataFrame(X_test, columns=all_feature_names, index=X_test_raw.index)\n",
    "    else:\n",
    "        all_feature_names = numerical_cols + categorical_cols  # or keep original order\n",
    "        X_train_num = pd.DataFrame(X_train_raw, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val_num = pd.DataFrame(X_val_raw, columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test_num = pd.DataFrame(X_test_raw, columns=all_feature_names, index=X_test_raw.index)\n",
    "\n",
    "\n",
    "    print(f\"Shapes — Train: {X_train_num.shape}, Val: {X_val_num.shape}, Test: {X_test_num.shape}\")\n",
    "    print(f\"Numerical features: {len(numerical_cols)} — {numerical_cols}\")\n",
    "    print(f\"Categorical features: {len(categorical_cols)} — {categorical_cols}\")\n",
    "    print(f\"Total features: {X_train_num.shape[1]}\")\n",
    "    if label_mapping:\n",
    "        print(f\"Target label mapping: {label_mapping}\")\n",
    "    \n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "\n",
    "    print(\"Attributes: \", attributes)\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.as_tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.as_tensor(X_test_num.values, dtype=torch.float32)\n",
    "    y_train_tensor = prepare_target_tensor(y_train, task_type)\n",
    "    y_val_tensor = prepare_target_tensor(y_val, task_type)\n",
    "    y_test_tensor = prepare_target_tensor(y_test, task_type)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    #X_train_img_tensor = X_train_img_tensor / 255.0\n",
    "    #X_val_img_tensor = X_val_img_tensor / 255.0\n",
    "    #X_test_img_tensor = X_test_img_tensor / 255.0\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset( X_train_num_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_num_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_num_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes,  le, class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, attributes, params, task, num_classes=None):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        mlp_layers = []\n",
    "        input_dim = attributes\n",
    "        activation = nn.ReLU if params.get(\"activation\", \"relu\") == \"relu\" else nn.GELU\n",
    "        dropout_rate = params.get(\"dropout\", 0.0)\n",
    "\n",
    "        for hidden_dim in params[\"mlp_hidden_dims\"]:\n",
    "            mlp_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            mlp_layers.append(activation())\n",
    "            if dropout_rate > 0.0:\n",
    "                mlp_layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        # Output layer\n",
    "        output_dim = 1 if task in ['regression', 'binary'] else num_classes\n",
    "        mlp_layers.append(nn.Linear(input_dim, output_dim))\n",
    "\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, num_input):\n",
    "        return self.mlp(num_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "\n",
    "from models.utils import get_loss_fn, calculate_metrics, calculate_metrics_from_numpy, get_class_weighted_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "def compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, \n",
    "                    model_name, trial_name=None, task='regression', epochs=200, max_lr=1, \n",
    "                    div_factor=10, final_div_factor=1, device='cuda', weight_decay=1e-2, pct_start=0.3, save_model=False, class_weights=None, save_dir=None, study=None, verbose=False):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if class_weights != None:\n",
    "        loss_fn = get_class_weighted_loss_fn(task, class_weights)\n",
    "    else:\n",
    "        loss_fn = get_loss_fn(task)\n",
    "\n",
    "    # Compute min_lr from max_lr and div_factor\n",
    "    min_lr = max_lr / div_factor\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=div_factor, final_div_factor=final_div_factor, total_steps=total_steps, pct_start=pct_start, anneal_strategy=\"cos\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "    #early_stopping_counter = 0\n",
    "    #patience = 10  # Early stopping patience\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    if task == 'regression':\n",
    "        history.update({'train_mse': [],  'val_mse': [], 'train_mae': [],  'val_mae': [], 'train_rmse': [], 'val_rmse': [], 'train_r2': [], 'val_r2': []})\n",
    "    elif task in ['binary', 'multiclass']:\n",
    "        history.update({'train_accuracy': [], 'val_accuracy': [], 'train_precision': [], 'val_precision': [], 'train_recall': [], 'val_recall': [], 'train_f1': [], 'val_f1': []})\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "\n",
    "        for num_data, targets in train_loader:\n",
    "            num_data, targets = num_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(num_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        if task == 'multiclass':\n",
    "            y_train_pred = np.vstack(train_preds)\n",
    "            y_train_true = train_targets\n",
    "        else:\n",
    "            y_train_pred = np.concatenate(train_preds)\n",
    "            y_train_true = np.concatenate(train_targets)\n",
    "            \n",
    "        train_metrics = calculate_metrics_from_numpy(y_train_true, y_train_pred, task)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for num_data, targets in val_loader:\n",
    "                num_data, targets = num_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                outputs = model(num_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        if task == 'multiclass':\n",
    "            y_val_pred = np.vstack(val_preds)\n",
    "            y_val_true = val_targets\n",
    "        else:\n",
    "            y_val_pred = np.concatenate(val_preds)\n",
    "            y_val_true = np.concatenate(val_targets)\n",
    "        \n",
    "        val_metrics = calculate_metrics_from_numpy(y_val_true, y_val_pred, task)\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "        for k, v in train_metrics.items():\n",
    "            history[f'train_{k}'].append(v)\n",
    "        for k, v in val_metrics.items():\n",
    "            history[f'val_{k}'].append(v)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "            #early_stopping_counter = 0\n",
    "        #else:\n",
    "        #    early_stopping_counter += 1\n",
    "        #    if early_stopping_counter >= patience:\n",
    "        #        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        #        break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Recompute metrics using the best model\n",
    "    train_metrics, y_true_train, y_pred_train, y_prob_train = calculate_metrics(model, train_loader, device, class_weights, task)\n",
    "    val_metrics, y_true_val, y_pred_val, y_prob_val  = calculate_metrics(model, val_loader, device, class_weights, task)\n",
    "    test_metrics, y_true_test, y_pred_test, y_prob_test = calculate_metrics(model, test_loader, device, class_weights, task)\n",
    "\n",
    "    # Store recomputed metrics\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "\n",
    "    # Add task-specific metrics\n",
    "    for k in train_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'train_{k}'] = train_metrics[k]\n",
    "    for k in val_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'val_{k}'] = val_metrics[k]\n",
    "    for k in test_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'test_{k}'] = test_metrics[k]\n",
    "    \n",
    "    if verbose:   \n",
    "        print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "        print(f\"Best model found at epoch {best_epoch}/{epochs}\")\n",
    "        print(f\"Best Train Loss: {metrics['train_loss']:.4f}, Best Val Loss: {metrics['val_loss']:.4f}\")\n",
    "        print(metrics)\n",
    "    \n",
    "    if save_model:\n",
    "        save_path = os.path.join(save_dir, f\"{model_name}/best_model/{trial_name}\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        plot_metric(history['train_loss'], history['val_loss'], 'Loss', save_path)\n",
    "        if task == 'regression':\n",
    "            plot_metric(history['train_mse'], history['val_mse'], 'MSE', save_path)\n",
    "            plot_metric(history['train_rmse'], history['val_rmse'], 'RMSE', save_path)\n",
    "        else:\n",
    "            plot_metric(history['train_accuracy'], history['val_accuracy'], 'Accuracy', save_path)\n",
    "            plot_metric(history['train_f1'], history['val_f1'], 'F1', save_path)\n",
    "\n",
    "        plot_learning_rate(history['learning_rate'], save_path)\n",
    "\n",
    "        # Save metrics\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        with open(f'{save_path}/best_model_metrics.txt', 'w') as f:\n",
    "            for key, value in metrics.items():\n",
    "                f.write(f'{key}: {value}\\n')\n",
    "\n",
    "        # Save model\n",
    "        torch.save(best_model, f\"{save_path}/best_model.pth\")\n",
    "        print(f\"Best model saved to {save_path}/best_model.pth\")\n",
    "\n",
    "        # Additional plots for classification\n",
    "        if task in [\"binary\"]:\n",
    "            plot_extra(\"Train\", y_true_train, y_pred_train, y_prob_train, save_path)\n",
    "            plot_extra(\"Validation\", y_true_val, y_pred_val, y_prob_val, save_path)\n",
    "            plot_extra(\"Test\", y_true_test, y_pred_test, y_prob_test, save_path)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_extra(split_name, y_true, y_pred, y_prob, save_path):\n",
    "    y_true = y_true.ravel()\n",
    "    y_pred = y_pred.ravel()\n",
    "\n",
    "    # ROC Curve\n",
    "    RocCurveDisplay.from_predictions(y_true, y_prob)\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
    "    plt.title(f\"{split_name} ROC Curve (AUC = {auc_score:.2f})\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_roc_curve.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    PrecisionRecallDisplay.from_predictions(y_true, y_prob)\n",
    "    avg_prec = average_precision_score(y_true, y_prob)\n",
    "    plt.title(f\"{split_name} PR Curve (AP = {avg_prec:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_pr_curve.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Normalized confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true').plot(cmap='Blues')\n",
    "    plt.title(f\"{split_name} Confusion Matrix (Normalized)\")\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_confusion_matrix_normalized.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Raw confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize=None).plot(cmap='Blues')\n",
    "    plt.title(f\"{split_name} Confusion Matrix (Counts)\")\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_confusion_matrix_counts.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "\n",
    "def plot_metric(train_metric, val_metric, metric_name, save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(train_metric, label=f'Train {metric_name}')\n",
    "    plt.plot(val_metric, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric_name} vs. Epoch')\n",
    "    save_path = f\"{save_path}/{metric_name.lower()}_plot.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(\"all\")\n",
    "\n",
    "def plot_learning_rate(learning_rates, save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate vs. Epoch')\n",
    "    save_path = f\"{save_path}/learning_rate_plot.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir =  os.path.join(\"logs\", task_type, dataset_name)\n",
    "model_name = \"mlp\"\n",
    "\n",
    "# Load config\n",
    "with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "batch_size = config[\"batch_size\"]\n",
    "epochs = 50\n",
    "n_trials = 100\n",
    "\n",
    "if task_type.lower() == 'multiclass':\n",
    "    num_classes = df.iloc[:,-1].nunique()\n",
    "else:\n",
    "    num_classes = 1\n",
    "\n",
    "device='cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model_name, task_type, \n",
    "              train_loader, val_loader, test_loader,\n",
    "              attributes, num_classes=None,\n",
    "              device='cuda', save_dir=None, class_weight=None, epochs=100):\n",
    "    \n",
    "    task = task_type.lower()\n",
    "    \n",
    "    params = load_search_space(model_name, trial)\n",
    "\n",
    "    params[\"mlp_hidden_dims\"] = json.loads(params[\"mlp_hidden_dims\"])\n",
    "    \n",
    "    with open(f\"configs/optuna_search/{model_name}.json\", \"r\") as f:\n",
    "        full_config = json.load(f)\n",
    "\n",
    "    config = full_config[model_name][\"fit\"]  # Access the model key\n",
    "\n",
    "    # Build and train model\n",
    "    model = MLP(attributes, params, task, num_classes)\n",
    "    metrics = compile_and_fit(\n",
    "        model,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=f\"trial_{trial.number}\",\n",
    "        task=task,  # assumed to be defined externally\n",
    "        max_lr=trial.suggest_float(\"max_lr\", config[\"max_lr\"][1], config[\"max_lr\"][2], log=True),\n",
    "        div_factor=trial.suggest_int(\"div_factor\", config[\"div_factor\"][1], config[\"div_factor\"][2]),\n",
    "        final_div_factor=trial.suggest_int(\"final_div_factor\", config[\"final_div_factor\"][1], config[\"final_div_factor\"][2]),\n",
    "        weight_decay=trial.suggest_float(\"weight_decay\", config[\"weight_decay\"][1], config[\"weight_decay\"][2], log=True),\n",
    "        pct_start=trial.suggest_float(\"pct_start\", config[\"pct_start\"][1], config[\"pct_start\"][2]),\n",
    "        epochs=epochs,\n",
    "        save_model=False,\n",
    "        class_weights=class_weight\n",
    "    )\n",
    "\n",
    "    save_dir = os.path.join(save_dir, model_name, \"optuna\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if task == 'regression':\n",
    "        score = metrics[\"val_rmse\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-RMSE: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    elif task == 'binary':\n",
    "        score = metrics[\"val_roc_auc\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-AUC: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    elif task == 'multiclass':\n",
    "        score = metrics[\"val_accuracy\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-Accuracy: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === benchmark_eval_mlp.py (tabular MLP; mirrors vision-only format) ==========\n",
    "from numbers import Number\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import optuna\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# NEW: optional calflops (robust fallback if not installed)\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    from calflops import calculate_flops as _calflops_calc\n",
    "    _HAVE_CALFLOPS = True\n",
    "except Exception:\n",
    "    _HAVE_CALFLOPS = False\n",
    "\n",
    "# -------- config --------\n",
    "TOP_K = 5\n",
    "SINGLE_PASS_SEED = 0              # seed for one-time eval of top-K\n",
    "FINAL_SEEDS = [0, 1, 2, 3, 4]     # seeds for the final winner\n",
    "FULL_EPOCHS = 100\n",
    "\n",
    "# -------- helpers --------\n",
    "def is_minimize_study(study):\n",
    "    try:\n",
    "        return study.direction == optuna.study.StudyDirection.MINIMIZE\n",
    "    except Exception:\n",
    "        try:\n",
    "            return study.directions[0] == optuna.study.StudyDirection.MINIMIZE\n",
    "        except Exception:\n",
    "            return True  # fallback\n",
    "\n",
    "def primary_val_key_for_task(task_type):\n",
    "    t = task_type.lower()\n",
    "    if t == \"regression\":   # lower is better\n",
    "        return \"val_rmse\", True\n",
    "    if t == \"binary\":       # higher is better\n",
    "        return \"val_roc_auc\", False\n",
    "    if t == \"multiclass\":   # higher is better\n",
    "        return \"val_accuracy\", False\n",
    "    return \"val_loss\", True\n",
    "\n",
    "def sort_trials(trials, minimize):\n",
    "    return sorted(trials, key=lambda t: t.value, reverse=not minimize)\n",
    "\n",
    "def metric_or_default(m, key, minimize):\n",
    "    if key in m and isinstance(m[key], (Number, np.floating, np.integer)):\n",
    "        return float(m[key])\n",
    "    return (np.inf if minimize else -np.inf)\n",
    "\n",
    "def _ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "# ---- NEW: params + FLOPs helpers (parity with vision file) -------------------\n",
    "def _count_params(model: nn.Module, trainable_only: bool = False) -> int:\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def _humanize(n: float, unit: str = \"\") -> str:\n",
    "    try:\n",
    "        n = float(n)\n",
    "        for u in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "            if abs(n) < 1000.0:\n",
    "                return f\"{n:.3f}{u}{unit}\"\n",
    "            n /= 1000.0\n",
    "        return f\"{n:.3f}E{unit}\"\n",
    "    except Exception:\n",
    "        return str(n)\n",
    "    \n",
    "def _try_compute_flops_tabular(model, n_features: int, batch_size: int = 1):\n",
    "    \"\"\"\n",
    "    Try real FLOPs/MACs via calflops using a 2D input (B, D).\n",
    "    Returns dict or None.\n",
    "    \"\"\"\n",
    "    if not _HAVE_CALFLOPS:\n",
    "        return None\n",
    "    try:\n",
    "        flops, macs, params_cf = _calflops_calc(\n",
    "            model=model,\n",
    "            input_shape=(int(batch_size), int(n_features)),\n",
    "            output_as_string=False\n",
    "        )\n",
    "        out = {\n",
    "            \"flops\": float(flops),\n",
    "            \"macs\": float(macs),\n",
    "            \"params_from_calflops\": float(params_cf),\n",
    "            \"flops_str\": _humanize(flops),\n",
    "            \"macs_str\": _humanize(macs),\n",
    "        }\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ========================= EVAL FN (mirrors vision style) =====================\n",
    "def evaluate_best_model(best_trial, train_loader, val_loader, test_loader, \n",
    "                        dataset_name, task_type, save_dir, attributes, trial_name,\n",
    "                        class_weight=None, num_classes=None, epochs=10):\n",
    "    \"\"\"\n",
    "    MLP-only evaluation that mirrors your vision-only evaluate_best_model:\n",
    "      - Builds MLP from best_trial params\n",
    "      - Saves param counts + FLOPs (calflops if available; analytic fallback)\n",
    "      - Trains and returns metrics augmented with capacity + compute\n",
    "    Expects these to exist in the outer scope:\n",
    "      - model_name: name used for save directories\n",
    "      - MLP: your nn.Module class\n",
    "      - compile_and_fit: your train/eval routine\n",
    "      - set_model_seed: your seeding helper\n",
    "    \"\"\"\n",
    "    task = task_type.lower()\n",
    "    best_params = best_trial.params\n",
    "\n",
    "    print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "    print(f\"  Best Score: {best_trial.value:.4f}\")\n",
    "    print(\"  Best Hyperparameters:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"    {k}: {v}\")\n",
    "\n",
    "    # ----- Extract architecture-related parameters -----\n",
    "    architecture_params = {k: v for k, v in best_params.items()\n",
    "                           if k in [\"mlp_hidden_dims\", \"activation\", \"dropout\"]}\n",
    "\n",
    "    # Convert JSON string to list if necessary\n",
    "    if isinstance(architecture_params.get(\"mlp_hidden_dims\"), str):\n",
    "        try:\n",
    "            architecture_params[\"mlp_hidden_dims\"] = json.loads(architecture_params[\"mlp_hidden_dims\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ----- Initialize model -----\n",
    "    # Expecting MLP(input_dim=attributes, params, task, num_classes)\n",
    "    model = MLP(attributes, architecture_params, task, num_classes)\n",
    "\n",
    "    # ----- Count params -----\n",
    "    total_params = _count_params(model, trainable_only=False)\n",
    "    trainable_params = _count_params(model, trainable_only=True)\n",
    "    print(f\"  Params: total={total_params:,}  trainable={trainable_params:,}\")\n",
    "\n",
    "    # ----- NEW: FLOPs/MACs (prefer calflops; fallback to analytic per-sample) -----\n",
    "    flops_info = _try_compute_flops_tabular(model, n_features=attributes, batch_size=1)\n",
    "    if flops_info is not None:\n",
    "        print(f\"  FLOPs: {flops_info['flops_str']}  MACs: {flops_info['macs_str']}\")\n",
    "\n",
    "    # Save full best params + counts (+ FLOPs) so you keep exact tuned config\n",
    "    base_dir = _ensure_dir(os.path.join(save_dir, f\"{model_name}/best_model/{trial_name}\"))\n",
    "    best_params_with_counts = dict(best_params)\n",
    "    best_params_with_counts[\"total_params\"] = int(total_params)\n",
    "    best_params_with_counts[\"trainable_params\"] = int(trainable_params)\n",
    "    if flops_info is not None:\n",
    "        best_params_with_counts[\"flops\"] = flops_info[\"flops\"]\n",
    "        best_params_with_counts[\"macs\"] = flops_info[\"macs\"]\n",
    "        best_params_with_counts[\"flops_str\"] = flops_info[\"flops_str\"]\n",
    "        best_params_with_counts[\"macs_str\"] = flops_info[\"macs_str\"]\n",
    "    with open(os.path.join(base_dir, \"best_params.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params_with_counts, f, indent=4)\n",
    "\n",
    "    # Also attach to the trial for later inspection (optional)\n",
    "    try:\n",
    "        best_trial.set_user_attr(\"total_params\", int(total_params))\n",
    "        best_trial.set_user_attr(\"trainable_params\", int(trainable_params))\n",
    "        best_trial.set_user_attr(\"flops\", float(best_params_with_counts.get(\"flops\", np.nan)))\n",
    "        best_trial.set_user_attr(\"macs\", float(best_params_with_counts.get(\"macs\", np.nan)))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ----- Train & evaluate -----\n",
    "    metrics = compile_and_fit(\n",
    "        model,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=model_name,\n",
    "        trial_name=f\"trial_{best_trial.number}\",\n",
    "        task=task,\n",
    "        max_lr=best_params[\"max_lr\"],\n",
    "        div_factor=best_params[\"div_factor\"],\n",
    "        final_div_factor=best_params[\"final_div_factor\"],\n",
    "        weight_decay=best_params[\"weight_decay\"],\n",
    "        pct_start=best_params[\"pct_start\"],\n",
    "        epochs=epochs,\n",
    "        save_model=True,\n",
    "        class_weights=class_weight,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "    # ----- Augment metrics with capacity + compute -----\n",
    "    # Augment metrics with parameter counts + FLOPs\n",
    "    metrics[\"total_params\"] = int(total_params)\n",
    "    metrics[\"trainable_params\"] = int(trainable_params)\n",
    "    if flops_info is not None:\n",
    "        metrics[\"flops\"] = flops_info[\"flops\"]\n",
    "        metrics[\"macs\"] = flops_info[\"macs\"]\n",
    "        metrics[\"flops_str\"] = flops_info[\"flops_str\"]\n",
    "        metrics[\"macs_str\"] = flops_info[\"macs_str\"]\n",
    "    return metrics\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Top-K → single-pass → winner → multi-seed driver (MLP-only)\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_topk_and_multiseed_mlp(\n",
    "    study, model_name, dataset_name, task_type, save_dir,\n",
    "    attributes, num_classes, class_weight,\n",
    "    train_loader, val_loader, test_loader\n",
    "):\n",
    "    \"\"\"\n",
    "    MLP-only version (no ViT branches). Saves/prints params and compute if present.\n",
    "    Expects:\n",
    "      - evaluate_best_model(...) defined in this file\n",
    "      - set_model_seed(...)\n",
    "      - helpers: is_minimize_study, sort_trials, primary_val_key_for_task, metric_or_default, _ensure_dir\n",
    "    \"\"\"\n",
    "    minimize = is_minimize_study(study)\n",
    "    completed = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    if not completed:\n",
    "        raise RuntimeError(\"No completed trials in the study.\")\n",
    "\n",
    "    top_trials = sort_trials(completed, minimize)[:TOP_K]\n",
    "    primary_key, primary_minimize = primary_val_key_for_task(task_type)\n",
    "    maximize_primary = not primary_minimize\n",
    "\n",
    "    print(f\"\\nEvaluating top-{len(top_trials)} trials once at {FULL_EPOCHS} epochs (seed={SINGLE_PASS_SEED})...\\n\")\n",
    "\n",
    "    # Single-pass over top-K\n",
    "    single_pass_results = []  # list of (trial, trial_name, metrics_dict)\n",
    "    for trial in top_trials:\n",
    "        trial_name = f\"trial_{trial.number}\"\n",
    "        header = f\"(Trial {trial.number}, ValObjective: {trial.value:.4f})\"\n",
    "\n",
    "        print(f\"→ Single-pass full run {header}\")\n",
    "        set_model_seed(SINGLE_PASS_SEED)\n",
    "\n",
    "        metrics = evaluate_best_model(\n",
    "            best_trial=trial,\n",
    "            train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "            dataset_name=dataset_name, task_type=task_type,\n",
    "            save_dir=save_dir, attributes=attributes,\n",
    "            trial_name=trial_name, class_weight=class_weight, num_classes=num_classes,\n",
    "            epochs=FULL_EPOCHS\n",
    "        )\n",
    "        if not isinstance(metrics, dict):\n",
    "            raise TypeError(f\"evaluate_best_model must return dict, got: {type(metrics)}\")\n",
    "\n",
    "        # brief printout (primary + params + compute)\n",
    "        if primary_key in metrics:\n",
    "            print(f\"   {primary_key}={float(metrics[primary_key]):.6f}\")\n",
    "        tp = metrics.get(\"total_params\"); trp = metrics.get(\"trainable_params\")\n",
    "        if tp is not None:\n",
    "            print(f\"   params: total={tp:,}, trainable={trp:,}\")\n",
    "        if \"flops_str\" in metrics and \"macs_str\" in metrics:\n",
    "            print(f\"   flops={metrics['flops_str']}, macs={metrics['macs_str']}\")\n",
    "\n",
    "        single_pass_results.append((trial, trial_name, metrics))\n",
    "\n",
    "    # Winner by primary metric\n",
    "    if maximize_primary:\n",
    "        winner_tuple = max(single_pass_results, key=lambda x: metric_or_default(x[2], primary_key, primary_minimize))\n",
    "    else:\n",
    "        winner_tuple = min(single_pass_results, key=lambda x: metric_or_default(x[2], primary_key, primary_minimize))\n",
    "\n",
    "    best_trial, best_trial_name, best_single_metrics = winner_tuple\n",
    "    best_primary_val = metric_or_default(best_single_metrics, primary_key, primary_minimize)\n",
    "    print(f\"\\nWinner after single-pass: Trial {best_trial.number} ({best_trial_name}) \"\n",
    "          f\"by {primary_key}={best_primary_val:.6f}\")\n",
    "\n",
    "    # Multi-seed evaluation of winner\n",
    "    print(f\"\\nRe-running winner with seeds {final_seeds} at {FULL_EPOCHS} epochs...\\n\")\n",
    "    winner_save_path = _ensure_dir(os.path.join(save_dir, f\"{model_name}/best_model/{best_trial_name}\"))\n",
    "\n",
    "    per_seed_metrics = []\n",
    "    numeric_keys = None\n",
    "\n",
    "    for s in FINAL_SEEDS:\n",
    "        set_model_seed(s)\n",
    "        m = evaluate_best_model(\n",
    "            best_trial=best_trial,\n",
    "            train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "            dataset_name=dataset_name, task_type=task_type,\n",
    "            save_dir=save_dir, attributes=attributes,\n",
    "            trial_name=f\"{best_trial_name}_seed{s}\", class_weight=class_weight, num_classes=num_classes,\n",
    "            epochs=FULL_EPOCHS\n",
    "        )\n",
    "        if not isinstance(m, dict):\n",
    "            raise TypeError(f\"evaluate_best_model must return dict, got: {type(m)}\")\n",
    "\n",
    "        if numeric_keys is None:\n",
    "            numeric_keys = [k for k, v in m.items() if isinstance(v, (Number, np.floating, np.integer))]\n",
    "        per_seed_metrics.append(m)\n",
    "\n",
    "        pk_val = metric_or_default(m, primary_key, primary_minimize)\n",
    "        extras = []\n",
    "        for k in [\"test_loss\", \"test_accuracy\", \"test_roc_auc\", \"test_rmse\", \"val_loss\"]:\n",
    "            if k in m and isinstance(m[k], (Number, np.floating, np.integer)):\n",
    "                extras.append(f\"{k}={float(m[k]):.6f}\")\n",
    "        print(f\"   Seed {s}: {primary_key}={pk_val:.6f}\" + (\", \" + \", \".join(extras) if extras else \"\"))\n",
    "\n",
    "    # Aggregate across seeds\n",
    "    aggregates = {}\n",
    "    for k in (numeric_keys or []):\n",
    "        vals = [float(m[k]) for m in per_seed_metrics if k in m]\n",
    "        if not vals:\n",
    "            continue\n",
    "        mean_k = float(np.mean(vals))\n",
    "        std_k = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "        aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "    # Param counts (same across seeds) & FLOPs (take from single-pass winner)\n",
    "    winner_total_params = best_single_metrics.get(\"total_params\")\n",
    "    winner_train_params = best_single_metrics.get(\"trainable_params\")\n",
    "    winner_flops_str = best_single_metrics.get(\"flops_str\")\n",
    "    winner_macs_str  = best_single_metrics.get(\"macs_str\")\n",
    "    winner_flops_num = best_single_metrics.get(\"flops\")\n",
    "    winner_macs_num  = best_single_metrics.get(\"macs\")\n",
    "\n",
    "    # Save summary\n",
    "    out_file = os.path.join(winner_save_path, \"winner_multi_seed_summary.txt\")\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# Final winner multi-seed evaluation (MLP/tabular)\\n\")\n",
    "        f.write(f\"trial_number: {best_trial.number}\\n\")\n",
    "        f.write(f\"primary_metric: {primary_key}\\n\")\n",
    "        f.write(f\"seeds: {final_seeds}\\n\")\n",
    "        if winner_flops_str is not None and winner_macs_str is not None:\n",
    "            f.write(\"compute:\\n\")\n",
    "            f.write(f\"  flops: {winner_flops_str}\\n\")\n",
    "            f.write(f\"  macs: {winner_macs_str}\\n\")\n",
    "            if winner_flops_num is not None and winner_macs_num is not None:\n",
    "                f.write(f\"  flops_num: {winner_flops_num}\\n\")\n",
    "                f.write(f\"  macs_num: {winner_macs_num}\\n\")\n",
    "        f.write(\"per_seed_metrics:\\n\")\n",
    "        for s, m in zip(final_seeds, per_seed_metrics):\n",
    "            f.write(f\"  - seed: {s}\\n\")\n",
    "            for k in (numeric_keys or []):\n",
    "                if k in m:\n",
    "                    f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "        f.write(\"aggregates:\\n\")\n",
    "        for k, mm in aggregates.items():\n",
    "            f.write(f\"  {k}:\\n\")\n",
    "            f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "            f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "    # Console summary\n",
    "    if primary_key in aggregates:\n",
    "        print(f\"\\nWinner aggregated {primary_key}: {aggregates[primary_key]['mean']:.6f} \"\n",
    "              f\"± {aggregates[primary_key]['std']:.6f}\")\n",
    "    elif \"val_loss\" in aggregates:\n",
    "        print(f\"\\nWinner aggregated val_loss: {aggregates['val_loss']['mean']:.6f} \"\n",
    "              f\"± {aggregates['val_loss']['std']:.6f}\")\n",
    "    if winner_total_params is not None:\n",
    "        print(f\"Model params: total={winner_total_params:,}, trainable={winner_train_params:,}\")\n",
    "    if winner_flops_str and winner_macs_str:\n",
    "        print(f\"Compute: FLOPs={winner_flops_str}, MACs={winner_macs_str}\")\n",
    "    print(f\"Saved multi-seed summary to: {out_file}\")\n",
    "\n",
    "    return {\n",
    "        \"winner_trial_number\": best_trial.number,\n",
    "        \"winner_trial_name\": best_trial_name,\n",
    "        \"primary_metric\": primary_key,\n",
    "        \"aggregates\": aggregates,\n",
    "        \"total_params\": winner_total_params,\n",
    "        \"trainable_params\": winner_train_params,\n",
    "        \"flops\": winner_flops_num,\n",
    "        \"macs\": winner_macs_num,\n",
    "        \"summary_path\": out_file,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_model_seed(seed: int):\n",
    "    # Python built-in RNG\n",
    "    random.seed(seed)\n",
    "    # NumPy RNG\n",
    "    np.random.seed(seed)\n",
    "    # Torch RNG\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you use multi-GPU\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def reduce_dataloader(train_loader, fraction=0.25, stratify=True, seed=42):\n",
    "    \"\"\"\n",
    "    Return a new DataLoader that draws from ~fraction of the original train dataset.\n",
    "    For classification (TensorDataset(..., y)), uses a stratified subsample.\n",
    "    \"\"\"\n",
    "    assert 0 < fraction <= 1.0\n",
    "    ds = train_loader.dataset\n",
    "    n = len(ds)\n",
    "    num_keep = max(1, int(round(n * fraction)))\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    # Try stratified pick if labels are available (TensorDataset last tensor is y)\n",
    "    subset_idx = None\n",
    "    if stratify and hasattr(ds, \"tensors\") and len(ds.tensors) >= 2:\n",
    "        y = ds.tensors[-1].cpu().numpy().ravel()\n",
    "        try:\n",
    "            from sklearn.model_selection import StratifiedShuffleSplit\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, train_size=fraction, random_state=seed)\n",
    "            chosen, _ = next(sss.split(idx, y))\n",
    "            subset_idx = idx[chosen]\n",
    "        except Exception:\n",
    "            subset_idx = None  # fallback to random below\n",
    "\n",
    "    # Fallback: random subset with a fixed seed\n",
    "    if subset_idx is None:\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        subset_idx = torch.randperm(n, generator=g)[:num_keep].tolist()\n",
    "\n",
    "    # Build subset dataset and a new DataLoader (reuse original loader settings)\n",
    "    subset = Subset(ds, subset_idx)  # official Subset utility\n",
    "    new_loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=train_loader.batch_size,\n",
    "        shuffle=True,                               # shuffle within the subset\n",
    "        num_workers=getattr(train_loader, \"num_workers\", 0),\n",
    "        pin_memory=getattr(train_loader, \"pin_memory\", False),\n",
    "        drop_last=getattr(train_loader, \"drop_last\", False),\n",
    "        persistent_workers=getattr(train_loader, \"persistent_workers\", False),\n",
    "    )\n",
    "    return new_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, task_type, seed=SEED, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(\n",
    "    lambda trial: objective(\n",
    "        trial=trial,\n",
    "        model_name=model_name,\n",
    "        task_type=task_type,\n",
    "        num_classes=num_classes,\n",
    "        train_loader=reduce_dataloader(train_loader) if reduce else train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        attributes=attributes,\n",
    "        device=device,\n",
    "        save_dir=save_dir,\n",
    "        class_weight=None,\n",
    "        epochs=epochs,\n",
    "    ),\n",
    "    n_trials=n_trials,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import optuna\n",
    "\n",
    "# -------- config --------\n",
    "TOP_K = 5\n",
    "single_pass_seed = 0              # seed for the one-time eval of top-5\n",
    "final_seeds = [0, 1, 2, 3, 4]     # seeds for the final winner\n",
    "FULL_EPOCHS = 100\n",
    "\n",
    "# -------- helpers --------\n",
    "def is_minimize_study(study):\n",
    "    try:\n",
    "        return study.direction == optuna.study.StudyDirection.MINIMIZE\n",
    "    except Exception:\n",
    "        try:\n",
    "            return study.directions[0] == optuna.study.StudyDirection.MINIMIZE\n",
    "        except Exception:\n",
    "            return True  # fallback\n",
    "\n",
    "def primary_val_key_for_task(task_type):\n",
    "    t = task_type.lower()\n",
    "    if t == \"regression\":   # lower is better\n",
    "        return \"val_rmse\", True\n",
    "    if t == \"binary\":       # higher is better\n",
    "        return \"val_roc_auc\", False\n",
    "    if t == \"multiclass\":   # higher is better\n",
    "        return \"val_accuracy\", False\n",
    "    return \"val_loss\", True\n",
    "\n",
    "def sort_trials(trials, minimize):\n",
    "    return sorted(trials, key=lambda t: t.value, reverse=not minimize)\n",
    "\n",
    "# -------- pick top-5 completed --------\n",
    "minimize = is_minimize_study(study)\n",
    "completed = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "if not completed:\n",
    "    raise RuntimeError(\"No completed trials in the study.\")\n",
    "\n",
    "top_trials = sort_trials(completed, minimize)[:TOP_K]\n",
    "primary_key, primary_minimize = primary_val_key_for_task(task_type)\n",
    "maximize_primary = not primary_minimize\n",
    "\n",
    "print(f\"\\nEvaluating top-{len(top_trials)} trials once at {FULL_EPOCHS} epochs (seed={single_pass_seed})...\\n\")\n",
    "\n",
    "# -------- evaluate each top trial once (single seed) --------\n",
    "single_pass_results = []  # list of (trial, trial_name, metrics_dict)\n",
    "\n",
    "for trial in top_trials:\n",
    "    if model_name == \"ViT_hybrid\":\n",
    "        best_patch = trial.params.get(\"patch_size\", None)\n",
    "        trial_name = f\"trial_{trial.number}_patch{best_patch}\"\n",
    "        header = f\"(Trial {trial.number}, ValObjective: {trial.value:.4f}, patch_size={best_patch})\"\n",
    "    else:\n",
    "        trial_name = f\"trial_{trial.number}\"\n",
    "        header = f\"(Trial {trial.number}, ValObjective: {trial.value:.4f})\"\n",
    "\n",
    "    print(f\"→ Single-pass full run {header}\")\n",
    "\n",
    "    # optional: fix the seed inside your eval (only if your function accepts it)\n",
    "    set_model_seed(single_pass_seed)\n",
    "    metrics = evaluate_best_model(\n",
    "        trial,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        task_type=task_type,\n",
    "        save_dir=save_dir,\n",
    "        attributes=attributes,\n",
    "        class_weight=None,\n",
    "        num_classes=num_classes,\n",
    "        epochs=FULL_EPOCHS,\n",
    "        trial_name=trial_name\n",
    "        # pass model_seed=single_pass_seed if evaluate_best_model supports it\n",
    "    )\n",
    "    if not isinstance(metrics, dict):\n",
    "        raise TypeError(f\"evaluate_best_model must return dict, got: {type(metrics)}\")\n",
    "\n",
    "    # brief printout\n",
    "    if primary_key in metrics:\n",
    "        print(f\"   {primary_key}={float(metrics[primary_key]):.6f}\")\n",
    "    single_pass_results.append((trial, trial_name, metrics))\n",
    "\n",
    "# -------- choose winner by primary metric from single-pass --------\n",
    "def metric_or_default(m, key, minimize):\n",
    "    if key in m and isinstance(m[key], (Number, np.floating, np.integer)):\n",
    "        return float(m[key])\n",
    "    # if missing, push it to worst side so it won't win\n",
    "    return (np.inf if minimize else -np.inf)\n",
    "\n",
    "if maximize_primary:\n",
    "    winner_tuple = max(single_pass_results, key=lambda x: metric_or_default(x[2], primary_key, primary_minimize))\n",
    "else:\n",
    "    winner_tuple = min(single_pass_results, key=lambda x: metric_or_default(x[2], primary_key, primary_minimize))\n",
    "\n",
    "best_trial, best_trial_name, best_single_metrics = winner_tuple\n",
    "print(f\"\\nWinner after single-pass: Trial {best_trial.number} ({best_trial_name}) \"\n",
    "      f\"by {primary_key}={metric_or_default(best_single_metrics, primary_key, primary_minimize):.6f}\")\n",
    "\n",
    "# -------- now re-run the winner with multiple seeds and aggregate --------\n",
    "print(f\"\\nRe-running winner with seeds {final_seeds} at {FULL_EPOCHS} epochs...\\n\")\n",
    "winner_save_path = os.path.join(save_dir, f\"{model_name}/best_model/{best_trial_name}\")\n",
    "os.makedirs(winner_save_path, exist_ok=True)\n",
    "\n",
    "per_seed_metrics = []\n",
    "numeric_keys = None\n",
    "\n",
    "for s in final_seeds:\n",
    "    set_model_seed(s)\n",
    "    m = evaluate_best_model(\n",
    "        best_trial,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        task_type=task_type,\n",
    "        save_dir=save_dir,\n",
    "        attributes=attributes,\n",
    "        class_weight=None,\n",
    "        num_classes=num_classes,\n",
    "        epochs=FULL_EPOCHS,\n",
    "        trial_name=f\"{best_trial_name}_seed{s}\"\n",
    "        # pass model_seed=s if your function supports it\n",
    "    )\n",
    "    if not isinstance(m, dict):\n",
    "        raise TypeError(f\"evaluate_best_model must return dict, got: {type(m)}\")\n",
    "    if numeric_keys is None:\n",
    "        numeric_keys = [k for k, v in m.items() if isinstance(v, (Number, np.floating, np.integer))]\n",
    "    per_seed_metrics.append(m)\n",
    "\n",
    "    # quick line\n",
    "    pk_val = metric_or_default(m, primary_key, primary_minimize)\n",
    "    extras = []\n",
    "    for k in [\"test_loss\", \"test_accuracy\", \"test_roc_auc\", \"test_rmse\", \"val_loss\"]:\n",
    "        if k in m and isinstance(m[k], (Number, np.floating, np.integer)):\n",
    "            extras.append(f\"{k}={float(m[k]):.6f}\")\n",
    "    print(f\"   Seed {s}: {primary_key}={pk_val:.6f}\" + (\", \" + \", \".join(extras) if extras else \"\"))\n",
    "\n",
    "# aggregate\n",
    "aggregates = {}\n",
    "for k in numeric_keys:\n",
    "    vals = [float(m[k]) for m in per_seed_metrics if k in m]\n",
    "    if not vals:\n",
    "        continue\n",
    "    mean_k = float(np.mean(vals))\n",
    "    std_k = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "    aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "# save summary\n",
    "out_file = os.path.join(winner_save_path, \"winner_multi_seed_summary.txt\")\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Final winner multi-seed evaluation\\n\")\n",
    "    f.write(f\"trial_number: {best_trial.number}\\n\")\n",
    "    f.write(f\"primary_metric: {primary_key}\\n\")\n",
    "    f.write(f\"seeds: {final_seeds}\\n\")\n",
    "    f.write(\"per_seed_metrics:\\n\")\n",
    "    for s, m in zip(final_seeds, per_seed_metrics):\n",
    "        f.write(f\"  - seed: {s}\\n\")\n",
    "        for k in numeric_keys:\n",
    "            if k in m:\n",
    "                f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "    f.write(\"aggregates:\\n\")\n",
    "    for k, mm in aggregates.items():\n",
    "        f.write(f\"  {k}:\\n\")\n",
    "        f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "        f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "# console summary\n",
    "if primary_key in aggregates:\n",
    "    print(f\"\\nWinner aggregated {primary_key}: {aggregates[primary_key]['mean']:.6f} \"\n",
    "          f\"± {aggregates[primary_key]['std']:.6f}\")\n",
    "elif \"val_loss\" in aggregates:\n",
    "    print(f\"\\nWinner aggregated val_loss: {aggregates['val_loss']['mean']:.6f} \"\n",
    "          f\"± {aggregates['val_loss']['std']:.6f}\")\n",
    "\n",
    "print(f\"Saved multi-seed summary to: {out_file}\")\n",
    "\n",
    "# finally, overwrite best_trial to the winner so any subsequent code uses it\n",
    "best_trial = best_trial  # (explicit; already set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
